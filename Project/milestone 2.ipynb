{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 : Data Collection and Description\n",
    "------\n",
    "In this notebook we are going to review everything that was done so far in the project and evaluate what the remaining tasks are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import time\n",
    "import os\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Abstract\n",
    "\n",
    "*What's the motivation behind your project? A 150 word description of the project idea, goals, dataset used. What story you would like to tell and why?*\n",
    "\n",
    "Major events happen on a regular basis all around the world, some involving high number of casualties but the resulting reaction on the international scale is often far from proportional. Most of the time the largest reaction comes from the place where the incident occurred or places which are closeby. The objective would be to create an awareness map, and determine why people react to an event. From that we would attempt to define an awareness metric. We want to see how factors other than physical proximity come into play such as country, culture, language, religion. With this we could determine which country has the highest level of international awareness. The project would require the Twitter API to acquire hashtag specific tweets with geolocation and therefore measure the awareness and reactions of different communities to a given event. GDELT would be used to recover standardised information regarding different events.\n",
    "____\n",
    "____\n",
    "____\n",
    "\n",
    "\n",
    "## 1.  Identifying Relevant Tweets\n",
    "-----\n",
    "\n",
    "### 1.1 Reminder of the events that we chose\n",
    "\n",
    "**Case 1**: Events of similar magnitude, civilian casualties, 6 months timeframe\n",
    "\n",
    "- Nigeria 30/01/2016, Shooting 65 Deaths, 136 Injured\n",
    "- Belgium 22/03/2016, Bombing in airport, 35 Deaths, 300+ Injured\n",
    "- Pakistan 27/03/2016, Bombing, 70 Deaths, 300 Injured\n",
    "- US 12/06/2016 Shooting in gay bar, 49 Deaths, 53 Injured\n",
    "- Turkey 28/06/2016, Shooting + bombing in airport, 45 Deaths, 230 Injured\n",
    "\n",
    "**Case 2**: Events of different magnitude\n",
    "\n",
    "- France 07/01/2015, Charlie Hebdo, 12 Deaths, 11 Injured\n",
    "- Nigeria 08/01/2015, Massacre Boko Haram, 200+ Deaths, unknown Injured\n",
    "- Lebanon 10/01/2015, suicide bombing, 9 Deaths, 30+ Injured\n",
    "\n",
    "\n",
    "### 1.2 Hashtags as Key Elements for Searching\n",
    "\n",
    "On twitter the Hashtags are mainly during events. In our case it is the perfect tool to evaluate the awareness across the world. It is very convenient because it is often specifically related to one event and tends to be in english even though the rest of the tweet is in a different language. In order to find all the tweets related to an event, we needed to find as many hashtags which were related and in as many languages as possible.  \n",
    "\n",
    "### 1.3 Selection of Hashtags \n",
    "For the selection of the hastags we need to take into acount these factors:\n",
    "- Which hashtags do we select?\n",
    "- How far do we have to go in time to make sure we get all the tweets to study the time evolution?\n",
    "- Hashtags can be written in different languages\n",
    "\n",
    "\n",
    "#### Which hashtags do we select?\n",
    "For selecting the hashtags we used the website http://hashtagify.me/hashtag/smm which after a given search for an initial hashtags it returns the most related hashtags given the timeframe and the actual hashtag similarity. In addition, we manually did an advanced search on twitter to manually check if the hashtags were related to the event and to search for another hashtags that may not appear in the website (some people use more than one related hashtag so that's why we also checked manually).\n",
    "\n",
    "Here is an example of the hashtags that we selected for Charlie Hebdo:\n",
    "\n",
    "- PrayForParis\n",
    "- JeSuisCharlie\n",
    "- NousSommesChalie\n",
    "- CharlieHebdo\n",
    "- LaFranceEstCharlie\n",
    "- LeMondeestCharlie\n",
    "- IAmCharlie\n",
    "- ParisShooting\n",
    "- FreedomOfSpeech\n",
    "- somCharlie\n",
    "- soyCharlie\n",
    "- SomCharlieHebdo\n",
    "- YoSoyCharlie\n",
    "- YoTambienSoyCharlieHebdoç\n",
    "- أنا_شارلي  \n",
    "- IchBinCharlie\n",
    "- EuSouCharlie\n",
    "- JsemCharlie\n",
    "- TodosSomosCharlieHebdo\n",
    "- ЯШарлиЭбдо\n",
    "- من‌شارلی‌هستم\n",
    "\n",
    "#### How far do we have to go in time to make sure we get all the tweets to study the time evolution?\n",
    "We decided that we would only retrieve the tweets done from the day of the event until one week after at maximum. We consider that it will be enough because we concluded that after one week people normally stop massively commenting about these kind of events. Although we think our assumption will be correct, it might not be true for Charlie Hebdo which is the most commented event, but after analyzing all the events we can always rerun the code that gathers all the tweets and get more.\n",
    "\n",
    "#### Hashtags can be written in different languages\n",
    "The methodology we applied to get all possible languages was first getting the main hashtags in English and then we manually checked if the translations were also used. We searched for the translations in the website above and also we checked manually in twitter, to check other similar hashtags but with the other languages rather than English. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "____\n",
    "\n",
    "## 2.  Tweets Acquisition\n",
    "We had originally planned to use the twitter dataset that was given in the course. Unfortunatelly it was containing only 10% of the tweets in a given time period and wasn't including any information on the location of the user nor the user profile. Because of this we decided to go get the tweets about specific events by ourselves. \n",
    "\n",
    "------\n",
    "### 2.1 Twitter API \n",
    "Our initial idea was to get the information we needed with the Twitter API, but there again we encountered several problems : \n",
    "\n",
    "- The **Rate Limit** of the Twitter API :  It would have taken a lot of time to get the tweets of a specific event, but we were ready to wait and launch the code on several computers (or on clusters)\n",
    "- The **Search Query** limitations : After designing a code that would allow us to get the tweets by searching specific hashtags over a time interval, we discovered a huge limitation : tweets can only we searched with the API if they are *less than one week old*. \n",
    "\n",
    "So we have to discard the idea to use the Twitter API.\n",
    "\n",
    "------\n",
    "### 2.2 Scrapping Manually the Tweets \n",
    "Fortunatelly the twitter html interface (the website) allows us to search for any query on anytime interval. So we decided get the data by scraping directly the website. For that we use a browser that doesn't have a user interface **PhantomJS** and **Selenium** a python package that allows us to load urls in this browser and scroll down the search page in order to load results. Once loaded the use **Beautifull Soup 4** with the parser **LXML** To get every tweets of the page.\n",
    "\n",
    "This was done using one script : [`tweet_acquisiton.py`](ADA2017_Homeworks/Project/TweetAcquisition/tweet_acquisition.py). \n",
    "For each event a new folder is created (for example here `Nigeria_1`). The logs of the tweet acquisition has been saved in this folder with an obvious name (Here `Nigeria_1.log`). Here is an example of the start of the log file : \n",
    "\n",
    "-----\n",
    "```javascript\n",
    "------------------------------------------- ACQUISITION PARAMETERS -------------------------------------------\n",
    "Started at : 2017-11-27 10:10:47.485905\n",
    "Tweets saved in ./Nigeria_1/\n",
    "Searching from 2016-01-29 to 2016-02-06\n",
    "Hastags used : ['Dalori', 'Dalorilivesmatter', 'Nigeria', 'BokoHaram', 'Bokoharam', 'bokoharam', 'Borno', 'StopBokoHaram', 'PrayForNigeria']\n",
    "------------------------------------------- STARTING ACQUISITION -------------------------------------------\n",
    "1 - Tweets : 2772 - Total : 2772 - Date : 2016-02-05 07:39:06 - Elapsed Time : 810.799 s - Delay : 810.799 s - Rate : 3.419 tw/s - Executed at 2017-11-27 10:24:20.470199\n",
    "     + First Tweet Time : 2016-02-05 22:11:24\n",
    "     + Last Tweet Time : 2016-02-05 07:39:06\n",
    "```\n",
    "\n",
    "------\n",
    "The query url is created using the list of hashtags specified inside the script. The explanations on how to use the scripts are in the [`README.md`](ADA2017_Homeworks/Project/TweetAcquisition/README.md) file.\n",
    "\n",
    "\n",
    "The tweets are acquired by segments : we scroll 500 times the page before parsing the html and saving a pickle containing the Raw data. Each pickle contains an average of 7000 tweets.  We show here an example of the structure of the dataframe acquired :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@FitzMP, #Biafrans, #Nigeria, #TyrantBuhari]</td>\n",
       "      <td>695731474243440640</td>\n",
       "      <td>en</td>\n",
       "      <td>@FitzMP,We #Biafrans have died enough, we don’...</td>\n",
       "      <td>1454710284</td>\n",
       "      <td>354778701</td>\n",
       "      <td>EmekaGift</td>\n",
       "      <td>2016-02-05 22:11:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[#Nigeria, http://bit.ly/1SR2k89 ]</td>\n",
       "      <td>695758765627281408</td>\n",
       "      <td>es</td>\n",
       "      <td>A más de dos años de que comenzó la crisis, ¿q...</td>\n",
       "      <td>1454716790</td>\n",
       "      <td>57683930</td>\n",
       "      <td>MSF_Mexico</td>\n",
       "      <td>2016-02-05 23:59:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[#PrayForNigeria]</td>\n",
       "      <td>695758763517730816</td>\n",
       "      <td>en</td>\n",
       "      <td>I wish I was a little kid again, where all I h...</td>\n",
       "      <td>1454716790</td>\n",
       "      <td>518819812</td>\n",
       "      <td>allthingselliej</td>\n",
       "      <td>2016-02-05 23:59:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[#Nigeria, http://bit.ly/1odNc9y , #VOA]</td>\n",
       "      <td>695758537289367552</td>\n",
       "      <td>en</td>\n",
       "      <td>#Nigeria E-readers Help Thousands in Africa Le...</td>\n",
       "      <td>1454716736</td>\n",
       "      <td>2468196914</td>\n",
       "      <td>Vincecob</td>\n",
       "      <td>2016-02-05 23:58:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        hashtags                  id language  \\\n",
       "0  [@FitzMP, #Biafrans, #Nigeria, #TyrantBuhari]  695731474243440640       en   \n",
       "1             [#Nigeria, http://bit.ly/1SR2k89 ]  695758765627281408       es   \n",
       "2                              [#PrayForNigeria]  695758763517730816       en   \n",
       "3       [#Nigeria, http://bit.ly/1odNc9y , #VOA]  695758537289367552       en   \n",
       "\n",
       "                                                text  time_stamp     user_id  \\\n",
       "0  @FitzMP,We #Biafrans have died enough, we don’...  1454710284   354778701   \n",
       "1  A más de dos años de que comenzó la crisis, ¿q...  1454716790    57683930   \n",
       "2  I wish I was a little kid again, where all I h...  1454716790   518819812   \n",
       "3  #Nigeria E-readers Help Thousands in Africa Le...  1454716736  2468196914   \n",
       "\n",
       "         user_name                date  \n",
       "0        EmekaGift 2016-02-05 22:11:24  \n",
       "1       MSF_Mexico 2016-02-05 23:59:50  \n",
       "2  allthingselliej 2016-02-05 23:59:50  \n",
       "3         Vincecob 2016-02-05 23:58:56  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pickle.load(open('TweetAcquisition/Nigeria_1/Tweets_1.pickle', 'rb'))\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have scrapped as many information as possible from the html page of the search query, bit we still miss the most important thing : the location of the tweet.\n",
    "\n",
    "------\n",
    "### 2.3 Scrapping the location of the tweets \n",
    "From each tweet we take the `user_name` field and we go to the user profile to get the location information that the user has written on his profile. \n",
    "The function that does that is : [`location_acquisiton.py`](ADA2017_Homeworks/Project/TweetAcquisition/location_acquisiton.py). As we don't need to scroll down the page we directly use the **requests** python package combined with **Beautiful Soup 4** and **LXML**. As the code is very slow, we launch several times the process in parrallel in order to get the tweets at the same rate. \n",
    "\n",
    "In the follwing we display the head of the *Located* version of the pickled dataframe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>date</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[@FitzMP, #Biafrans, #Nigeria, #TyrantBuhari]</td>\n",
       "      <td>695731474243440640</td>\n",
       "      <td>en</td>\n",
       "      <td>@FitzMP,We #Biafrans have died enough, we don’...</td>\n",
       "      <td>1454710284</td>\n",
       "      <td>354778701</td>\n",
       "      <td>EmekaGift</td>\n",
       "      <td>2016-02-05 22:11:24</td>\n",
       "      <td>[www.radiobiafra.co]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[#Nigeria, http://bit.ly/1SR2k89 ]</td>\n",
       "      <td>695758765627281408</td>\n",
       "      <td>es</td>\n",
       "      <td>A más de dos años de que comenzó la crisis, ¿q...</td>\n",
       "      <td>1454716790</td>\n",
       "      <td>57683930</td>\n",
       "      <td>MSF_Mexico</td>\n",
       "      <td>2016-02-05 23:59:50</td>\n",
       "      <td>[Ciudad, de, Mexico]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[#PrayForNigeria]</td>\n",
       "      <td>695758763517730816</td>\n",
       "      <td>en</td>\n",
       "      <td>I wish I was a little kid again, where all I h...</td>\n",
       "      <td>1454716790</td>\n",
       "      <td>518819812</td>\n",
       "      <td>allthingselliej</td>\n",
       "      <td>2016-02-05 23:59:50</td>\n",
       "      <td>[SomewhereOnlyWeKnow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[#Nigeria, http://bit.ly/1odNc9y , #VOA]</td>\n",
       "      <td>695758537289367552</td>\n",
       "      <td>en</td>\n",
       "      <td>#Nigeria E-readers Help Thousands in Africa Le...</td>\n",
       "      <td>1454716736</td>\n",
       "      <td>2468196914</td>\n",
       "      <td>Vincecob</td>\n",
       "      <td>2016-02-05 23:58:56</td>\n",
       "      <td>[Brussels,, Belgium]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        hashtags                  id language  \\\n",
       "0  [@FitzMP, #Biafrans, #Nigeria, #TyrantBuhari]  695731474243440640       en   \n",
       "1             [#Nigeria, http://bit.ly/1SR2k89 ]  695758765627281408       es   \n",
       "2                              [#PrayForNigeria]  695758763517730816       en   \n",
       "3       [#Nigeria, http://bit.ly/1odNc9y , #VOA]  695758537289367552       en   \n",
       "\n",
       "                                                text  time_stamp     user_id  \\\n",
       "0  @FitzMP,We #Biafrans have died enough, we don’...  1454710284   354778701   \n",
       "1  A más de dos años de que comenzó la crisis, ¿q...  1454716790    57683930   \n",
       "2  I wish I was a little kid again, where all I h...  1454716790   518819812   \n",
       "3  #Nigeria E-readers Help Thousands in Africa Le...  1454716736  2468196914   \n",
       "\n",
       "         user_name                date               location  \n",
       "0        EmekaGift 2016-02-05 22:11:24   [www.radiobiafra.co]  \n",
       "1       MSF_Mexico 2016-02-05 23:59:50   [Ciudad, de, Mexico]  \n",
       "2  allthingselliej 2016-02-05 23:59:50  [SomewhereOnlyWeKnow]  \n",
       "3         Vincecob 2016-02-05 23:58:56   [Brussels,, Belgium]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pickle.load(open('TweetAcquisition/Nigeria_1/Located_Tweets_1.pickle', 'rb'))\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the raw location information for each event. We need to geocode it to the associated country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## 3.  Geocoding the tweets\n",
    "\n",
    "------\n",
    "\n",
    "When geocoding the tweets multiple factors needed to be taken into account : \n",
    "- Not all the tweets had a provided location, in which case we scraped the user's location. However, most users do not provide that information which means that a large number of tweets had to be discarded as no location could be attributed. \n",
    "- The locations provided are entered manually. That means that users can write their locations however they want, with any spelling (\"USAAAAAA\"), with any type of special characters (\"P@ris\"). Some locations are even invented (\"Somewhere only we know\", \"Heaven\"). \n",
    "- The locations can be either countries, cities, regions, neighborhoods... \n",
    "- The locations can be written in any language\n",
    "\n",
    "This is only listing a few of the issues which were encountered. That is why the first step was to create a mapping dictionary which would link various country names (in different languages and with different spellings) as well as cities to the ISO2 country code. Afterwards we went through the various dataframes containing the tweets for the different events and mapped the locations using the different dictionaries created. Finally for each event we determined the activity level of each country as the number tweets associated to the country.\n",
    "\n",
    "All of this was done using dictionaries to have a fast localization process. Currently, for pickles containing around 8000 tweets, we require around one second to identify the corresponding countries from the different dictionaries.\n",
    "\n",
    "\n",
    "The notebooks used for the different steps are available here : \n",
    "-  [`Constructing the Mappings`](https://github.com/LailaHms/ADA2017_Homeworks/blob/Laila_Project/Project/GeocodingTweets/Constructing%20the%20Mappings.ipynb)\n",
    "\n",
    "- [`Geocoding Tweets Using the Mappings`](https://github.com/LailaHms/ADA2017_Homeworks/blob/Laila_Project/Project/GeocodingTweets/Geocoding%20Tweets%20Using%20the%20Mappings.ipynb)\n",
    "\n",
    "- [`Determining Number of Tweets Per Event`](https://github.com/LailaHms/ADA2017_Homeworks/blob/Laila_Project/Project/GeocodingTweets/Determining%20Number%20of%20Tweets%20Per%20Event.ipynb)\n",
    "\n",
    "We will however explain some of the main points here. \n",
    "\n",
    "Remark : due to the size of certain files and some of these mappings they could not be put on the github. \n",
    "\n",
    "_____\n",
    "\n",
    "### 3.1 Creating the Different Location Mappings\n",
    "\n",
    "As mentioned previously we need to take into account multiple things for the mappings. The most important being languages and alternative spellings. That is why we created our mappings from databases which gave alternative names and spellings for each country, capital, city when possible. Once all of these we obtained we also removed all possible accents from the strings. We then combined the original spellings with the formatted strings and removed duplicates using sets. The remaining names were then used as keys of the dictionnaries and the corresponding country as the value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String formatting functions and accent removal as well as list formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/8694815/removing-accent-and-special-characters\n",
    "def remove_accents(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    else :\n",
    "        clean = ''.join(x.lower().strip() for x in unicodedata.normalize('NFKD', data) if \\\n",
    "                unicodedata.category(x)[0] == 'L').lower()\n",
    "        return clean\n",
    "    \n",
    "def string_formatting(string):\n",
    "    string = string.replace(\"-\", \" \").replace(\" \", \",\").split(\",\")\n",
    "    formatted_string = [remove_accents(x.lower()) for x in string]\n",
    "    return string,formatted_string\n",
    "\n",
    "def clean_sublist(x):\n",
    "    return list(set(filter(None, np.hstack(x))))\n",
    "\n",
    "def remove_accents_in_sublist(l):\n",
    "    return list(map(lambda x:remove_accents(x.lower()),l))\n",
    "    \n",
    "def remove_accents_in_list(lists):\n",
    "    return list(map(lambda x:remove_accents_in_sublist(x),lists))\n",
    "\n",
    "def clean_and_remove_accents_in_list(lists):\n",
    "    return list(map(lambda x:clean_sublist(remove_accents_in_sublist(x)),lists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show an example of the string formatting used on two countries using the data provided in https://datahub.io/core/country-codes which gives the name of the county in different languages and the capital in english. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['阿富汗', 'афганистан', 'afganistan', 'afghanistan', 'kabul', 'افغانستان'],\n",
       " ['阿尔巴尼亚', 'албания', 'tirana', 'albania', 'albanie', 'البانيا']]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [['أفغانستان', 'afganistán', '阿富汗', 'афганистан', 'Kabul', 'afghanistan'], ['阿尔巴尼亚', 'албания', 'Tirana', 'ألبانيا', 'albania', 'albanie']]\n",
    "clean_and_remove_accents_in_list(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is used to convert the dataframe to a dictionary where the values is the index and the keys are all the elements in the different rows. We also take the variants of all the elements in the rows and use them as keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_dict(df, do_prints = False):\n",
    "    \n",
    "    # Converting the dataframe values to list and cleaning them\n",
    "    t = time.time()\n",
    "    df_list = list(map(lambda x:clean_sublist(x),df.values.tolist()))\n",
    "    if do_prints : print(\"Converting to list :\", time.time()-t)\n",
    "\n",
    "    # Removing all the accents from the elements in the list\n",
    "    t = time.time()\n",
    "    df_variants = clean_and_remove_accents_in_list(df_list)\n",
    "    if do_prints : print(\"Getting variants :\", time.time()-t)\n",
    "    \n",
    "    # Combining the lists with original spellings and without accents\n",
    "    t = time.time()\n",
    "    df_all =  list(map(lambda x: list(set(df_list[x] + df_variants[x])),range(len(df))))\n",
    "    if do_prints : print(\"Combining Lists :\", time.time()-t)\n",
    "        \n",
    "    # Getting all the keys\n",
    "    t = time.time()\n",
    "    keys = list(map(lambda x: [df.index[x]]*(len(df_all[x])),range(len(df_all))))\n",
    "    if do_prints : print(\"Getting all keys :\", time.time()-t)\n",
    "      \n",
    "    # Creating the dictionary\n",
    "    t = time.time()\n",
    "    mapping = dict(zip(sum(df_all, []),sum(keys, [])))\n",
    "    if do_prints : print(\"Converting to dict :\", time.time()-t)\n",
    "        \n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 The country_mapping dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how some of the previous functions were used to construct a portion of the country_mapping dictionary using the data from https://datahub.io/core/country-codes. At the end we print the first 18 elements of the dictionary which are used to identify Afghanistan and Albania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Afghanistan', 'AF'), ('阿富汗', 'AF'), ('Афганистан', 'AF'), ('афганистан', 'AF'), ('Afganistán', 'AF'), ('afghanistan', 'AF'), ('Kabul', 'AF'), ('afganistan', 'AF'), ('kabul', 'AF'), ('افغانستان', 'AF'), ('أفغانستان', 'AF'), ('Албания', 'AL'), ('阿尔巴尼亚', 'AL'), ('албания', 'AL'), ('Albanie', 'AL'), ('tirana', 'AL'), ('albania', 'AL'), ('ألبانيا', 'AL'), ('albanie', 'AL'), ('Tirana', 'AL'), ('البانيا', 'AL'), ('Albania', 'AL')]\n"
     ]
    }
   ],
   "source": [
    "# Load the country names in different languages mapping\n",
    "country_codes = pd.read_csv(\"GeocodingTweets/Mapping Files/country-codes.csv\")\n",
    "keep_columns = ['official_name_ar', 'official_name_cn', 'official_name_en',\n",
    "                'official_name_es', 'official_name_fr', 'official_name_ru',\n",
    "                'ISO3166-1-Alpha-2', 'ISO3166-1-Alpha-3', 'ISO3166-1-numeric',\n",
    "                'Capital', 'Continent', 'Region Name','Sub-region Name']       \n",
    "# Keep only the desired columns\n",
    "country_codes = country_codes[keep_columns]\n",
    "country_codes.rename(inplace = True, index=str, columns={\"official_name_ar\": \"arabic\", \"official_name_cn\":\"chinese\", \"official_name_en\":\"english\", \n",
    "                                                         \"official_name_es\":\"spanish\", \"official_name_fr\":\"french\", \"official_name_ru\":\"russian\",\n",
    "                                                         \"ISO3166-1-Alpha-2\":\"ISO2\", \"ISO3166-1-Alpha-3\":\"ISO3\", \"ISO3166-1-numeric\":\"ISONum\"})\n",
    "# Remove the first element in the dataframe\n",
    "country_codes = country_codes.iloc[1:]\n",
    "\n",
    "# Create the dictionary\n",
    "country_codes.set_index(\"ISO2\", inplace = True)\n",
    "col = [\"english\", \"french\", \"spanish\", \"chinese\", \"russian\", \"arabic\", \"Capital\"]\n",
    "country_mapping1 = convert_df_to_dict(country_codes[col])\n",
    "\n",
    "print(list(zip(country_mapping1.keys(), country_mapping1.values()))[:22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second portion of the country mapping was created taking the data from https://raw.githubusercontent.com/mledoze/countries/master/countries.json\n",
    "which gives the names of the different countries in different languages with alternative spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions necessary to extract the information from the different cells of the dataframe\n",
    "\n",
    "# Get the common native name from the dictionary in the native column\n",
    "def extract_native_name(x):\n",
    "    try:\n",
    "        return x[\"native\"][list(x[\"native\"].keys())[0]][\"common\"]\n",
    "    except:\n",
    "        return \n",
    "\n",
    "# Get the different translations from the dictionary in the official column\n",
    "def extract_translations(x):\n",
    "    val = x.values()\n",
    "    try:\n",
    "        return[name[\"common\"] for name in x.values()]\n",
    "    except:\n",
    "        return \n",
    "\n",
    "# Load the json into a dataframe and keep only relevant columns\n",
    "country_df = pd.read_json(\"GeocodingTweets/Mapping Files/countries.json\")\n",
    "country_df = country_df[[\"altSpellings\", \"capital\", \"cca2\", \"name\", \"translations\"]]\n",
    "country_df.rename(inplace = True, index=str, columns={\"cca2\": \"ISO2\"})\n",
    "country_df.set_index(\"ISO2\", inplace = True)\n",
    "\n",
    "# Extract from the different columns the alternative names and spellings in different languages\n",
    "country_df[\"common\"] = country_df[\"name\"].apply(lambda x: x[\"common\"])\n",
    "country_df[\"official\"] = country_df[\"name\"].apply(lambda x: x[\"official\"])\n",
    "country_df[\"native\"] = country_df[\"name\"].apply(lambda x: extract_native_name(x))\n",
    "country_df[\"common translations\"] = country_df[\"translations\"].apply(lambda x: extract_translations(x))\n",
    "country_df[\"altSpellings\"] = country_df[\"altSpellings\"] .apply(lambda x: x[1:] if len(x)>1 else [])\n",
    "country_df.drop([\"name\",\"translations\"], axis = 1, inplace = True)\n",
    "\n",
    "# Convert the dataframe to a dictionary\n",
    "country_mapping2 = convert_df_to_dict(country_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally both mappings were merged into one. The entire process is only run once as the output is pickled. For more details refer to the original [`Constructing the Mappings`](https://github.com/LailaHms/ADA2017_Homeworks/blob/Laila_Project/Project/GeocodingTweets/Constructing%20the%20Mappings.ipynb) notebook. Here we output the mapping for Afghanistan using respectively the first dataset, the second and the combination of both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Afghanistan AF\n",
      "阿富汗 AF\n",
      "Афганистан AF\n",
      "афганистан AF\n",
      "Afganistán AF\n",
      "afghanistan AF\n",
      "Kabul AF\n",
      "afganistan AF\n",
      "kabul AF\n",
      "افغانستان AF\n",
      "أفغانستان AF\n",
      "---------------------------------------------------------------\n",
      "Afġānistān AF\n",
      "アフガニスタン AF\n",
      "Afghanistan AF\n",
      "Афганистан AF\n",
      "Islamic Republic of Afghanistan AF\n",
      "阿富汗 AF\n",
      "афганистан AF\n",
      "アフカニスタン AF\n",
      "islamicrepublicofafghanistan AF\n",
      "Afganistan AF\n",
      "Afganistán AF\n",
      "Kabul AF\n",
      "afganistan AF\n",
      "afghanistan AF\n",
      "افغانستان AF\n",
      "Afeganistão AF\n",
      "Affganistan AF\n",
      "kabul AF\n",
      "afeganistao AF\n",
      "affganistan AF\n",
      "---------------------------------------------------------------\n",
      "Afghanistan AF\n",
      "阿富汗 AF\n",
      "Афганистан AF\n",
      "афганистан AF\n",
      "Afganistán AF\n",
      "afghanistan AF\n",
      "Kabul AF\n",
      "afganistan AF\n",
      "kabul AF\n",
      "افغانستان AF\n",
      "أفغانستان AF\n",
      "Afġānistān AF\n",
      "アフガニスタン AF\n",
      "Islamic Republic of Afghanistan AF\n",
      "アフカニスタン AF\n",
      "islamicrepublicofafghanistan AF\n",
      "Afganistan AF\n",
      "Afeganistão AF\n",
      "Affganistan AF\n",
      "afeganistao AF\n",
      "affganistan AF\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "country_mapping = {**country_mapping1, **country_mapping2}\n",
    "\n",
    "for mapping in [country_mapping1, country_mapping2,country_mapping]:\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    for key, val in zip(mapping.keys(), mapping.values()):\n",
    "        if val == \"AF\":\n",
    "            print(key, val)\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 The city_mapping dictionary\n",
    "\n",
    "This mapping was created using the Cities of the world dataset in Json format which is based on GeoNames Gazetteer taken from https://github.com/lutangar/cities.json. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sant Julià de Lòria</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pas de la Casa</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ordino</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>les Escaldes</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la Massana</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ISO2\n",
       "city                    \n",
       "Sant Julià de Lòria   AD\n",
       "Pas de la Casa        AD\n",
       "Ordino                AD\n",
       "les Escaldes          AD\n",
       "la Massana            AD"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_df = pd.read_json(\"GeocodingTweets/Mapping Files/cities.json\")\n",
    "city_df.drop([\"lat\", \"lng\"], axis = 1, inplace = True)\n",
    "city_df.rename(inplace = True, index=str, columns={\"country\": \"ISO2\", \"name\":\"city\"})\n",
    "city_df.set_index(\"city\", inplace = True)\n",
    "city_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with this mapping is that there are multiple cities with the same name in different countries. As we have no way of determining which city is the most likely, we drop those rows from the dataframe and store them in a second one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10409\n"
     ]
    }
   ],
   "source": [
    "doublons = city_df.copy()\n",
    "doublons[\"num\"] = 1\n",
    "doublons = doublons.groupby(\"city\").sum()\n",
    "doublons = doublons[doublons.num>1]\n",
    "doublons = doublons.index.tolist()\n",
    "print(len(doublons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an example of why the mapping provided is problematic, especially since we cannot rely on language to determine to which country the city belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "Toronto    AU\n",
       "Toronto    CA\n",
       "Toronto    US\n",
       "Name: ISO2, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_df.loc[\"Toronto\",\"ISO2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all problematic cities from the mapping and creating a dictionary from the remaining cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_city_df = city_df.drop(doublons)\n",
    "city_mapping = dict(zip(reduced_city_df.index, reduced_city_df.ISO2))\n",
    "\n",
    "alt_names = [remove_accents(x) for x in reduced_city_df.index]\n",
    "city_mapping = {**dict(zip(alt_names, reduced_city_df.ISO2)), **city_mapping}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this mapping is far from complete and is missing many cities, especially after having removed the cities with identical names. However we can quickly check a few of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nantes in : FR\n",
      "Lausanne in : CH\n",
      "Abu Dhabi in : AE\n",
      "Shanghai in : CN\n",
      "Beijing in : CN\n",
      "Tokyo in : JP\n"
     ]
    }
   ],
   "source": [
    "print(\"Nantes in :\", city_mapping[remove_accents(\"Nantes\")])\n",
    "print(\"Lausanne in :\", city_mapping[remove_accents(\"Lausanne\")])\n",
    "print(\"Abu Dhabi in :\", city_mapping[remove_accents(\"Abu Dhabi\")])\n",
    "print(\"Shanghai in :\", city_mapping[remove_accents(\"Shanghai\")])\n",
    "print(\"Beijing in :\", city_mapping[remove_accents(\"Beijing\")])\n",
    "print(\"Tokyo in :\", city_mapping[remove_accents(\"Tokyo\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 The full_city_mapping dictionaries\n",
    "\n",
    "This mapping was constructed using a databse of cities in each country taken from http://download.geonames.org/export/dump/. This database contains a zip file for each country with a textfile containing the different cities as well as alternate names. The functions used to process this database are much longer which is why we urge any curious readers to refer to the original notebook used to construct this mapping [`Constructing the Mappings`](https://github.com/LailaHms/ADA2017_Homeworks/blob/Laila_Project/Project/GeocodingTweets/Constructing%20the%20Mappings.ipynb) for more details (see the section Method 4 : Using the Geonames Database) \n",
    "\n",
    "The main steps are the following: \n",
    "- Load the text file for each country\n",
    "- If the text file is larger than a max size, split it into smaller text files to speed up the processing\n",
    "- For each text file load the data into a dataframe \n",
    "- Use the convert df to dict function to save a dictionary for each text file\n",
    "- Once the text files have been processed, load the different dictionaries and merge them into dictionaries with the name full_city_mapping_i which each contain 100 dictionaries. \n",
    "\n",
    "Breaking up the data into smaller subsets was necessary to speed up the processing time as well as to account for the large amount of memory needed and the time it takes to load the dictionaries from the pickle formats. \n",
    "\n",
    "It is important to mention that we did not handle cities with the same name in this dataset. One idea was to use the information regarding the population of each city to determine which was the most likely. This question still has to be adressed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### 3.2. Using the Dictionaries to Map the Locations \n",
    "\n",
    "\n",
    "To map the tweets to their locations we used in order : \n",
    "\n",
    "- The country_mapping dictionary to check whether the country name or capital was contained in the string using the country_mapping dictionary. This was possible using the data obtained from : https://mledoze.github.io/countries/ and https://datahub.io/core/country-codes. The first links the country iso codes to country names in multiple languages with not only the official but also the common names of a country. The latter links the country iso codes to country names in different languages (arabic, chinese, english, spanish, french, russian). \n",
    "\n",
    "- The city_mapping dictionary which was constructed using data from https://github.com/lutangar/cities.json from which we removed duplicate cities. Note that this list was not exhaustive to being with which is why we also used the last mapping \n",
    "\n",
    "- A city to country mapper extracted from : http://www.geonames.org/export/ and http://download.geonames.org/export/dump/. The issue with this dataframe is that the duplicate cities were not handled. The advantage of this mapper however is that it is more extensive than the previous one, contaning a larger number of cities as well as alternative spellings and different languages. Ideally, what should have been done in the case of multiple cities with same name would be to select based on the population of the cities. However we do not have the population size for all the cities provided. \n",
    "\n",
    "- If none provided a valid location, one idea which would have been a good solution if we had payed the subscriptions would have been to use APIs such as the google API to obtain the most likely location corresponding to a given input. The issue with these APIs is that they tend to be slow (1 second per tweet) and are limited to a given number of queries which is why they were not used. Some of the APIs we looked at were those based on the same dataset as the one used to create the full_city_mapping dictionaries such as the ones which can be found here http://geocoder.readthedocs.io/results.html. It outputs the most probable location to which the user selected location corresponds to. Even if we had subscriptions to these services, given that the number of tweets is in the order of magnitude of the millions and that the query was relatively slow, this would not have been feasible on the entire dataset. Note : we do not know whether it would have been faster with a subscription but it would have had to be at least 100 times faster to be a viable candidate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simplified version of the function used in the [`Geocoding Tweets Using the Mappings`](https://github.com/LailaHms/ADA2017_Homeworks/blob/Laila_Project/Project/GeocodingTweets/Geocoding%20Tweets%20Using%20the%20Mappings.ipynb) notebook. The function here only looks at the country mapping which is already in the notebook just to serve as a proof of concept and see how much time it takes to identify the country / city using the dictionaries. \n",
    "\n",
    "The idea is that we need to take into account that certain locations are made up of multiple words which is why we test the combination of adjacent words. We then check for each of the comabinations whether the combination is in the mapping. If it is then we output the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_in_string(loc, do_prints = False): \n",
    "    t = time.time()\n",
    "    \n",
    "    # Get the formatted and non formatted version of the words\n",
    "    words, formatted_words = string_formatting(loc)\n",
    "    if do_prints : print(words)\n",
    "        \n",
    "    # Remove words smaller than 2 characters and get all their combinations\n",
    "    # considering only adjacent words\n",
    "    words = [x.lower() for x in words if len(x)>2]\n",
    "    formatted_words = [x for x in formatted_words if len(x)>2]\n",
    "    \n",
    "    word_combinations = [\" \".join(words[i:j]) for j in range(len(words)+1) for i in range(j)]\n",
    "    word_combinations += [\" \".join(words[i:j]) for j in range(len(formatted_words)+1) for i in range(j)]\n",
    "    if do_prints : print(word_combinations)\n",
    "    \n",
    "    # If one of the combinations is in the dict then output it\n",
    "    matching = []\n",
    "    for word in word_combinations:\n",
    "        if do_prints : print(\"Testing: \", word)\n",
    "        if word in country_mapping:\n",
    "            return country_mapping[word], time.time()-t\n",
    "\n",
    "    return None, time.time()-t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the function above with different countries, in different languages playing with the upper case and lower case letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AF', 8.988380432128906e-05)\n",
      "('AF', 3.409385681152344e-05)\n",
      "('ES', 8.416175842285156e-05)\n",
      "('AT', 5.817413330078125e-05)\n",
      "('AT', 3.314018249511719e-05)\n",
      "('AT', 3.62396240234375e-05)\n",
      "('AT', 2.288818359375e-05)\n",
      "('AT', 2.2172927856445312e-05)\n",
      "('NZ', 3.409385681152344e-05)\n",
      "('NZ', 3.0279159545898438e-05)\n",
      "('US', 2.1219253540039062e-05)\n",
      "('EG', 1.9073486328125e-05)\n"
     ]
    }
   ],
   "source": [
    "print(country_in_string(\"أفغانستان hello I am bored\"))\n",
    "print(country_in_string(\"أفغانستان\"))\n",
    "print(country_in_string(\"España oiejdoew sdjoidsjf sdnfosid\"))\n",
    "print(country_in_string(\"autriche\"))\n",
    "print(country_in_string(\"oesterreich\"))\n",
    "print(country_in_string(\"osterreich\"))\n",
    "print(country_in_string(\"austria\"))\n",
    "print(country_in_string(\"vienna\"))\n",
    "print(country_in_string(\"Hello New Zealand\"))\n",
    "print(country_in_string(\"Hello New ZeAlAND\"))\n",
    "print(country_in_string(\"Washington\"))\n",
    "print(country_in_string(\"CAIro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of tests using all the different mappings can be seen in the original notebook [`Geocoding Tweets Using the Mappings`](https://github.com/LailaHms/ADA2017_Homeworks/blob/Laila_Project/Project/GeocodingTweets/Geocoding%20Tweets%20Using%20the%20Mappings.ipynb). \n",
    "\n",
    "This function was then run on the different dataframes of the tweets which were acquired and the results were stored into the Geolocated folder. This way we can always access the different tweets and their locations if we want. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### 3.3. Determining the Number of Tweets Per Country Per Event \n",
    "\n",
    "Using the dataframes with the tweets and their locations we then used the groupby functionality and count to determine hoe many tweets there were per dataframe per country and merged them all into one summary_dataframe in the Geolocated folder of each event. The code used can be seen here : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute = False\n",
    "\n",
    "if execute:\n",
    "    cwd = os.getcwd()\n",
    "    path = os.path.join(cwd, \"../../../Project Data/Tweets\")\n",
    "    # Get all the files in the current working directory\n",
    "    folders = os.listdir(path)\n",
    "    # Keep only the folders excluding the checkpoints folder -> event folders\n",
    "    folders = [x for x in folders if os.path.isdir(os.path.join(path, x)) if \"checkpoints\" not in x if \"DS_Store\" not in x]\n",
    "\n",
    "    do_prints = False\n",
    "\n",
    "    # Get the country codes from the country mapping pickle. This will be used to init\n",
    "    # the dataframe which will contain the overall number of tweets per country per event. \n",
    "\n",
    "    country_codes = pd.read_pickle(\"country_mapping.pickle\")\n",
    "    if do_prints : print(type(list(set(country_codes.values()))[0]))\n",
    "    country_codes = [x for x in list(set(country_codes.values())) if type(x) is not float]\n",
    "\n",
    "    # Go through all the different events folders\n",
    "    for folder in folders:\n",
    "        # Get all the files in the event folder\n",
    "        files_path = os.path.join(path, folder, \"Geocoded\")\n",
    "        located_files = [x for x in  os.listdir(files_path) if \"Located\" in x]\n",
    "\n",
    "        # Create the first empty dataframe in which all the counts will be stored\n",
    "        event_locations = pd.DataFrame(pd.Series(country_codes), columns = [\"country\"])\n",
    "        event_locations.set_index(\"country\", inplace = True)\n",
    "        event_locations[\"text\"] = 0\n",
    "        event_locations[\"text\"] = event_locations[\"text\"]\n",
    "\n",
    "        # Go through all the different files in the folder and process them.\n",
    "        for pkl_file in tqdm(located_files):\n",
    "            # Read the pickle file, groupby country and count the number of tweets then add\n",
    "            # to the final df for the event\n",
    "            df = pd.read_pickle(os.path.join(files_path,pkl_file))\n",
    "            interm_df = df[[\"country\", \"text\"]].groupby(\"country\").count()\n",
    "            event_locations = event_locations.add(interm_df, fill_value=0)\n",
    "\n",
    "        # Pickle the event dataframes\n",
    "        if do_prints: print(event_locations[\"text\"].tolist())\n",
    "        event_locations.to_pickle(os.path.join(files_path, \"summary.pickle\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## 4.  Enriching the Data\n",
    "------\n",
    "### 4.1 Need for enriching with data of each country\n",
    "So far we have explained all the process from retrieving the tweets to geolocalizing them. That would be enough for visualizing the data and have a general overview of the awareness in each country. However, we didn't want to stop here. As explained in Milestone 1, in part two of our project we want to study the different factors that influence the level of awareness of a given country to a certain event. \n",
    "\n",
    "In this section, we describe the process of gathering and cleaning the data. Where did we take the datasets from? What do they look like? We will include a description of the raw data and the necessary steps to clean and transform the data according to our needs.\n",
    "\n",
    "Basically, we want to have a final dataframe with one row that contains all the features of each country. The neccessary features are listed in the last cell in section 4.1.3.\n",
    "\n",
    "### 4.1.1.Size of data\n",
    "Due to the relatively few number of existing countries in the world we did not have any problems for this part regarding the size of the data and memory usage. The dataframes have all around 250 rows, one for each country, and less than 100 feaures.\n",
    "\n",
    "### 4.1.2. Description of the raw data\n",
    "The datasets that we used were taken from the Internet. They are all open projects that use official data that can be freely used for studies. We kept some more features than we actually need which we think they could be used in the future. Here is the list of features and description of the raw data that we got:\n",
    "\n",
    "Link dataset 1: https://github.com/mledoze/countries\n",
    "- Country name: dictionary of dictionaries.\n",
    "    - common : common name in english\n",
    "    - official : official name in english\n",
    "    - native : list of all native names\n",
    "        - key: ISO 3166-1 alpha-3 language code\n",
    "        - value: name object\n",
    "            - key: official - official name translation\n",
    "            - key: common - common name translation\n",
    "            \n",
    "- Country code: code ISO 3166-1 alpha-2\n",
    "- Country code: code ISO 3166-1 alpha-3\n",
    "- Borders: list of all country codes (alpha-3) that touch the border of each country\n",
    "- Land area (in $km^2$)\n",
    "- Latitude and Longitude: in a list [latitude, longitude]\n",
    "- Official languages: dictionary of dictionaries.\n",
    "    - key: ISO 3166-1 alpha-3 language code\n",
    "    - value: name of the language in english\n",
    "\n",
    "Link dataset 2: http://www.thearda.com/Archive/Files/Downloads/WRDNATL_DL2.asp\n",
    "- Population: total\n",
    "- Religions: each religion is one column and the data is given both in total number of adherents in each country and also as a percentage\n",
    "- Country code: code ISO 3166-1 alpha-3\n",
    "\n",
    "Link dataset 3a: https://data.worldbank.org/indicator/NY.GDP.MKTP.CD\n",
    "- Total GDP: in USD\n",
    "\n",
    "Link dataset 3b: https://data.worldbank.org/indicator/NY.GDP.PCAP.CD\n",
    "- Per capita GDP: in USD\n",
    "\n",
    "Link dataset 4: https://github.com/opendatajson/factbook.json\n",
    "- Type of government: not categorized. Needs string processing to create categorical government types\n",
    "- Country code: GEC codes\n",
    "\n",
    "##### Missing feature\n",
    "- Number of active tweeter users.\n",
    "\n",
    "This issue will be adressed in section 6.\n",
    "\n",
    "#### Examples of the first dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the useful columns as they are directly read from the file\n",
    "cols = ['area', 'cca2', 'cca3', 'borders', 'name', 'latlng', 'languages']\n",
    "countries = pd.read_json(r'DataEnriching/countries.json')[cols]\n",
    "countries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of format of name column and language column \n",
    "print('name', countries.name[0], '\\n')\n",
    "print('languages', countries.languages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of the second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the useful columns as they are directly read from the file  \n",
    "pop_rel_df = pd.read_excel('DataEnriching/World Religion Dataset - National Religion Dataset.xlsx')\n",
    "cols = ['YEAR', 'ISO3', 'POP', 'DUALREL'] + \\\n",
    "        [col for col in pop_rel_df.columns if 'PCT' in col]\n",
    "    \n",
    "pop_rel_df = pop_rel_df[cols]\n",
    "pop_rel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have data for every year, we selected the most recent data which is from 2010.\n",
    "\n",
    "We only selected the columns that have the data of the percentage of adherents, not the totals. The percentage data columns have PCT in the column name. For any given religion, we have the percentage of adherents for its main branches but also for the whole religion itself (i.e. for Chrisianity we have CHGENPCT: Total percentage adherents but also CHPRTPCT: Protestants percentage,\n",
    "CHANGPCT: Anglican percentage, etc). For our project we are only interested in the total percentage of adherents in the whole religion, not its branches.\n",
    "\n",
    "To consider wheter a religion is practiced in one country or not we will need to set a treshold on the percentage of adherents.\n",
    "\n",
    "We also took the DUALREL and the SUMPCT columns to show that the sum of the percentages ('SUMPCT') can add up to more than 1 because in some countries they have dual religion as shown in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_rel_df[pop_rel_df.DUALREL == 1][['ISO3', 'DUALREL', 'SUMPCT']].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of the third dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_total = pd.read_csv('DataEnriching/gdp_total.csv', skiprows=3)[['Country Name', 'Country Code', '2016']]\n",
    "gdp_total.rename(columns={'2016': '2016_gdp_total', 'Country Code': 'ISO3'}, inplace=True)\n",
    "gdp_capita = pd.read_csv('DataEnriching/gdp_per_capita.csv', skiprows=4)[['Country Name', 'Country Code', '2016']]\n",
    "gdp_capita.rename(columns={'2016': '2016_gdp_capita', 'Country Code': 'ISO3'}, inplace=True)\n",
    "\n",
    "gdp_df = pd.merge(gdp_total, gdp_capita, on=['ISO3', 'Country Name'])\n",
    "gdp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of the fourth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names of the folders\n",
    "region_folders = ['africa', 'australia-oceania', 'central-america-n-caribbean', 'central-asia', 'east-n-southeast-asia',\n",
    "          'europe', 'middle-east', 'north-america', 'south-america', 'south-asia']\n",
    "\n",
    "# We use a temporaty df to load the data for a particular country and we append it to the main GEC_gov_type_df\n",
    "GEC_gov_type_df = pd.DataFrame()\n",
    "for region in region_folders:\n",
    "    for country_file in os.listdir(r'DataEnriching/factbook.json/' + region):\n",
    "        df = pd.read_json(r'DataEnriching/factbook.json/' + region + '/' + country_file)\n",
    "        try:\n",
    "            gov_type = df.loc['Government type', 'Government']['text']\n",
    "        except:\n",
    "            gov_type = 'unknown'\n",
    "        \n",
    "        GEC_gov_type_df = GEC_gov_type_df.append({'GEC_code': country_file[:2], 'gov_type': gov_type}, ignore_index=True)\n",
    "    \n",
    "GEC_gov_type_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset doesn't contain any ISO code for the countries. Instead, we could only get the GEC code, so we will have to map it with the ISO codes that we will use to merge all the information together in a single dataframe.\n",
    "\n",
    "With respect to the government type, we can see in the cell below that it's not standarized. We will have to analyse each one and group them in broader categories so that we end up with a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example to show that the gov_type is not standarized\n",
    "print(GEC_gov_type_df.gov_type[10])\n",
    "print(GEC_gov_type_df.gov_type[29])\n",
    "print(GEC_gov_type_df.gov_type[221])\n",
    "print(GEC_gov_type_df.gov_type[234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3. Filtering, transforming the data according to our needs\n",
    "In this section the pipeline will be inverted. We will explain all the cleaning and selection process and finally we will present the final dataframe with a description of the features.\n",
    "\n",
    "The actual code of all the procedures is in the notebook: \"Country data gathering.ipynb\".\n",
    "Here we just load the pickled dataframes in order to show the results.\n",
    "\n",
    "#### Dataset 1\n",
    "For this dataset we added some columns to the initial dataframe. From the original languages column we extracted the official languages and the codes  and we added them separately as two new columns, while dropping the original one. The same procedure was applied to the name, to create the name and name_native columns. The name that was extracted to create the column name is the 'common' name in english of the country (see raw data description for all posible options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.read_pickle('DataEnriching/countries_df.pickle')\n",
    "countries_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 2\n",
    "The only things that we did were to select the most recent data of 2010 with a simple query and select the columns that we will need, which are the percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing just data of 2010\n",
    "pop_rel_df = pd.read_pickle('DataEnriching/pop_rel_df.pickle')\n",
    "pop_rel_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 3\n",
    "No cleaning needed\n",
    "\n",
    "#### Dataset 4\n",
    "We only read the json files and extracted the government type. There were some countries that we didn't have this data so we set the variable to 'unknown'. The GEC code was taken from the two first strings of the file (i.e. sp for Spain was taken from sp.json file).\n",
    "\n",
    "To merge the datasets together we need ISO3 codes, so we mapped the GEC codes. To do that we did some web scraping in http://www.statoids.com/wab.html and then we merged into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gov_type_df = pd.read_pickle('DataEnriching/gov_type_df.pickle')\n",
    "gov_type_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging into one dataframe & transforming columns\n",
    "We merged the four previous dataframes on ISO3 country codes. Now we need to categorize the gov_type column and compress the religions into one column.\n",
    "\n",
    "For the religions, we proceded with the following:\n",
    "- we only consider that a country has a certain religion if the corresponding percentage is greater than a treshold of 10%\n",
    "So we run a loop for every religion in every country and we only keep the ones that pass the treshold.\n",
    "\n",
    "For the gov_type the methodology was the following. We are going to run a function to all the rows of the gov_type column that will return the most common sequences of words, so that we can then manually check which are the main type of government. \n",
    "Once the types of government are defined, we will run loop through each row and replace the value with the mapped categorical government type.\n",
    "\n",
    "Here is an example of the code we used to make the manual checking. As we noticed most gov_types had 2 or 3 words, we filtered to sequences of that number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrases(string):\n",
    "    \"\"\"Splits the input string on whitespace and returns all possible substrings of any length\"\"\"\n",
    "    words = string.split()\n",
    "    result = []\n",
    "    for number in range(len(words)):\n",
    "        for start in range(len(words)-number):\n",
    "             result.append(\" \".join(words[start:start+number+1]))\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "phrases('Hi my name is Jacob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('DataEnriching/data.pickle')\n",
    "\n",
    "all_strings = list(data.gov_type)\n",
    "\n",
    "# Counts all ocurrences of a substring \n",
    "all_phrases = collections.Counter(phrase for subject in all_strings for phrase in phrases(subject))\n",
    "\n",
    "# Printing the most common substrings and the number of occurences\n",
    "ocurrences = [(phrase, count) for phrase, count in all_phrases.items() if count > 1]\n",
    "filtered_ocurrences = [ocurrences[i][0] for i in range(len(ocurrences)) if 2 <= len(ocurrences[i][0].split()) <= 3]\n",
    "filtered_ocurrences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the manual checking, the types of government considered where:\n",
    "- parliamentary democracy\n",
    "- parliamentary republic\n",
    "- presidential republic\n",
    "- semi-presidential republic\n",
    "- presidential democracy\n",
    "- absolute monarchy\n",
    "- federal republic\n",
    "- communist state\n",
    "- monarchy\n",
    "- others\n",
    "\n",
    "Missing data is under the category 'unknown'.\n",
    "\n",
    "After all the modifications, the final dataframe with all the data looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the cleaned data\n",
    "Index:\n",
    "- name: common country name in english\n",
    "\n",
    "Features:\n",
    "- area: land area (in $km^2$)\n",
    "- ISO2: code ISO 3166-1 alpha-2\n",
    "- languages: list of official languages\n",
    "- latlng: latitude and longitude\n",
    "- language_codes: list of the official language codes\n",
    "- POP: total number of inhabitants\n",
    "- religion: dictionary of main religions (PCT>10%). Example of value: {rel1: percentage1, ... , relN: percentageN}. Value can be empty dict {} when we did not have the data (see note below)\n",
    "- 2016_gdp_total: total gdp in USD\n",
    "- 2016_gdp_capita: per capita gdp in USD\n",
    "- gov_type: categorical value. List of categories above.\n",
    "- active tweeter users: we did not find this data yet. We explain our solution to this problem in section 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## 5. Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## 6. Critical Assessment\n",
    "\n",
    "- **Fact that twitter is biased by nature**\n",
    "\n",
    "It is very unlikely that the distribution of people that use Twitter in each country will be the same between countries which results in a probable biase due to the fact that we only used Twitter to define the awareness. Ideally, to counteract this effect, we should have scrapped data from different social media.  \n",
    "\n",
    "-  **Location is not always provided.** \n",
    "\n",
    "There are certain countries which are more aware about the risks of providing locations on social media. Therfore when discarding the tweets which had no location provided this adds to the bias. For example, when comparing the number of tweets between two similar countries, one may have been more reactive to an event but with very few users actually providing locations. Therefore the reactions we are measuring are only true for a subset of users. We have to assume that the same proportion of users in the different countries provide their locations so that the results be pertinent.\n",
    "\n",
    "\n",
    "- **Locations are never to be perfect, the location information is not objective**\n",
    "\n",
    "The mapping is not perfect. As there are multiple cities with the same name all over the world, when we obtain a mapping we can never be sure where it came from. Of course there are some locations which are more probable than others which is something we will attempt to address for the next milestone. \n",
    "\n",
    "We could have used the Google API for example, but it is limited in the number of queries, and I won't be perfect either because usually google maps uses contextual infomation to find the location you are looking for. \n",
    "\n",
    "\n",
    "- **Didn't need to use GDELT dataset for the moment**\n",
    "\n",
    "We thought that we would need the GDELT dataset to gather extra information on the events but after searching for these information we found that we didn't have useful features so although we said we would use it, for the moment we can continue without it.\n",
    "\n",
    "- **Didn't find number of active twitter users**\n",
    "\n",
    "This data exists (for example on statista) but to access it we need to pay a yearly subscription fee of 600 dollars. This was not an option as the course was not going to fund it. We asked the TAs whether the lab had access to a similar dataset but it did not seem to be the case. For the time being, this data is not available for the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## 7. What's next ? \n",
    "- Merge retrieved tweets dataframe with the enriching country information.\n",
    "- Define how we are going to normalize the number of tweets of each country given that we didn't find the number of active users in Twitter.\n",
    "- For each event, plot normalized awareness (number of tweets) & time evolution in a choropleth map.\n",
    "\n",
    "\n",
    "- awareness metric\n",
    "- Start thinking about the data story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA]",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
