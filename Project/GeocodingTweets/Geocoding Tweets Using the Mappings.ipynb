{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Geocodes For The Tweets\n",
    "\n",
    "The purpose of this notebook is to create the mapping dictionaries used to recover the tweets that were pickled and to map them to a specific country. The tweets were recovered with their location (when provided by the user). If the tweet did not have a provided location, the location of the user was scraped. However not all users provide this information on their page. That is why all the tweets which did not have either information were dropped. Then the locations provided were mapped to the countries. \n",
    "\n",
    "To map the tweets to their locations we used in order : \n",
    "- Automatic verification of whether the country name or a capital name was contained in the string. This was possible using the data obtained from : https://mledoze.github.io/countries/ and https://datahub.io/core/country-codes. The first links the country iso codes to country names in multiple languages with not only the official but also the common names of a country. The latter links the country iso codes to country names in different languages (arabic, chinese, english, spanish, french, russian). \n",
    "- A city to country mapper from which we removed duplicate cities taken from : https://github.com/lutangar/cities.json \n",
    "- A city to country mapper extracted from : http://www.geonames.org/export/ and http://download.geonames.org/export/dump/. The issue with this dataframe is that the duplicate cities were not handled. They were progressively overwritten. The advantage of this mapper however is that it is more extensive than the previous one, contaning a larger number of cities as well as alternative spellings and different languages. Ideally, what should have been done in the case of multiple cities with same name would be to select based on the population of the cities. \n",
    "- If none of the above yielded any results we queried an API based on the works of http://www.geonames.org/export/, http://geocoder.readthedocs.io/results.html which outputs the most probable location to which the user selected location corresponds to. From that we can recover the ISO country code which can directly be used in the Chloropleth maps. Note that we could not query the API for all the locations as this takes around 1 second per tweet. Given that the number of tweets is in the order of magnitude of the millions this would not have been feasible on the entire dataset.\n",
    "\n",
    "All of this was done using dictionaries to speed up the identification process. Currently, for pickles containing around 2000 tweets, we require under 10 seconds of processing. \n",
    "\n",
    "To create the dictionaries the given locations were set as keys with alternative spellings as well as string formatting to maximize the chance of identifying the country. What is time consuming however is creating the dictionaries themselves which is why the dictionaries were pickled once the process was finished. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String formatting functions, these are the same ones which were used when creating the mappings in the constructing mappings notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/8694815/removing-accent-and-special-characters\n",
    "def remove_accents(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    else :\n",
    "        clean = ''.join(x.lower().strip() for x in unicodedata.normalize('NFKD', data) if \\\n",
    "                unicodedata.category(x)[0] == 'L').lower()\n",
    "        return clean\n",
    "\n",
    "def string_formatting(string):\n",
    "    string = string.replace(\"-\", \" \").replace(\" \", \",\").split(\",\")\n",
    "    formatted_string = [remove_accents(x) for x in string]\n",
    "    return string,formatted_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the different mappings to speed up the geolocalization. Requires about 5GB of RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "folders_path = os.path.join(cwd,\"../../../Project Data\")\n",
    "full_city_mapping_files = [x for x in os.listdir(folders_path) if \"full_city_mapping\" in x]\n",
    "\n",
    "full_city_mappings = list()\n",
    "for file in full_city_mapping_files:\n",
    "    pkl_file = open(os.path.join(folders_path,file), 'rb')\n",
    "    full_city_mappings.append(pickle.load(pkl_file))\n",
    "    pkl_file.close()\n",
    "\n",
    "\n",
    "pkl_file = open(\"country_mapping.pickle\", 'rb')\n",
    "country_mapping = pickle.load(pkl_file)\n",
    "pkl_file.close()\n",
    "\n",
    "pkl_file = open(\"city_mapping.pickle\", 'rb')\n",
    "city_mapping = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to go through the 3 main mappings loaded above and determine the geolocation of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_map = {0:\"Country/Capital\", 1:\"City\"}\n",
    "\n",
    "for i in range(len(full_city_mapping_files)):\n",
    "    idx = i+2\n",
    "    dicts_map.update({idx:\"Full City Mapping\"})\n",
    "\n",
    "def location_in_string(string, do_prints = False):\n",
    "    t = time.time()\n",
    "\n",
    "    if do_prints : print(string)\n",
    "        \n",
    "    words,formatted_words = string_formatting(string)\n",
    "    \n",
    "    words = [x for x in words if len(x)>2]\n",
    "    formatted_words = [x for x in formatted_words if len(x)>2]\n",
    "    \n",
    "    word_combinations = [\" \".join(words[i:j]) for j in range(len(words)+1) for i in range(j)]\n",
    "    word_combinations += [\" \".join(words[i:j]) for j in range(len(formatted_words)+1) for i in range(j)]\n",
    "    \n",
    "    if do_prints : print(words, formatted_words)\n",
    "    if do_prints : print(word_combinations)\n",
    "    \n",
    "    # Test whether the country name and variants is in the string\n",
    "    # Test whether one of the capital names is in the string\n",
    "    # Test whether the name of one of the mapped cities in the string\n",
    "    # All this in the order of priority given \n",
    "    \n",
    "    mappings = [country_mapping, city_mapping] + full_city_mappings\n",
    "    \n",
    "    for m, mapping in enumerate(mappings):\n",
    "\n",
    "        maps = mapping\n",
    "        \n",
    "        for word in word_combinations:\n",
    "            if do_prints : print(\"Testing: \", word)\n",
    "                \n",
    "            if word in maps:\n",
    "                if do_prints : print(\"Found word: \", word,time.time()-t)\n",
    "                return maps[word], dicts_map[m]\n",
    "            \n",
    "            if remove_accents(word) in maps:\n",
    "                if do_prints : print(\"Found word without accents: \", remove_accents(word),time.time()-t)\n",
    "                return maps[remove_accents(word)], dicts_map[m]\n",
    "\n",
    "        \n",
    "    if do_prints : print(\"Nothing found\", time.time()-t)\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the result of the different functions and mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nantes in : ('FR', 'City')\n",
      "Lausanne in : ('CH', 'City')\n",
      "Abu Dhabi in : ('AE', 'Country/Capital')\n",
      "Shanghai in : ('CN', 'City')\n",
      "Beijing in : ('CN', 'Country/Capital')\n",
      "Tokyo in : ('JP', 'Country/Capital')\n",
      "Beijing in  ('CN', 'Country/Capital')\n",
      "Cairo in  ('EG', 'Country/Capital')\n",
      "Paris in  ('FR', 'Country/Capital')\n",
      "Lausanne in  ('CH', 'City')\n",
      "Morges in  ('CH', 'City')\n",
      "Ontario in  ('ES', 'Full City Mapping')\n",
      "Oxford in  ('AU', 'Full City Mapping')\n",
      "Shanghai in  ('CN', 'City')\n",
      "New Castle in  ('AL', 'Full City Mapping')\n",
      "Edinburgh in  ('AU', 'Full City Mapping')\n",
      "Amsterdam in  ('NL', 'Country/Capital')\n",
      "Brussels in  ('BE', 'Country/Capital')\n",
      "Athens in  ('GR', 'Country/Capital')\n",
      "Cork in  ('IE', 'City')\n",
      "Nice in  ('AL', 'Full City Mapping')\n",
      "Dublin in  ('IE', 'Country/Capital')\n",
      "Kuala Lumpur in  ('MY', 'Country/Capital')\n",
      "Madrid in  ('ES', 'Country/Capital')\n",
      "Budapest in  ('HU', 'Country/Capital')\n",
      "Zealand:  ('DK', 'Full City Mapping')\n",
      "Washington :  ('US', 'Country/Capital')\n",
      "cairo :  ('EG', 'Country/Capital')\n",
      "Alexandria :  ('AU', 'Full City Mapping')\n",
      "autriche :  ('AT', 'Country/Capital')\n",
      "oesterreich :  ('AT', 'Country/Capital')\n",
      "osterreich :  ('AT', 'Country/Capital')\n",
      "austria :  ('AT', 'Country/Capital')\n",
      "vienna :  ('AT', 'Country/Capital')\n",
      "Brugges :  (None, None)\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "أفغانستان hello my name is bloop :  ('AF', 'Country/Capital')\n",
      "أفغانستان location is going great :  ('AF', 'Country/Capital')\n",
      "España going to be fun :  ('ES', 'Country/Capital')\n",
      "Hello New Zealand :  ('NZ', 'Country/Capital')\n"
     ]
    }
   ],
   "source": [
    "#location_in_string(\"Milan / Bruxelles\")\n",
    "print(\"Nantes in :\", location_in_string(\"Nantes\"))\n",
    "print(\"Lausanne in :\", location_in_string(\"Lausanne\"))\n",
    "print(\"Abu Dhabi in :\", location_in_string(\"Abu Dhabi\"))\n",
    "print(\"Shanghai in :\", location_in_string(\"Shanghai\"))\n",
    "print(\"Beijing in :\", location_in_string(\"Beijing\"))\n",
    "print(\"Tokyo in :\", location_in_string(\"Tokyo\"))\n",
    "print(\"Beijing in \", location_in_string(\"Beijing\"))\n",
    "print(\"Cairo in \", location_in_string(\"Cairo\"))\n",
    "print(\"Paris in \", location_in_string(\"Paris\"))\n",
    "print(\"Lausanne in \", location_in_string(\"Lausanne\"))\n",
    "print(\"Morges in \", location_in_string(\"Morges\"))\n",
    "print(\"Ontario in \", location_in_string(\"Ontario\"))\n",
    "print(\"Oxford in \",location_in_string(\"Oxford\"))\n",
    "print(\"Shanghai in \", location_in_string(\"Shanghai\"))\n",
    "print(\"New Castle in \", location_in_string(\"New Castle\"))\n",
    "print(\"Edinburgh in \", location_in_string(\"Edinburgh\"))\n",
    "print(\"Amsterdam in \", location_in_string(\"Amsterdam\"))\n",
    "print(\"Brussels in \", location_in_string(\"Brussels\"))\n",
    "print(\"Athens in \", location_in_string(\"Athens\"))\n",
    "print(\"Cork in \", location_in_string(\"Cork\"))\n",
    "print(\"Nice in \", location_in_string(\"Nice\"))\n",
    "print(\"Dublin in \", location_in_string(\"Dublin\"))\n",
    "print(\"Kuala Lumpur in \", location_in_string(\"Kuala Lumpur\"))\n",
    "print(\"Madrid in \", location_in_string(\"Madrid\"))\n",
    "print(\"Budapest in \", location_in_string(\"Budapest\"))\n",
    "print(\"Zealand: \", location_in_string(\"Zealand\"))\n",
    "print(\"Washington : \", location_in_string(\"Washington\"))\n",
    "print(\"cairo : \", location_in_string(\"cairo\"))\n",
    "print(\"Alexandria : \", location_in_string(\"Alexandria\"))\n",
    "print(\"autriche : \", location_in_string(\"autriche\"))\n",
    "print(\"oesterreich : \", location_in_string(\"oesterreich\"))\n",
    "print(\"osterreich : \", location_in_string(\"osterreich\"))\n",
    "print(\"austria : \", location_in_string(\"austria\"))\n",
    "print(\"vienna : \", location_in_string(\"vienna\"))\n",
    "print(\"Brugges : \", location_in_string(\"brugges\"))\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"---------------------------------------------\")\n",
    "print(\"أفغانستان hello my name is bloop : \", location_in_string(\"أفغانستان hello my name is bloop\"))\n",
    "print(\"أفغانستان location is going great : \", location_in_string(\"أفغانستان is a great place to be\"))\n",
    "print(\"España going to be fun : \", location_in_string(\"España going to be fun\"))\n",
    "print(\"Hello New Zealand : \", location_in_string(\"Hello New Zealand\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to extract the location for all elements in a pickled dataframe and store them in a new folder. This uses all the previously coded functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = list()\n",
    "def extract_geocodes_for_pickle(folder, pickle_file, do_prints = False):\n",
    "    try:\n",
    "        if do_prints : print(pickle_file);\n",
    "        df = pd.read_pickle(os.path.join(path, folder,pickle_file))\n",
    "        if do_prints : print(\"Successfully loaded\", pickle_file);\n",
    "        # Putting the location in the correct format\n",
    "        df[\"location\"] = df[\"location\"].apply(lambda x: ' '.join(x))\n",
    "        # Dropping rows without locations\n",
    "        df = df[df['location'].map(len) > 0] \n",
    "        if do_prints : print(\"Dropped rows without locations\", pickle_file);\n",
    "        # Mapping the locations to countries\n",
    "        df[\"number\"] =  1\n",
    "        df[\"country\"] = df[\"location\"].apply(lambda x: location_in_string(x)[0])\n",
    "        df[\"source\"] = df[\"location\"].apply(lambda x: location_in_string(x)[1])\n",
    "        # Pickling the dataframe\n",
    "        df.to_pickle(os.path.join(path, folder, \"Geocoded\", pickle_file))\n",
    "        if do_prints : print(df[[\"location\", \"country\"]].head(10))\n",
    "        if do_prints : print(df.groupby(\"country\").count()[\"text\"])\n",
    "    except:\n",
    "        print(\"Failure :\", folder, pickle_file)\n",
    "        failures.append([folder, pickle_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the folders containing the pickle files and load the mappings and calling the function above to extract the dataframe from the pickle file, map all the locations in the dataframe and save the result in a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/333 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/333 [00:02<13:46,  2.49s/it]\u001b[A\n",
      "  1%|          | 2/333 [00:05<13:56,  2.53s/it]\u001b[A\n",
      "  1%|          | 3/333 [00:07<13:24,  2.44s/it]\u001b[A\n",
      "  1%|          | 4/333 [00:09<13:04,  2.39s/it]\u001b[A\n",
      "  2%|▏         | 5/333 [00:12<13:10,  2.41s/it]\u001b[A\n",
      "  2%|▏         | 6/333 [00:14<13:27,  2.47s/it]\u001b[A\n",
      "  2%|▏         | 7/333 [00:17<13:16,  2.44s/it]\u001b[A\n",
      "  2%|▏         | 8/333 [00:19<13:07,  2.42s/it]\u001b[A\n",
      "  3%|▎         | 9/333 [00:21<12:58,  2.40s/it]\u001b[A\n",
      "  3%|▎         | 10/333 [00:23<12:48,  2.38s/it]\u001b[A\n",
      "  3%|▎         | 11/333 [00:25<12:33,  2.34s/it]\u001b[A\n",
      "  4%|▎         | 12/333 [00:28<12:40,  2.37s/it]\u001b[A\n",
      "  4%|▍         | 13/333 [00:31<12:58,  2.43s/it]\u001b[A\n",
      "  4%|▍         | 14/333 [00:33<12:52,  2.42s/it]\u001b[A\n",
      "  5%|▍         | 15/333 [00:35<12:39,  2.39s/it]\u001b[A\n",
      "  5%|▍         | 16/333 [00:38<12:34,  2.38s/it]\u001b[A\n",
      "  5%|▌         | 17/333 [00:40<12:30,  2.38s/it]\u001b[A\n",
      "  5%|▌         | 18/333 [00:42<12:21,  2.35s/it]\u001b[A\n",
      "  6%|▌         | 19/333 [00:44<12:18,  2.35s/it]\u001b[A\n",
      "  6%|▌         | 20/333 [00:46<12:09,  2.33s/it]\u001b[A\n",
      "  6%|▋         | 21/333 [00:49<12:14,  2.35s/it]\u001b[A\n",
      "  7%|▋         | 22/333 [00:51<12:11,  2.35s/it]\u001b[A\n",
      "  7%|▋         | 23/333 [00:54<12:13,  2.37s/it]\u001b[A\n",
      "  7%|▋         | 24/333 [00:57<12:15,  2.38s/it]\u001b[A\n",
      "  8%|▊         | 25/333 [00:59<12:13,  2.38s/it]\u001b[A\n",
      "  8%|▊         | 26/333 [01:02<12:15,  2.40s/it]\u001b[A\n",
      "  8%|▊         | 27/333 [01:04<12:14,  2.40s/it]\u001b[A\n",
      "  8%|▊         | 28/333 [01:07<12:12,  2.40s/it]\u001b[A\n",
      "  9%|▊         | 29/333 [01:10<12:16,  2.42s/it]\u001b[A\n",
      "  9%|▉         | 30/333 [01:12<12:16,  2.43s/it]\u001b[A\n",
      "  9%|▉         | 31/333 [01:15<12:15,  2.43s/it]\u001b[A\n",
      " 10%|▉         | 32/333 [01:17<12:08,  2.42s/it]\u001b[A\n",
      " 10%|▉         | 33/333 [01:19<12:04,  2.41s/it]\u001b[A\n",
      " 10%|█         | 34/333 [01:21<11:58,  2.40s/it]\u001b[A\n",
      " 11%|█         | 35/333 [01:24<12:02,  2.42s/it]\u001b[A\n",
      " 11%|█         | 36/333 [01:28<12:06,  2.45s/it]\u001b[A\n",
      " 11%|█         | 37/333 [01:30<12:04,  2.45s/it]\u001b[A\n",
      " 11%|█▏        | 38/333 [01:33<12:02,  2.45s/it]\u001b[A\n",
      " 12%|█▏        | 39/333 [01:35<12:02,  2.46s/it]\u001b[A\n",
      " 12%|█▏        | 40/333 [01:38<11:58,  2.45s/it]\u001b[A\n",
      " 12%|█▏        | 41/333 [01:40<11:53,  2.44s/it]\u001b[A\n",
      " 13%|█▎        | 42/333 [01:42<11:49,  2.44s/it]\u001b[A\n",
      " 13%|█▎        | 43/333 [01:44<11:44,  2.43s/it]\u001b[A\n",
      " 13%|█▎        | 44/333 [01:46<11:39,  2.42s/it]\u001b[A\n",
      " 14%|█▎        | 45/333 [01:48<11:35,  2.42s/it]\u001b[A\n",
      " 14%|█▍        | 46/333 [01:51<11:34,  2.42s/it]\u001b[A\n",
      " 14%|█▍        | 47/333 [01:53<11:31,  2.42s/it]\u001b[A\n",
      " 14%|█▍        | 48/333 [01:55<11:27,  2.41s/it]\u001b[A\n",
      " 15%|█▍        | 49/333 [01:58<11:26,  2.42s/it]\u001b[A\n",
      " 15%|█▌        | 50/333 [02:01<11:27,  2.43s/it]\u001b[A\n",
      " 15%|█▌        | 51/333 [02:05<11:33,  2.46s/it]\u001b[A\n",
      " 16%|█▌        | 52/333 [02:08<11:34,  2.47s/it]\u001b[A\n",
      " 16%|█▌        | 53/333 [02:10<11:30,  2.47s/it]\u001b[A\n",
      " 16%|█▌        | 54/333 [02:13<11:29,  2.47s/it]\u001b[A\n",
      " 17%|█▋        | 55/333 [02:15<11:25,  2.47s/it]\u001b[A\n",
      " 17%|█▋        | 56/333 [02:17<11:21,  2.46s/it]\u001b[A\n",
      " 17%|█▋        | 57/333 [02:20<11:20,  2.47s/it]\u001b[A\n",
      " 17%|█▋        | 58/333 [02:23<11:19,  2.47s/it]\u001b[A\n",
      " 18%|█▊        | 59/333 [02:25<11:16,  2.47s/it]\u001b[A\n",
      " 18%|█▊        | 60/333 [02:27<11:12,  2.46s/it]\u001b[A\n",
      " 18%|█▊        | 61/333 [02:30<11:09,  2.46s/it]\u001b[A\n",
      " 19%|█▊        | 62/333 [02:32<11:05,  2.45s/it]\u001b[A\n",
      " 19%|█▉        | 63/333 [02:34<11:00,  2.45s/it]\u001b[A\n",
      " 19%|█▉        | 64/333 [02:36<10:57,  2.44s/it]\u001b[A\n",
      " 20%|█▉        | 65/333 [02:38<10:53,  2.44s/it]\u001b[A\n",
      " 20%|█▉        | 66/333 [02:41<10:51,  2.44s/it]\u001b[A\n",
      " 20%|██        | 67/333 [02:42<10:46,  2.43s/it]\u001b[A\n",
      " 20%|██        | 68/333 [02:45<10:43,  2.43s/it]\u001b[A\n",
      " 21%|██        | 69/333 [02:47<10:40,  2.42s/it]\u001b[A\n",
      " 21%|██        | 70/333 [02:49<10:37,  2.42s/it]\u001b[A\n",
      " 21%|██▏       | 71/333 [02:51<10:32,  2.42s/it]\u001b[A\n",
      " 22%|██▏       | 72/333 [02:53<10:28,  2.41s/it]\u001b[A\n",
      " 22%|██▏       | 73/333 [02:55<10:24,  2.40s/it]\u001b[A\n",
      " 22%|██▏       | 74/333 [02:56<10:19,  2.39s/it]\u001b[A\n",
      " 23%|██▎       | 75/333 [02:58<10:15,  2.38s/it]\u001b[A\n",
      " 23%|██▎       | 76/333 [03:00<10:11,  2.38s/it]\u001b[A\n",
      " 23%|██▎       | 77/333 [03:02<10:07,  2.37s/it]\u001b[A\n",
      " 23%|██▎       | 78/333 [03:04<10:03,  2.37s/it]\u001b[A\n",
      " 24%|██▎       | 79/333 [03:06<09:59,  2.36s/it]\u001b[A\n",
      " 24%|██▍       | 80/333 [03:08<09:55,  2.35s/it]\u001b[A\n",
      " 24%|██▍       | 81/333 [03:10<09:52,  2.35s/it]\u001b[A\n",
      " 25%|██▍       | 82/333 [03:12<09:50,  2.35s/it]\u001b[A\n",
      " 25%|██▍       | 83/333 [03:15<09:47,  2.35s/it]\u001b[A\n",
      " 25%|██▌       | 84/333 [03:17<09:44,  2.35s/it]\u001b[A\n",
      " 26%|██▌       | 85/333 [03:19<09:41,  2.34s/it]\u001b[A\n",
      " 26%|██▌       | 86/333 [03:21<09:38,  2.34s/it]\u001b[A\n",
      " 26%|██▌       | 87/333 [03:23<09:35,  2.34s/it]\u001b[A\n",
      " 26%|██▋       | 88/333 [03:25<09:33,  2.34s/it]\u001b[A\n",
      " 27%|██▋       | 89/333 [03:27<09:29,  2.33s/it]\u001b[A\n",
      " 27%|██▋       | 90/333 [03:29<09:26,  2.33s/it]\u001b[A\n",
      " 27%|██▋       | 91/333 [03:31<09:22,  2.33s/it]\u001b[A\n",
      " 28%|██▊       | 92/333 [03:33<09:18,  2.32s/it]\u001b[A\n",
      " 28%|██▊       | 93/333 [03:35<09:14,  2.31s/it]\u001b[A\n",
      " 28%|██▊       | 94/333 [03:36<09:11,  2.31s/it]\u001b[A\n",
      " 29%|██▊       | 95/333 [03:38<09:07,  2.30s/it]\u001b[A\n",
      " 29%|██▉       | 96/333 [03:40<09:03,  2.29s/it]\u001b[A\n",
      " 29%|██▉       | 97/333 [03:41<08:59,  2.29s/it]\u001b[A\n",
      " 29%|██▉       | 98/333 [03:44<08:57,  2.29s/it]\u001b[A\n",
      " 30%|██▉       | 99/333 [03:45<08:53,  2.28s/it]\u001b[A\n",
      " 30%|███       | 100/333 [03:47<08:50,  2.27s/it]\u001b[A\n",
      " 30%|███       | 101/333 [03:49<08:47,  2.27s/it]\u001b[A\n",
      " 31%|███       | 102/333 [03:51<08:43,  2.27s/it]\u001b[A\n",
      " 31%|███       | 103/333 [03:52<08:39,  2.26s/it]\u001b[A\n",
      " 31%|███       | 104/333 [03:54<08:36,  2.26s/it]\u001b[A\n",
      " 32%|███▏      | 105/333 [03:56<08:32,  2.25s/it]\u001b[A\n",
      " 32%|███▏      | 106/333 [03:57<08:29,  2.24s/it]\u001b[A\n",
      " 32%|███▏      | 107/333 [03:58<08:24,  2.23s/it]\u001b[A\n",
      " 32%|███▏      | 108/333 [04:00<08:20,  2.22s/it]\u001b[A\n",
      " 33%|███▎      | 109/333 [04:01<08:16,  2.22s/it]\u001b[A\n",
      " 33%|███▎      | 110/333 [04:03<08:13,  2.21s/it]\u001b[A\n",
      " 33%|███▎      | 111/333 [04:05<08:10,  2.21s/it]\u001b[A\n",
      " 34%|███▎      | 112/333 [04:07<08:07,  2.21s/it]\u001b[A\n",
      " 34%|███▍      | 113/333 [04:08<08:04,  2.20s/it]\u001b[A\n",
      " 34%|███▍      | 114/333 [04:10<08:01,  2.20s/it]\u001b[A\n",
      " 35%|███▍      | 115/333 [04:12<07:58,  2.20s/it]\u001b[A\n",
      " 35%|███▍      | 116/333 [04:15<07:57,  2.20s/it]\u001b[A\n",
      " 35%|███▌      | 117/333 [04:18<07:56,  2.21s/it]\u001b[A\n",
      " 35%|███▌      | 118/333 [04:20<07:54,  2.21s/it]\u001b[A\n",
      " 36%|███▌      | 119/333 [04:23<07:53,  2.21s/it]\u001b[A\n",
      " 36%|███▌      | 120/333 [04:25<07:51,  2.21s/it]\u001b[A\n",
      " 36%|███▋      | 121/333 [04:28<07:51,  2.22s/it]\u001b[A\n",
      " 37%|███▋      | 122/333 [04:31<07:50,  2.23s/it]\u001b[A\n",
      " 37%|███▋      | 123/333 [04:35<07:50,  2.24s/it]\u001b[A\n",
      " 37%|███▋      | 124/333 [04:37<07:48,  2.24s/it]\u001b[A\n",
      " 38%|███▊      | 125/333 [04:41<07:48,  2.25s/it]\u001b[A\n",
      " 38%|███▊      | 126/333 [04:44<07:46,  2.25s/it]\u001b[A\n",
      " 38%|███▊      | 127/333 [04:46<07:44,  2.26s/it]\u001b[A\n",
      " 38%|███▊      | 128/333 [04:49<07:43,  2.26s/it]\u001b[A\n",
      " 39%|███▊      | 129/333 [04:51<07:41,  2.26s/it]\u001b[A\n",
      " 39%|███▉      | 130/333 [04:54<07:39,  2.26s/it]\u001b[A\n",
      " 39%|███▉      | 131/333 [04:56<07:36,  2.26s/it]\u001b[A\n",
      " 40%|███▉      | 132/333 [04:58<07:34,  2.26s/it]\u001b[A\n",
      " 40%|███▉      | 133/333 [05:00<07:31,  2.26s/it]\u001b[A\n",
      " 40%|████      | 134/333 [05:02<07:28,  2.26s/it]\u001b[A\n",
      " 41%|████      | 135/333 [05:04<07:26,  2.26s/it]\u001b[A\n",
      " 41%|████      | 136/333 [05:06<07:24,  2.25s/it]\u001b[A\n",
      " 41%|████      | 137/333 [05:08<07:21,  2.25s/it]\u001b[A\n",
      " 41%|████▏     | 138/333 [05:10<07:18,  2.25s/it]\u001b[A\n",
      " 42%|████▏     | 139/333 [05:12<07:16,  2.25s/it]\u001b[A\n",
      " 42%|████▏     | 140/333 [05:14<07:14,  2.25s/it]\u001b[A\n",
      " 42%|████▏     | 141/333 [05:16<07:11,  2.25s/it]\u001b[A\n",
      " 43%|████▎     | 142/333 [05:18<07:08,  2.24s/it]\u001b[A\n",
      " 43%|████▎     | 143/333 [05:20<07:05,  2.24s/it]\u001b[A\n",
      " 43%|████▎     | 144/333 [05:22<07:02,  2.24s/it]\u001b[A\n",
      " 44%|████▎     | 145/333 [05:23<06:59,  2.23s/it]\u001b[A\n",
      " 44%|████▍     | 146/333 [05:25<06:57,  2.23s/it]\u001b[A\n",
      " 44%|████▍     | 147/333 [05:27<06:54,  2.23s/it]\u001b[A\n",
      " 44%|████▍     | 148/333 [05:29<06:52,  2.23s/it]\u001b[A\n",
      " 45%|████▍     | 149/333 [05:31<06:49,  2.23s/it]\u001b[A\n",
      " 45%|████▌     | 150/333 [05:33<06:47,  2.22s/it]\u001b[A\n",
      " 45%|████▌     | 151/333 [05:35<06:44,  2.22s/it]\u001b[A\n",
      " 46%|████▌     | 152/333 [05:37<06:42,  2.22s/it]\u001b[A\n",
      " 46%|████▌     | 153/333 [05:39<06:39,  2.22s/it]\u001b[A\n",
      " 46%|████▌     | 154/333 [05:41<06:37,  2.22s/it]\u001b[A\n",
      " 47%|████▋     | 155/333 [05:43<06:34,  2.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 156/333 [05:45<06:32,  2.22s/it]\u001b[A\n",
      " 47%|████▋     | 157/333 [05:48<06:30,  2.22s/it]\u001b[A\n",
      " 47%|████▋     | 158/333 [05:50<06:28,  2.22s/it]\u001b[A\n",
      " 48%|████▊     | 159/333 [05:52<06:26,  2.22s/it]\u001b[A\n",
      " 48%|████▊     | 160/333 [05:55<06:23,  2.22s/it]\u001b[A\n",
      " 48%|████▊     | 161/333 [05:57<06:21,  2.22s/it]\u001b[A\n",
      " 49%|████▊     | 162/333 [05:59<06:19,  2.22s/it]\u001b[A\n",
      " 49%|████▉     | 163/333 [06:01<06:17,  2.22s/it]\u001b[A\n",
      " 49%|████▉     | 164/333 [06:03<06:14,  2.22s/it]\u001b[A\n",
      " 50%|████▉     | 165/333 [06:05<06:12,  2.21s/it]\u001b[A\n",
      " 50%|████▉     | 166/333 [06:07<06:09,  2.21s/it]\u001b[A\n",
      " 50%|█████     | 167/333 [06:09<06:06,  2.21s/it]\u001b[A\n",
      " 50%|█████     | 168/333 [06:10<06:04,  2.21s/it]\u001b[A\n",
      " 51%|█████     | 169/333 [06:12<06:01,  2.20s/it]\u001b[A\n",
      " 51%|█████     | 170/333 [06:14<05:58,  2.20s/it]\u001b[A\n",
      " 51%|█████▏    | 171/333 [06:15<05:56,  2.20s/it]\u001b[A\n",
      " 52%|█████▏    | 172/333 [06:17<05:53,  2.20s/it]\u001b[A\n",
      " 52%|█████▏    | 173/333 [06:19<05:51,  2.19s/it]\u001b[A\n",
      " 52%|█████▏    | 174/333 [06:21<05:48,  2.19s/it]\u001b[A\n",
      " 53%|█████▎    | 175/333 [06:22<05:45,  2.19s/it]\u001b[A\n",
      " 53%|█████▎    | 176/333 [06:24<05:43,  2.19s/it]\u001b[A\n",
      " 53%|█████▎    | 177/333 [06:27<05:41,  2.19s/it]\u001b[A\n",
      " 53%|█████▎    | 178/333 [06:29<05:38,  2.19s/it]\u001b[A\n",
      " 54%|█████▍    | 179/333 [06:31<05:36,  2.19s/it]\u001b[A\n",
      " 54%|█████▍    | 180/333 [06:33<05:34,  2.19s/it]\u001b[A\n",
      " 54%|█████▍    | 181/333 [06:36<05:32,  2.19s/it]\u001b[A\n",
      " 55%|█████▍    | 182/333 [06:38<05:30,  2.19s/it]\u001b[A\n",
      " 55%|█████▍    | 183/333 [06:41<05:28,  2.19s/it]\u001b[A\n",
      " 55%|█████▌    | 184/333 [06:43<05:26,  2.19s/it]\u001b[A\n",
      " 56%|█████▌    | 185/333 [06:46<05:24,  2.20s/it]\u001b[A\n",
      " 56%|█████▌    | 186/333 [06:48<05:22,  2.19s/it]\u001b[A\n",
      " 56%|█████▌    | 187/333 [06:49<05:20,  2.19s/it]\u001b[A\n",
      " 56%|█████▋    | 188/333 [06:51<05:17,  2.19s/it]\u001b[A\n",
      " 57%|█████▋    | 189/333 [06:53<05:15,  2.19s/it]\u001b[A\n",
      " 57%|█████▋    | 190/333 [06:56<05:13,  2.19s/it]\u001b[A\n",
      " 57%|█████▋    | 191/333 [06:57<05:10,  2.19s/it]\u001b[A\n",
      " 58%|█████▊    | 192/333 [06:59<05:08,  2.19s/it]\u001b[A\n",
      " 58%|█████▊    | 193/333 [07:01<05:05,  2.18s/it]\u001b[A\n",
      " 58%|█████▊    | 194/333 [07:03<05:03,  2.18s/it]\u001b[A\n",
      " 59%|█████▊    | 195/333 [07:05<05:01,  2.18s/it]\u001b[A\n",
      " 59%|█████▉    | 196/333 [07:07<04:58,  2.18s/it]\u001b[A\n",
      " 59%|█████▉    | 197/333 [07:09<04:56,  2.18s/it]\u001b[A\n",
      " 59%|█████▉    | 198/333 [07:11<04:54,  2.18s/it]\u001b[A\n",
      " 60%|█████▉    | 199/333 [07:13<04:52,  2.18s/it]\u001b[A\n",
      " 60%|██████    | 200/333 [07:15<04:49,  2.18s/it]\u001b[A\n",
      " 60%|██████    | 201/333 [07:17<04:47,  2.18s/it]\u001b[A\n",
      " 61%|██████    | 202/333 [07:20<04:45,  2.18s/it]\u001b[A\n",
      " 61%|██████    | 203/333 [07:22<04:43,  2.18s/it]\u001b[A\n",
      " 61%|██████▏   | 204/333 [07:24<04:40,  2.18s/it]\u001b[A\n",
      " 62%|██████▏   | 205/333 [07:25<04:37,  2.17s/it]\u001b[A\n",
      " 62%|██████▏   | 206/333 [07:26<04:35,  2.17s/it]\u001b[A\n",
      " 62%|██████▏   | 207/333 [07:28<04:33,  2.17s/it]\u001b[A\n",
      " 62%|██████▏   | 208/333 [07:30<04:30,  2.17s/it]\u001b[A\n",
      " 63%|██████▎   | 209/333 [07:32<04:28,  2.17s/it]\u001b[A\n",
      " 63%|██████▎   | 210/333 [07:34<04:26,  2.17s/it]\u001b[A\n",
      " 63%|██████▎   | 211/333 [07:37<04:24,  2.17s/it]\u001b[A\n",
      " 64%|██████▎   | 212/333 [07:39<04:22,  2.17s/it]\u001b[A\n",
      " 64%|██████▍   | 213/333 [07:42<04:20,  2.17s/it]\u001b[A\n",
      " 64%|██████▍   | 214/333 [07:44<04:18,  2.17s/it]\u001b[A\n",
      " 65%|██████▍   | 215/333 [07:47<04:16,  2.17s/it]\u001b[A\n",
      " 65%|██████▌   | 217/333 [07:49<04:10,  2.16s/it]\u001b[A\n",
      " 65%|██████▌   | 218/333 [07:51<04:08,  2.16s/it]\u001b[A\n",
      " 66%|██████▌   | 219/333 [07:53<04:06,  2.16s/it]\u001b[A\n",
      " 66%|██████▌   | 220/333 [07:55<04:04,  2.16s/it]\u001b[A\n",
      " 66%|██████▋   | 221/333 [07:57<04:01,  2.16s/it]\u001b[A\n",
      " 67%|██████▋   | 222/333 [07:59<03:59,  2.16s/it]\u001b[A\n",
      " 67%|██████▋   | 223/333 [07:59<03:56,  2.15s/it]\u001b[A\n",
      " 67%|██████▋   | 224/333 [08:01<03:54,  2.15s/it]\u001b[A\n",
      " 68%|██████▊   | 225/333 [08:03<03:52,  2.15s/it]\u001b[A\n",
      " 68%|██████▊   | 226/333 [08:06<03:50,  2.15s/it]\u001b[A\n",
      " 68%|██████▊   | 227/333 [08:08<03:47,  2.15s/it]\u001b[A\n",
      " 68%|██████▊   | 228/333 [08:10<03:45,  2.15s/it]\u001b[A\n",
      " 69%|██████▉   | 229/333 [08:12<03:43,  2.15s/it]\u001b[A\n",
      " 69%|██████▉   | 230/333 [08:14<03:41,  2.15s/it]\u001b[A\n",
      " 69%|██████▉   | 231/333 [08:15<03:38,  2.15s/it]\u001b[A\n",
      " 70%|██████▉   | 232/333 [08:17<03:36,  2.15s/it]\u001b[A\n",
      " 70%|██████▉   | 233/333 [08:19<03:34,  2.14s/it]\u001b[A\n",
      " 70%|███████   | 234/333 [08:21<03:32,  2.14s/it]\u001b[A\n",
      " 71%|███████   | 235/333 [08:22<03:29,  2.14s/it]\u001b[A\n",
      " 71%|███████   | 236/333 [08:24<03:27,  2.14s/it]\u001b[A\n",
      " 71%|███████   | 237/333 [08:26<03:25,  2.14s/it]\u001b[A\n",
      " 71%|███████▏  | 238/333 [08:28<03:23,  2.14s/it]\u001b[A\n",
      " 72%|███████▏  | 239/333 [08:30<03:20,  2.14s/it]\u001b[A\n",
      " 72%|███████▏  | 240/333 [08:32<03:18,  2.14s/it]\u001b[A\n",
      " 72%|███████▏  | 241/333 [08:34<03:16,  2.14s/it]\u001b[A\n",
      " 73%|███████▎  | 242/333 [08:36<03:14,  2.14s/it]\u001b[A\n",
      " 73%|███████▎  | 243/333 [08:38<03:12,  2.14s/it]\u001b[A\n",
      " 73%|███████▎  | 244/333 [08:41<03:10,  2.14s/it]\u001b[A\n",
      " 74%|███████▎  | 245/333 [08:44<03:08,  2.14s/it]\u001b[A\n",
      " 74%|███████▍  | 246/333 [08:46<03:06,  2.14s/it]\u001b[A\n",
      " 74%|███████▍  | 247/333 [08:49<03:04,  2.14s/it]\u001b[A\n",
      " 74%|███████▍  | 248/333 [08:51<03:02,  2.14s/it]\u001b[A\n",
      " 75%|███████▍  | 249/333 [08:51<02:59,  2.14s/it]\u001b[A\n",
      " 75%|███████▌  | 250/333 [08:54<02:57,  2.14s/it]\u001b[A\n",
      " 75%|███████▌  | 251/333 [08:56<02:55,  2.14s/it]\u001b[A\n",
      " 76%|███████▌  | 252/333 [08:58<02:53,  2.14s/it]\u001b[A\n",
      " 76%|███████▌  | 253/333 [09:01<02:51,  2.14s/it]\u001b[A\n",
      " 76%|███████▋  | 254/333 [09:03<02:48,  2.14s/it]\u001b[A\n",
      " 77%|███████▋  | 255/333 [09:05<02:46,  2.14s/it]\u001b[A\n",
      " 77%|███████▋  | 256/333 [09:07<02:44,  2.14s/it]\u001b[A\n",
      " 77%|███████▋  | 257/333 [09:08<02:42,  2.14s/it]\u001b[A\n",
      " 77%|███████▋  | 258/333 [09:10<02:40,  2.13s/it]\u001b[A\n",
      " 78%|███████▊  | 259/333 [09:12<02:37,  2.13s/it]\u001b[A\n",
      " 78%|███████▊  | 260/333 [09:14<02:35,  2.13s/it]\u001b[A\n",
      " 78%|███████▊  | 261/333 [09:15<02:33,  2.13s/it]\u001b[A\n",
      " 79%|███████▊  | 262/333 [09:17<02:31,  2.13s/it]\u001b[A\n",
      " 79%|███████▉  | 263/333 [09:19<02:28,  2.13s/it]\u001b[A\n",
      " 79%|███████▉  | 264/333 [09:21<02:26,  2.13s/it]\u001b[A\n",
      " 80%|███████▉  | 265/333 [09:23<02:24,  2.13s/it]\u001b[A\n",
      " 80%|███████▉  | 266/333 [09:25<02:22,  2.12s/it]\u001b[A\n",
      " 80%|████████  | 267/333 [09:26<02:20,  2.12s/it]\u001b[A\n",
      " 80%|████████  | 268/333 [09:29<02:18,  2.12s/it]\u001b[A\n",
      " 81%|████████  | 269/333 [09:30<02:15,  2.12s/it]\u001b[A\n",
      " 81%|████████  | 270/333 [09:32<02:13,  2.12s/it]\u001b[A\n",
      " 81%|████████▏ | 271/333 [09:34<02:11,  2.12s/it]\u001b[A\n",
      " 82%|████████▏ | 272/333 [09:36<02:09,  2.12s/it]\u001b[A\n",
      " 82%|████████▏ | 273/333 [09:38<02:07,  2.12s/it]\u001b[A\n",
      " 82%|████████▏ | 274/333 [09:40<02:04,  2.12s/it]\u001b[A\n",
      " 83%|████████▎ | 275/333 [09:42<02:02,  2.12s/it]\u001b[A\n",
      " 83%|████████▎ | 276/333 [09:44<02:00,  2.12s/it]\u001b[A\n",
      " 83%|████████▎ | 277/333 [09:46<01:58,  2.12s/it]\u001b[A\n",
      " 83%|████████▎ | 278/333 [09:48<01:56,  2.12s/it]\u001b[A\n",
      " 84%|████████▍ | 279/333 [09:50<01:54,  2.12s/it]\u001b[A\n",
      " 84%|████████▍ | 280/333 [09:52<01:52,  2.12s/it]\u001b[A\n",
      " 84%|████████▍ | 281/333 [09:54<01:50,  2.12s/it]\u001b[A\n",
      " 85%|████████▍ | 282/333 [09:56<01:47,  2.12s/it]\u001b[A\n",
      " 85%|████████▍ | 283/333 [09:58<01:45,  2.12s/it]\u001b[A\n",
      " 85%|████████▌ | 284/333 [10:00<01:43,  2.12s/it]\u001b[A\n",
      " 86%|████████▌ | 285/333 [10:02<01:41,  2.11s/it]\u001b[A\n",
      " 86%|████████▌ | 286/333 [10:04<01:39,  2.11s/it]\u001b[A\n",
      " 86%|████████▌ | 287/333 [10:06<01:37,  2.11s/it]\u001b[A\n",
      " 86%|████████▋ | 288/333 [10:08<01:35,  2.11s/it]\u001b[A\n",
      " 87%|████████▋ | 289/333 [10:10<01:32,  2.11s/it]\u001b[A\n",
      " 87%|████████▋ | 290/333 [10:11<01:30,  2.11s/it]\u001b[A\n",
      " 87%|████████▋ | 291/333 [10:13<01:28,  2.11s/it]\u001b[A\n",
      " 88%|████████▊ | 292/333 [10:15<01:26,  2.11s/it]\u001b[A\n",
      " 88%|████████▊ | 293/333 [10:17<01:24,  2.11s/it]\u001b[A\n",
      " 88%|████████▊ | 294/333 [10:19<01:22,  2.11s/it]\u001b[A\n",
      " 89%|████████▊ | 295/333 [10:21<01:19,  2.11s/it]\u001b[A\n",
      " 89%|████████▉ | 296/333 [10:22<01:17,  2.10s/it]\u001b[A\n",
      " 89%|████████▉ | 297/333 [10:24<01:15,  2.10s/it]\u001b[A\n",
      " 89%|████████▉ | 298/333 [10:26<01:13,  2.10s/it]\u001b[A\n",
      " 90%|████████▉ | 299/333 [10:28<01:11,  2.10s/it]\u001b[A\n",
      " 90%|█████████ | 300/333 [10:30<01:09,  2.10s/it]\u001b[A\n",
      " 90%|█████████ | 301/333 [10:32<01:07,  2.10s/it]\u001b[A\n",
      " 91%|█████████ | 302/333 [10:34<01:05,  2.10s/it]\u001b[A\n",
      " 91%|█████████ | 303/333 [10:36<01:02,  2.10s/it]\u001b[A\n",
      " 91%|█████████▏| 304/333 [10:37<01:00,  2.10s/it]\u001b[A\n",
      " 92%|█████████▏| 305/333 [10:39<00:58,  2.10s/it]\u001b[A\n",
      " 92%|█████████▏| 306/333 [10:41<00:56,  2.10s/it]\u001b[A\n",
      " 92%|█████████▏| 307/333 [10:43<00:54,  2.10s/it]\u001b[A\n",
      " 92%|█████████▏| 308/333 [10:45<00:52,  2.10s/it]\u001b[A\n",
      " 93%|█████████▎| 309/333 [10:47<00:50,  2.10s/it]\u001b[A\n",
      " 93%|█████████▎| 310/333 [10:49<00:48,  2.10s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 311/333 [10:52<00:46,  2.10s/it]\u001b[A\n",
      " 94%|█████████▎| 312/333 [10:54<00:44,  2.10s/it]\u001b[A\n",
      " 94%|█████████▍| 313/333 [10:56<00:41,  2.10s/it]\u001b[A\n",
      " 94%|█████████▍| 314/333 [10:59<00:39,  2.10s/it]\u001b[A\n",
      " 95%|█████████▍| 315/333 [11:01<00:37,  2.10s/it]\u001b[A\n",
      " 95%|█████████▍| 316/333 [11:04<00:35,  2.10s/it]\u001b[A\n",
      " 95%|█████████▌| 317/333 [11:06<00:33,  2.10s/it]\u001b[A\n",
      " 95%|█████████▌| 318/333 [11:08<00:31,  2.10s/it]\u001b[A\n",
      " 96%|█████████▌| 319/333 [11:11<00:29,  2.10s/it]\u001b[A\n",
      " 96%|█████████▌| 320/333 [11:13<00:27,  2.10s/it]\u001b[A\n",
      " 96%|█████████▋| 321/333 [11:14<00:25,  2.10s/it]\u001b[A\n",
      " 97%|█████████▋| 322/333 [11:16<00:23,  2.10s/it]\u001b[A\n",
      " 97%|█████████▋| 323/333 [11:18<00:21,  2.10s/it]\u001b[A\n",
      " 97%|█████████▋| 324/333 [11:20<00:18,  2.10s/it]\u001b[A\n",
      " 98%|█████████▊| 325/333 [11:22<00:16,  2.10s/it]\u001b[A\n",
      " 98%|█████████▊| 326/333 [11:24<00:14,  2.10s/it]\u001b[A\n",
      " 98%|█████████▊| 327/333 [11:26<00:12,  2.10s/it]\u001b[A\n",
      " 98%|█████████▊| 328/333 [11:28<00:10,  2.10s/it]\u001b[A\n",
      " 99%|█████████▉| 329/333 [11:30<00:08,  2.10s/it]\u001b[A\n",
      " 99%|█████████▉| 330/333 [11:32<00:06,  2.10s/it]\u001b[A\n",
      " 99%|█████████▉| 331/333 [11:33<00:04,  2.10s/it]\u001b[A\n",
      "100%|█████████▉| 332/333 [11:35<00:02,  2.10s/it]\u001b[A\n",
      "100%|██████████| 333/333 [11:37<00:00,  2.10s/it]\u001b[A\n",
      " 25%|██▌       | 1/4 [11:37<34:53, 697.75s/it]\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|▎         | 1/28 [00:00<00:08,  3.29it/s]\u001b[A\n",
      " 32%|███▏      | 9/28 [00:00<00:01, 15.75it/s]\u001b[A\n",
      " 36%|███▌      | 10/28 [00:03<00:05,  3.27it/s]\u001b[A\n",
      " 39%|███▉      | 11/28 [00:05<00:09,  1.85it/s]\u001b[A\n",
      " 46%|████▋     | 13/28 [00:08<00:10,  1.45it/s]\u001b[A\n",
      " 50%|█████     | 14/28 [00:12<00:12,  1.16it/s]\u001b[A\n",
      " 54%|█████▎    | 15/28 [00:15<00:13,  1.02s/it]\u001b[A\n",
      " 57%|█████▋    | 16/28 [00:19<00:14,  1.19s/it]\u001b[A\n",
      " 61%|██████    | 17/28 [00:22<00:14,  1.32s/it]\u001b[A\n",
      " 64%|██████▍   | 18/28 [00:25<00:14,  1.43s/it]\u001b[A\n",
      " 68%|██████▊   | 19/28 [00:28<00:13,  1.52s/it]\u001b[A\n",
      " 71%|███████▏  | 20/28 [00:32<00:12,  1.62s/it]\u001b[A\n",
      " 75%|███████▌  | 21/28 [00:34<00:11,  1.66s/it]\u001b[A\n",
      " 93%|█████████▎| 26/28 [00:34<00:02,  1.34s/it]\u001b[A\n",
      " 50%|█████     | 2/4 [12:12<12:12, 366.34s/it]]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:01<00:06,  1.52s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [00:04<00:02,  1.38s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:06<00:01,  1.72s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:10<00:00,  2.07s/it]\u001b[A\n",
      " 75%|███████▌  | 3/4 [12:23<04:07, 247.68s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|█         | 1/10 [00:02<00:24,  2.76s/it]\u001b[A\n",
      " 20%|██        | 2/10 [00:05<00:20,  2.53s/it]\u001b[A\n",
      " 30%|███       | 3/10 [00:07<00:17,  2.44s/it]\u001b[A\n",
      " 40%|████      | 4/10 [00:09<00:14,  2.42s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [00:12<00:12,  2.50s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [00:14<00:09,  2.49s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [00:17<00:07,  2.47s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [00:19<00:04,  2.45s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [00:21<00:02,  2.43s/it]\u001b[A\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.41s/it]\u001b[A\n",
      "100%|██████████| 4/4 [12:47<00:00, 191.79s/it]\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "path = os.path.join(cwd, \"../../../Project Data\",\"Tweets\")\n",
    "# Get all the files in the current working directory\n",
    "folders = os.listdir(path)\n",
    "# Keep only the folders excluding the checkpoints folder -> event folders\n",
    "folders = [x for x in folders if os.path.isdir(os.path.join(path, x)) if \"checkpoints\" not in x]\n",
    "\n",
    "do_prints = False\n",
    "if do_prints: print(folders)\n",
    "    \n",
    "for folder in folders:\n",
    "    # Get all the files in the event folder\n",
    "    files = os.listdir(os.path.join(path, folder))\n",
    "\n",
    "    # If the geocoded folder does not exist create one for the given event\n",
    "    if not os.path.exists(os.path.join(path, folder, \"Geocoded\")):\n",
    "        os.makedirs(os.path.join(path, folder, \"Geocoded\"))\n",
    "    \n",
    "    # exclude the log file\n",
    "    files = [x for x in files if \"log\" not in x if \"Geocoded\" not in x if \"DS_Store\" not in x if \"Located\" in x]\n",
    "    \n",
    "    if do_prints: print(files)\n",
    "    \n",
    "    # Go through all the different files in the folder and process them.\n",
    "    for file in tqdm(files):\n",
    "        extract_geocodes_for_pickle(folder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA]",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
