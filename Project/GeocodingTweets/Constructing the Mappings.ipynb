{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding Tweets  With The Mappings\n",
    "\n",
    "The purpose of this notebook is to create the mapping dictionaries used to recover the tweets that were pickled and to map them to a specific country. The tweets were recovered with their location (when provided by the user). If the tweet did not have a provided location, the location of the user was scraped. However not all users provide this information on their page. That is why all the tweets which did not have either information were dropped. Then the locations provided were mapped to the countries. \n",
    "\n",
    "To map the tweets to their locations we used in order : \n",
    "- Automatic verification of whether the country name or a capital name was contained in the string. This was possible using the data obtained from : https://mledoze.github.io/countries/ and https://datahub.io/core/country-codes. The first links the country iso codes to country names in multiple languages with not only the official but also the common names of a country. The latter links the country iso codes to country names in different languages (arabic, chinese, english, spanish, french, russian). \n",
    "- A city to country mapper from which we removed duplicate cities taken from : https://github.com/lutangar/cities.json \n",
    "- A city to country mapper extracted from : http://www.geonames.org/export/ and http://download.geonames.org/export/dump/. The issue with this dataframe is that the duplicate cities were not handled. They were progressively overwritten. The advantage of this mapper however is that it is more extensive than the previous one, contaning a larger number of cities as well as alternative spellings and different languages. Ideally, what should have been done in the case of multiple cities with same name would be to select based on the population of the cities. \n",
    "- If none of the above yielded any results we queried an API based on the works of http://www.geonames.org/export/, http://geocoder.readthedocs.io/results.html which outputs the most probable location to which the user selected location corresponds to. From that we can recover the ISO country code which can directly be used in the Chloropleth maps. Note that we could not query the API for all the locations as this takes around 1 second per tweet. Given that the number of tweets is in the order of magnitude of the millions this would not have been feasible on the entire dataset.\n",
    "\n",
    "All of this was done using dictionaries to speed up the identification process. Currently, for pickles containing around 2000 tweets, we require under 10 seconds of processing. \n",
    "\n",
    "To create the dictionaries the given locations were set as keys with alternative spellings as well as string formatting to maximize the chance of identifying the country. What is time consuming however is creating the dictionaries themselves which is why the dictionaries were pickled once the process was finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geocoder, geopy\n",
    "import time\n",
    "import unicodedata\n",
    "import pickle\n",
    "import contextlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/8694815/removing-accent-and-special-characters\n",
    "def remove_accents(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    else :\n",
    "        clean = ''.join(x.lower().strip() for x in unicodedata.normalize('NFKD', data) if \\\n",
    "                unicodedata.category(x)[0] == 'L').lower()\n",
    "        return clean\n",
    "\n",
    "def string_formatting(string):\n",
    "    string = string.replace(\"-\", \" \").replace(\" \", \",\").split(\",\")\n",
    "    formatted_string = [remove_accents(x.lower()) for x in string]\n",
    "    return string,formatted_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to apply transformations to elements in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sublist(x):\n",
    "    return list(set(filter(None, np.hstack(x))))\n",
    "\n",
    "def remove_accents_in_sublist(l):\n",
    "    return list(map(lambda x:remove_accents(x.lower()),l))\n",
    "    \n",
    "def remove_accents_in_list(lists):\n",
    "    return list(map(lambda x:remove_accents_in_sublist(x),lists))\n",
    "\n",
    "def clean_and_remove_accents_in_list(lists):\n",
    "    return list(map(lambda x:clean_sublist(remove_accents_in_sublist(x)),lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['阿富汗', 'afganistan', 'afghanistan', 'афганистан', 'kabul', 'افغانستان'],\n",
       " ['阿尔巴尼亚', 'البانيا', 'албания', 'albania', 'tirana', 'albanie']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [['أفغانستان', 'afganistán', '阿富汗', 'афганистан', 'Kabul', 'afghanistan'], ['阿尔巴尼亚', 'албания', 'Tirana', 'ألبانيا', 'albania', 'albanie']]\n",
    "clean_and_remove_accents_in_list(test_list)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_dict(df, do_prints = False):\n",
    "    \n",
    "    # Converting the dataframe values to list and cleaning them\n",
    "    t = time.time()\n",
    "    df_list = list(map(lambda x:clean_sublist(x),df.values.tolist()))\n",
    "    if do_prints : print(\"Converting to list :\", time.time()-t)\n",
    "\n",
    "    # Removing all the accents from the elements in the list\n",
    "    t = time.time()\n",
    "    df_variants = clean_and_remove_accents_in_list(df_list)\n",
    "    if do_prints : print(\"Getting variants :\", time.time()-t)\n",
    "    \n",
    "    # Combining the lists with original spellings and without accents\n",
    "    t = time.time()\n",
    "    df_all =  list(map(lambda x: list(set(df_list[x] + df_variants[x])),range(len(df))))\n",
    "    if do_prints : print(\"Combining Lists :\", time.time()-t)\n",
    "        \n",
    "    # Getting all the keys\n",
    "    t = time.time()\n",
    "    keys = list(map(lambda x: [df.index[x]]*(len(df_all[x])),range(len(df_all))))\n",
    "    if do_prints : print(\"Getting all keys :\", time.time()-t)\n",
    "      \n",
    "    # Creating the dictionary\n",
    "    t = time.time()\n",
    "    mapping = dict(zip(sum(df_all, []),sum(keys, [])))\n",
    "    if do_prints : print(\"Converting to dict :\", time.time()-t)\n",
    "        \n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Country and Capitals Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating the Mappings\n",
    "\n",
    "**Mapping 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arabic</th>\n",
       "      <th>chinese</th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>french</th>\n",
       "      <th>russian</th>\n",
       "      <th>ISO3</th>\n",
       "      <th>ISONum</th>\n",
       "      <th>Capital</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Region Name</th>\n",
       "      <th>Sub-region Name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISO2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AF</th>\n",
       "      <td>أفغانستان</td>\n",
       "      <td>阿富汗</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Afganistán</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Афганистан</td>\n",
       "      <td>AFG</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Kabul</td>\n",
       "      <td>AS</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Southern Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AL</th>\n",
       "      <td>ألبانيا</td>\n",
       "      <td>阿尔巴尼亚</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Albania</td>\n",
       "      <td>Albanie</td>\n",
       "      <td>Албания</td>\n",
       "      <td>ALB</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Tirana</td>\n",
       "      <td>EU</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DZ</th>\n",
       "      <td>الجزائر</td>\n",
       "      <td>阿尔及利亚</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>Argelia</td>\n",
       "      <td>Algérie</td>\n",
       "      <td>Алжир</td>\n",
       "      <td>DZA</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Algiers</td>\n",
       "      <td>AF</td>\n",
       "      <td>Africa</td>\n",
       "      <td>Northern Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AS</th>\n",
       "      <td>ساموا الأمريكية</td>\n",
       "      <td>美属萨摩亚</td>\n",
       "      <td>American Samoa</td>\n",
       "      <td>Samoa Americana</td>\n",
       "      <td>Samoa américaines</td>\n",
       "      <td>Американское Самоа</td>\n",
       "      <td>ASM</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Pago Pago</td>\n",
       "      <td>OC</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Polynesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AD</th>\n",
       "      <td>أندورا</td>\n",
       "      <td>安道尔</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorre</td>\n",
       "      <td>Андорра</td>\n",
       "      <td>AND</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>EU</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Southern Europe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               arabic chinese         english          spanish  \\\n",
       "ISO2                                                             \n",
       "AF          أفغانستان     阿富汗     Afghanistan       Afganistán   \n",
       "AL            ألبانيا   阿尔巴尼亚         Albania          Albania   \n",
       "DZ            الجزائر   阿尔及利亚         Algeria          Argelia   \n",
       "AS    ساموا الأمريكية   美属萨摩亚  American Samoa  Samoa Americana   \n",
       "AD             أندورا     安道尔         Andorra          Andorra   \n",
       "\n",
       "                 french             russian ISO3  ISONum           Capital  \\\n",
       "ISO2                                                                         \n",
       "AF          Afghanistan          Афганистан  AFG     4.0             Kabul   \n",
       "AL              Albanie             Албания  ALB     8.0            Tirana   \n",
       "DZ              Algérie               Алжир  DZA    12.0           Algiers   \n",
       "AS    Samoa américaines  Американское Самоа  ASM    16.0         Pago Pago   \n",
       "AD              Andorre             Андорра  AND    20.0  Andorra la Vella   \n",
       "\n",
       "     Continent Region Name  Sub-region Name  \n",
       "ISO2                                         \n",
       "AF          AS        Asia    Southern Asia  \n",
       "AL          EU      Europe  Southern Europe  \n",
       "DZ          AF      Africa  Northern Africa  \n",
       "AS          OC     Oceania        Polynesia  \n",
       "AD          EU      Europe  Southern Europe  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the country names in different languages mapping\n",
    "country_codes = pd.read_csv(\"Mapping Files/country-codes.csv\")\n",
    "keep_columns = ['official_name_ar', 'official_name_cn', 'official_name_en',\n",
    "                'official_name_es', 'official_name_fr', 'official_name_ru',\n",
    "                'ISO3166-1-Alpha-2', 'ISO3166-1-Alpha-3', 'ISO3166-1-numeric',\n",
    "                'Capital', 'Continent', 'Region Name','Sub-region Name']       \n",
    "\n",
    "# Keep only the desired columns\n",
    "country_codes = country_codes[keep_columns]\n",
    "country_codes.rename(inplace = True, index=str, columns={\"official_name_ar\": \"arabic\", \"official_name_cn\":\"chinese\", \"official_name_en\":\"english\", \n",
    "                                                         \"official_name_es\":\"spanish\", \"official_name_fr\":\"french\", \"official_name_ru\":\"russian\",\n",
    "                                                         \"ISO3166-1-Alpha-2\":\"ISO2\", \"ISO3166-1-Alpha-3\":\"ISO3\", \"ISO3166-1-numeric\":\"ISONum\"})\n",
    "\n",
    "# Remove the first element in the dataframe\n",
    "country_codes = country_codes.iloc[1:]\n",
    "\n",
    "country_codes.set_index(\"ISO2\", inplace = True)\n",
    "country_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arabic                     بلجيكا\n",
       "chinese                       比利时\n",
       "english                   Belgium\n",
       "spanish                   Bélgica\n",
       "french                   Belgique\n",
       "russian                   Бельгия\n",
       "ISO3                          BEL\n",
       "ISONum                         56\n",
       "Capital                  Brussels\n",
       "Continent                      EU\n",
       "Region Name                Europe\n",
       "Sub-region Name    Western Europe\n",
       "Name: BE, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_codes.loc[\"BE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dictionary\n",
    "col = [\"english\", \"french\", \"spanish\", \"chinese\", \"russian\", \"arabic\", \"Capital\"]\n",
    "country_mapping1 = convert_df_to_dict(country_codes[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapping 2**\n",
    "\n",
    "https://raw.githubusercontent.com/mledoze/countries/master/countries.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions necessary to extract the information from the different cells of the dataframe\n",
    "\n",
    "# Get the common native name from the dictionary in the native column\n",
    "\n",
    "def extract_native_name(x):\n",
    "    try:\n",
    "        return x[\"native\"][list(x[\"native\"].keys())[0]][\"common\"]\n",
    "    except:\n",
    "        return \n",
    "    \n",
    "# Get the different translations from the dictionary in the official column\n",
    "def extract_translations(x):\n",
    "    val = x.values()\n",
    "    try:\n",
    "        return[name[\"common\"] for name in x.values()]\n",
    "    except:\n",
    "        return \n",
    "    \n",
    "# Load the json into a dataframe and keep only relevant columns\n",
    "country_df = pd.read_json(\"Mapping Files/countries.json\")\n",
    "country_df = country_df[[\"altSpellings\", \"capital\", \"cca2\", \"name\", \"translations\"]]\n",
    "country_df.rename(inplace = True, index=str, columns={\"cca2\": \"ISO2\"})\n",
    "country_df.set_index(\"ISO2\", inplace = True)\n",
    "\n",
    "# Extract from the different columns the alternative names and spellings in different languages\n",
    "country_df[\"common\"] = country_df[\"name\"].apply(lambda x: x[\"common\"])\n",
    "country_df[\"official\"] = country_df[\"name\"].apply(lambda x: x[\"official\"])\n",
    "country_df[\"native\"] = country_df[\"name\"].apply(lambda x: extract_native_name(x))\n",
    "country_df[\"common translations\"] = country_df[\"translations\"].apply(lambda x: extract_translations(x))\n",
    "country_df[\"altSpellings\"] = country_df[\"altSpellings\"] .apply(lambda x: x[1:] if len(x)>1 else [])\n",
    "country_df.drop([\"name\",\"translations\"], axis = 1, inplace = True)\n",
    "\n",
    "# Convert the dataframe to a dictionary\n",
    "country_mapping2 = convert_df_to_dict(country_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Both Country Mappings and Pickling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_mapping = {**country_mapping1, **country_mapping2}\n",
    "\n",
    "file = open(\"country_mapping.pickle\", 'wb')\n",
    "pickle.dump(country_mapping, file, protocol=4)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the Country Mappings**\n",
    "\n",
    "Function used to test whether the name of a country is in a string. A similar version is used with the different mappings in the final method for the tweets.\n",
    "\n",
    "The idea is that we need to take into account that certain locations are made up of multiple words which is why we test the combination of adjacent words. We then check for each of the comabinations whether the combination is in the mapping. If it is then we output the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_in_string(loc, do_prints = False): \n",
    "    t = time.time()\n",
    "    \n",
    "    # Get the formatted and non formatted version of the words\n",
    "    words, formatted_words = string_formatting(loc)\n",
    "    if do_prints : print(words)\n",
    "        \n",
    "    # Remove words smaller than 2 characters and get all their combinations\n",
    "    # considering only adjacent words\n",
    "    words = [x.lower() for x in words if len(x)>2]\n",
    "    formatted_words = [x for x in formatted_words if len(x)>2]\n",
    "    \n",
    "    word_combinations = [\" \".join(words[i:j]) for j in range(len(words)+1) for i in range(j)]\n",
    "    word_combinations += [\" \".join(words[i:j]) for j in range(len(formatted_words)+1) for i in range(j)]\n",
    "    if do_prints : print(word_combinations)\n",
    "    \n",
    "    # If one of the combinations is in the dict then output it\n",
    "    matching = []\n",
    "    for word in word_combinations:\n",
    "        if do_prints : print(\"Testing: \", word)\n",
    "        if word in country_mapping:\n",
    "            print(time.time()-t)\n",
    "            return country_mapping[word]\n",
    "\n",
    "    print(time.time()-t)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that the function works properly as well as the execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010704994201660156\n",
      "AF\n",
      "9.799003601074219e-05\n",
      "AF\n",
      "8.082389831542969e-05\n",
      "JP\n",
      "3.1948089599609375e-05\n",
      "AT\n",
      "3.409385681152344e-05\n",
      "AT\n",
      "3.1948089599609375e-05\n",
      "AT\n",
      "2.4080276489257812e-05\n",
      "AT\n",
      "3.719329833984375e-05\n",
      "AT\n",
      "5.698204040527344e-05\n",
      "None\n",
      "4.673004150390625e-05\n",
      "None\n",
      "2.8133392333984375e-05\n",
      "US\n",
      "2.3126602172851562e-05\n",
      "EG\n"
     ]
    }
   ],
   "source": [
    "print(country_in_string(\"أفغانستان hello my name is bloop\"))\n",
    "print(country_in_string(\"أفغانستان hello my name Japan\"))\n",
    "print(country_in_string(\"España hello my name Japan\"))\n",
    "print(country_in_string(\"autriche\"))\n",
    "print(country_in_string(\"oesterreich\"))\n",
    "print(country_in_string(\"osterreich\"))\n",
    "print(country_in_string(\"austria\"))\n",
    "print(country_in_string(\"vienna\"))\n",
    "print(country_in_string(\"Hello New Zealand\"))\n",
    "print(country_in_string(\"Hello New Zealand\"))\n",
    "print(country_in_string(\"Washington\"))\n",
    "print(country_in_string(\"CAIro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. City Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : GEODATASOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the mapping taken from the GEODATASOURCE-CITIES-FREE.TXT from https://www.geodatasource.com/file-download. As we can see with a few simple tests,the output is almost always wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv(\"Mapping Files/GEODATASOURCE-CITIES-FREE.TXT\", sep = \"\\t\")\n",
    "cities.head()\n",
    "city_mapping = dict(zip(cities[\"FULL_NAME_ND\"].tolist(), cities[\"CC_FIPS\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beijing in  CH\n",
      "Cairo in  US\n",
      "Paris in  US\n",
      "Lausanne in  SZ\n",
      "Morges in  US\n",
      "Ontario in  US\n",
      "Oxford in  VQ\n",
      "Shanghai in  US\n"
     ]
    }
   ],
   "source": [
    "print(\"Beijing in \", city_mapping[\"Beijing\"])\n",
    "print(\"Cairo in \", city_mapping[\"Cairo\"])\n",
    "print(\"Paris in \", city_mapping[\"Paris\"])\n",
    "print(\"Lausanne in \", city_mapping[\"Lausanne\"])\n",
    "print(\"Morges in \", city_mapping[\"Morges\"])\n",
    "print(\"Ontario in \", city_mapping[\"Ontario\"])\n",
    "print(\"Oxford in \", city_mapping[\"Oxford\"])\n",
    "print(\"Shanghai in \", city_mapping[\"Shanghai\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Cities of the world in Json, based on GeoNames Gazetteer\n",
    "https://github.com/lutangar/cities.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISO2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sant Julià de Lòria</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pas de la Casa</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ordino</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>les Escaldes</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la Massana</th>\n",
       "      <td>AD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ISO2\n",
       "city                    \n",
       "Sant Julià de Lòria   AD\n",
       "Pas de la Casa        AD\n",
       "Ordino                AD\n",
       "les Escaldes          AD\n",
       "la Massana            AD"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_df = pd.read_json(\"Mapping Files/cities.json\")\n",
    "city_df.drop([\"lat\", \"lng\"], axis = 1, inplace = True)\n",
    "city_df.rename(inplace = True, index=str, columns={\"country\": \"ISO2\", \"name\":\"city\"})\n",
    "city_df.set_index(\"city\", inplace = True)\n",
    "city_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with this mapping is that there are multiple cities with the same name in different countries. As we have no way of determining which city is the most likely, we drop those rows from the dataframe and store them in a second one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10409\n"
     ]
    }
   ],
   "source": [
    "doublons = city_df.copy()\n",
    "doublons[\"num\"] = 1\n",
    "doublons = doublons.groupby(\"city\").sum()\n",
    "doublons = doublons[doublons.num>1]\n",
    "doublons = doublons.index.tolist()\n",
    "print(len(doublons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an example of why the mapping provided is problematic, especially since we cannot rely on language to determine to which country the city belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "Toronto    AU\n",
       "Toronto    CA\n",
       "Toronto    US\n",
       "Name: ISO2, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_df.loc[\"Toronto\",\"ISO2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all problematic cities from the mapping and creating a dictionary from the remaining cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_city_df = city_df.drop(doublons)\n",
    "city_mapping = dict(zip(reduced_city_df.index, reduced_city_df.ISO2))\n",
    "\n",
    "alt_names = [remove_accents(x) for x in reduced_city_df.index]\n",
    "city_mapping = {**dict(zip(alt_names, reduced_city_df.ISO2)), **city_mapping}\n",
    "\n",
    "file = open(\"city_mapping.pickle\", 'wb')\n",
    "pickle.dump(city_mapping, file, protocol=4)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this mapping is far from complete and is missing many cities, especially after having removed the cities with identical names. However we can quickly check a few of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nantes in : FR\n",
      "Lausanne in : CH\n",
      "Abu Dhabi in : AE\n",
      "Shanghai in : CN\n",
      "Beijing in : CN\n",
      "Tokyo in : JP\n"
     ]
    }
   ],
   "source": [
    "print(\"Nantes in :\", city_mapping[remove_accents(\"Nantes\")])\n",
    "print(\"Lausanne in :\", city_mapping[remove_accents(\"Lausanne\")])\n",
    "print(\"Abu Dhabi in :\", city_mapping[remove_accents(\"Abu Dhabi\")])\n",
    "print(\"Shanghai in :\", city_mapping[remove_accents(\"Shanghai\")])\n",
    "print(\"Beijing in :\", city_mapping[remove_accents(\"Beijing\")])\n",
    "print(\"Tokyo in :\", city_mapping[remove_accents(\"Tokyo\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 : Using APIs - https://github.com/geopy/geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple APIs were considered in order to map the cities which were neither in the Country/Capital mapping nor the city mapping where the duplicates were removed. \n",
    "\n",
    "http://geocoder.readthedocs.io/providers/GeoNames.html\n",
    "\n",
    "https://github.com/geopy/geopy\n",
    "\n",
    "https://github.com/dsoprea/GeonamesRdf\n",
    "\n",
    "Unforturnately there were multiple issues with this method. First the results are not consistent. Running the query multiple times does not always lead to the same result. Then the APIs are limited in number of queries without actually subscribing to their services. That is why this method was not kept for the final geolocalisation method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " France\n",
      "Stalingrad RU\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées FR\n",
      "Alexandria EG\n",
      "Zurich US\n",
      "Stalingrad RU\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées FR\n",
      "Alexandria EG\n",
      "Zurich US\n",
      "Stalingrad RU\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées None\n",
      "Alexandria EG\n",
      "Zurich US\n",
      "Stalingrad None\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées FR\n",
      "Alexandria None\n",
      "Zurich US\n",
      "Stalingrad RU\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées None\n",
      "Alexandria EG\n",
      "Zurich US\n",
      "Stalingrad RU\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées FR\n",
      "Alexandria EG\n",
      "Zurich US\n",
      "Stalingrad RU\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées FR\n",
      "Alexandria None\n",
      "Zurich None\n",
      "Stalingrad RU\n",
      "5th Avenue US\n",
      "Morges CH\n",
      "Champs élysées FR\n",
      "Alexandria EG\n",
      "Zurich None\n",
      "Stalingrad None\n",
      "5th Avenue None\n",
      "Morges CH\n",
      "Champs élysées None\n",
      "Alexandria None\n",
      "Zurich None\n",
      "Stalingrad None\n",
      "5th Avenue None\n",
      "Morges CH\n",
      "Champs élysées None\n",
      "Alexandria EG\n",
      "Zurich US\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim()\n",
    "location = geolocator.geocode(\"stalingrad\")\n",
    "print(location.raw['display_name'].split(\",\")[-1])    \n",
    "\n",
    "def query_geocoder_api(loc):\n",
    "    g = geocoder.google(loc)\n",
    "    try:\n",
    "        country = g.json[\"country\"]\n",
    "        return country\n",
    "    except:\n",
    "        return \n",
    "    \n",
    "for i in range(10):\n",
    "    print(\"Stalingrad\", query_geocoder_api(\"Stalingrad\"))\n",
    "    print(\"5th Avenue\", query_geocoder_api(\"5th Avenue\"))\n",
    "    print(\"Morges\", query_geocoder_api(\"Morges\"))\n",
    "    print(\"Champs élysées\", query_geocoder_api(\"Champs élysées\"))\n",
    "    print(\"Alexandria\", query_geocoder_api(\"Alexandria\"))\n",
    "    print(\"Zurich\", query_geocoder_api(\"Zurich\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4 : Using the Geonames Database \n",
    "http://download.geonames.org/export/dump/\n",
    "\n",
    "This database contains a zip file for each country with a textfile containing the different cities as well as alternate names. The functions below are used to load and process the text files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions Used to Process the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alternate_names(x):\n",
    "    try:\n",
    "        out = x.split(\",\")\n",
    "        return out\n",
    "    except:\n",
    "        return []\n",
    "        \n",
    "def process_dataframe(full_filename, do_prints = False):\n",
    "    # Load the text file as a csv\n",
    "    \n",
    "    dtypes = [int,str, str,str,float,float,str,str,\\\n",
    "             str,str,str,str, str,str,int,str,\\\n",
    "             str,str,str]\n",
    "    \n",
    "    columns = [\"geonameid\",\"name\", \"asciiname\",\"alternatenames\",\\\n",
    "               \"latitude\",\"longitude\",\"feature class\",\"feature code\",\\\n",
    "               \"country code\",\"cc2\",\"admin1 code\",\"admin2 code\",\\\n",
    "               \"admin3 code\",\"admin4 code\",\"population\",\"elevation\",\\\n",
    "               \"dem\",\"timezone\",\"modification date\"]\n",
    "    \n",
    "    cities = pd.read_csv(full_filename, sep = \"\\t\", header=None, names=columns, dtype = dict(zip(columns,dtypes)))\n",
    "        \n",
    "    if do_prints: print(\"Loaded\")\n",
    "    \n",
    "    # Keep only the relevant columns\n",
    "    cities = cities[[\"name\",\"asciiname\", \"alternatenames\", \"country code\",\"population\"]]\n",
    "    \n",
    "    # Format the given columns\n",
    "    cities[\"name\"] = cities[\"name\"].apply(lambda x: extract_alternate_names(x))\n",
    "    cities[\"asciiname\"] = cities[\"asciiname\"].apply(lambda x: extract_alternate_names(x))\n",
    "    cities[\"alternatenames\"] = cities[\"alternatenames\"].astype(\"object\")\n",
    "    cities[\"alternatenames\"] = cities[\"alternatenames\"].apply(lambda x: extract_alternate_names(x))\n",
    "    \n",
    "    \n",
    "    # Store the population and cities dataframes\n",
    "    if do_prints: print(\"Processed\")\n",
    "    pop = cities.copy()\n",
    "    \n",
    "    pop.drop([\"country code\"], axis = 1, inplace = True)\n",
    "    cities.drop([\"population\"], axis = 1, inplace = True)\n",
    "    \n",
    "    cities.set_index(\"country code\", inplace = True)\n",
    "    pop.set_index(\"population\", inplace = True)\n",
    "    \n",
    "    if do_prints: print(\"Indexed\")\n",
    "    \n",
    "    return cities, pop\n",
    "\n",
    "def modulo(i,l):\n",
    "    return i%l\n",
    "\n",
    "def writeline(fd_out, line):\n",
    "    fd_out.write('{}\\n'.format(line))\n",
    "\n",
    "    \n",
    "# Function used to split text files which were too big and slowed\n",
    "# down the process\n",
    "def split_large_files(file_path, file_large):\n",
    "    l = 15*10**2  # lines per split file\n",
    "    idx = 0\n",
    "    new_files = []\n",
    "    split_file_path = os.path.join(file_path, \"split\")\n",
    "    #print(file_path)\n",
    "    \n",
    "    # Open the file\n",
    "    with contextlib.ExitStack() as stack:\n",
    "        with open(os.path.join(file_path,file_large)) as open_file:\n",
    "            with stack.enter_context(open_file) as fd_in:\n",
    "                # Iterate through all the lines in the file\n",
    "                for i, line in enumerate(fd_in):\n",
    "                    # Split the file if the max number of lines is reached\n",
    "                    if not modulo(i,l):\n",
    "                        \n",
    "                        if not os.path.isdir(split_file_path):\n",
    "                            os.makedirs(split_file_path)\n",
    "                        \n",
    "                        file_split = '{}{}.txt'.format(os.path.join(split_file_path,file.split(\".\")[0]),idx)\n",
    "                        new_files.append(file_split)\n",
    "                        idx +=1\n",
    "                        # Close the file if one is already open and open a new one\n",
    "                        try: \n",
    "                            fd_out.close()\n",
    "                            fd_out = stack.enter_context(open(file_split, 'w'))\n",
    "                        except:\n",
    "                            fd_out = stack.enter_context(open(file_split, 'w'))\n",
    "                    \n",
    "                    # Write the lines to the file\n",
    "                    fd_out.write('{}\\n'.format(line))\n",
    "            \n",
    "    return new_files\n",
    "\n",
    "# Process each of the text files and create the city and population dictionaries\n",
    "def process_text_files_and_pickle(folders_path, folder, file, not_processed):\n",
    "    statinfo = os.stat(os.path.join(folders_path, folder, file))\n",
    "    print(file,statinfo.st_size//10**6 )\n",
    "    \n",
    "    # If the file is too big then split it \n",
    "    if statinfo.st_size>2*10**6:\n",
    "        new_files = split_large_files(os.path.join(folders_path, folder),file)\n",
    "        #print(new_files)\n",
    "        #not_processed.append(os.path.join(folders_path, folder, file))\n",
    "    else :\n",
    "        new_files = [file]\n",
    "       \n",
    "    save_path = os.path.join(folders_path, folder)\n",
    "    \n",
    "    # Go through all the different files making up the original txt file\n",
    "    for file in tqdm(new_files):\n",
    "        try : \n",
    "            # Get the name of the file for saving\n",
    "            if len(new_files) == 1:\n",
    "                path = save_path\n",
    "                full_filename = os.path.join(path, file)\n",
    "            else:\n",
    "                full_filename = file\n",
    "                path = \"/\".join(file.split(\"/\")[:-1])\n",
    "                file = file.split(\"/\")[-1]\n",
    "                file = file.split(\".\")[0]\n",
    "            \n",
    "            # Process the dataframes and convert to dictionaries\n",
    "            cities, pop = process_dataframe(full_filename)\n",
    "            #print(\"Done Processing\")\n",
    "            city_mapping2 = convert_df_to_dict(cities)\n",
    "            #print(\"City to Dict\")\n",
    "            pop_mapping = convert_df_to_dict(pop)\n",
    "            #print(\"Pop to Dict\")\n",
    "\n",
    "            # Pickle the results\n",
    "            pickle_file = open(os.path.join(save_path, file+\"_city_map.pickle\"), 'wb')\n",
    "            pickle.dump(city_mapping2, pickle_file, protocol=4)\n",
    "            pickle_file.close()\n",
    "            #print(\"Pickled City\")\n",
    "            pickle_file = open(os.path.join(save_path, file+\"_pop_map.pickle\"), 'wb')\n",
    "            pickle.dump(pop_mapping, pickle_file, protocol=4)\n",
    "            pickle_file.close()\n",
    "        \n",
    "        except:\n",
    "            not_processed.append(os.path.join(folders_path, folder, file))\n",
    "    \n",
    "    return not_processed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the text files and storing the results into dictionaries with the desired information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "# Get all the files in the Cities Folder\n",
    "folders_path = os.path.join(cwd,\"../../../Project Data\", \"Cities\")\n",
    "folders = os.listdir(folders_path)\n",
    "folders = [x for x in folders if len(x)==2]\n",
    "\n",
    "repeat = False\n",
    "not_processed = list()\n",
    "\n",
    "# Go through all the folders\n",
    "for folder in tqdm(folders):\n",
    "    # Extract only the country text file\n",
    "    file = [x for x in os.listdir(os.path.join(folders_path, folder)) \\\n",
    "            if \"readme\" not in x if \"DS_Store\" not in x if \"pickle\" not in x\\\n",
    "            if \"split\" not in x][0]\n",
    "    \n",
    "    if do_prints: print(file)\n",
    "    \n",
    "    # If we do not want to restart the entire process \n",
    "    # check if there already pickle files and continue if\n",
    "    # that is the case\n",
    "    if not repeat: \n",
    "        pickle_files = [x for x in os.listdir(os.path.join(folders_path, folder)) \\\n",
    "                        if \"pickle\" in x]\n",
    "        if len(pickle_files):\n",
    "            continue\n",
    "    \n",
    "    not_processed = process_text_files_and_pickle(folders_path, folder, file, not_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_processed\n",
    "#pickle_file = open(\"not_processed.pickle\", 'wb')\n",
    "#pickle.dump(not_processed, pickle_file, protocol=4)\n",
    "#pickle_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dictionaries from the city map pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# Get all the files in the Cities Folder\n",
    "folders_path = os.path.join(cwd,\"../../../Project Data\", \"Cities\")\n",
    "folders = os.listdir(folders_path)\n",
    "folders = [x for x in folders if len(x)==2]\n",
    "\n",
    "\n",
    "full_city_mapping = dict()\n",
    "dict_not_processed = list()\n",
    "\n",
    "idx_mapping = 0\n",
    "num_processed_files = 0\n",
    "do_prints = False\n",
    "\n",
    "# Go through all the country folders\n",
    "for folder in tqdm(folders):\n",
    "    \n",
    "    # Extract only the country text file\n",
    "    files = [x for x in os.listdir(os.path.join(folders_path, folder)) \\\n",
    "            if \"city_map.pickle\" in x]\n",
    "    if do_prints : print(\"Processing :\", files)\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        try : \n",
    "            file_path = os.path.join(folders_path, folder, file)\n",
    "            if do_prints : print(file_path)\n",
    "            \n",
    "            # Load the city mapping dictionary\n",
    "            pkl_file = open(file_path, 'rb')\n",
    "            country_city_dict = pickle.load(pkl_file)\n",
    "            if do_prints : print(len(country_city_dict))\n",
    "            \n",
    "            # Concatenate the dictionaries \n",
    "            full_city_mapping = {**full_city_mapping, **country_city_dict}\n",
    "            \n",
    "            if do_prints : print(file, len(full_city_mapping), (set(list(full_city_mapping.values()))))\n",
    "            \n",
    "            # Create a new dictionnary every 100 files processed\n",
    "            if (num_processed_files+1)%100 == 0:\n",
    "                filename = os.path.join(folders_path, \"full_city_mapping_{}.pickle\".format(idx_mapping))\n",
    "                pkl_file = open(filename, 'wb')\n",
    "                pickle.dump(full_city_mapping, pkl_file, protocol=4)\n",
    "                pkl_file.close()\n",
    "                full_city_mapping = dict()\n",
    "                idx_mapping +=1        \n",
    "            \n",
    "            if do_prints : print(\"Finished :\", file)\n",
    "            num_processed_files += 1\n",
    "            \n",
    "        # If the process fails store the name of the file for future checks\n",
    "        except:\n",
    "            dict_not_processed.append(file)\n",
    "            filename = os.path.join(folders_path, \"dicts_not_processed.pickle\")\n",
    "            pkl_file = open(filename, 'wb')\n",
    "            pickle.dump(dict_not_processed, pkl_file, protocol=4)\n",
    "            pkl_file.close()\n",
    "            if do_prints : print(\"Failed :\", file)\n",
    "\n",
    "    if do_prints : print(\"Pickled Pop\")\n",
    "\n",
    "# Store the last file\n",
    "filename = os.path.join(folders_path, \"full_city_mapping_{}.pickle\".format(idx_mapping))\n",
    "pkl_file = open(filename, 'wb')\n",
    "pickle.dump(full_city_mapping, pkl_file, protocol=4)\n",
    "pkl_file.close()\n",
    "\n",
    "if do_prints : \n",
    "    print(dict_not_processed)\n",
    "    pickles = os.listdir(folders_path)\n",
    "    pickles = [x for x in pickles if \"full_city_mapping\" in x]\n",
    "\n",
    "    for pkl in pickles:\n",
    "        pkl_file = open(pkl, 'rb')\n",
    "        interm_dict = pickle.load(pkl_file)\n",
    "        print(pkl, len(interm_dict), (set(list(interm_dict.values()))))\n",
    "        pkl_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA]",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
