{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geocoding Tweets \n",
    "\n",
    "The purpose of this notebook is to create the mapping dictionaries used to recover the tweets that were pickled and to map them to a specific country. The tweets were recovered with their location (when provided by the user). If the tweet did not have a provided location, the location of the user was scraped. However not all users provide this information on their page. That is why all the tweets which did not have either information were dropped. Then the locations provided were mapped to the countries. \n",
    "\n",
    "To map the tweets to their locations we used in order : \n",
    "- Automatic verification of whether the country name or a capital name was contained in the string. This was possible using the data obtained from : https://mledoze.github.io/countries/ and https://datahub.io/core/country-codes. The first links the country iso codes to country names in multiple languages with not only the official but also the common names of a country. The latter links the country iso codes to country names in different languages (arabic, chinese, english, spanish, french, russian). \n",
    "- A city to country mapper from which we removed duplicate cities taken from : https://github.com/lutangar/cities.json \n",
    "- A city to country mapper extracted from : http://www.geonames.org/export/ and http://download.geonames.org/export/dump/. The issue with this dataframe is that the duplicate cities were not handled. They were progressively overwritten. The advantage of this mapper however is that it is more extensive than the previous one, contaning a larger number of cities as well as alternative spellings and different languages. Ideally, what should have been done in the case of multiple cities with same name would be to select based on the population of the cities. \n",
    "- If none of the above yielded any results we queried an API based on the works of http://www.geonames.org/export/, http://geocoder.readthedocs.io/results.html which outputs the most probable location to which the user selected location corresponds to. From that we can recover the ISO country code which can directly be used in the Chloropleth maps. Note that we could not query the API for all the locations as this takes around 1 second per tweet. Given that the number of tweets is in the order of magnitude of the millions this would not have been feasible on the entire dataset.\n",
    "\n",
    "All of this was done using dictionaries to speed up the identification process. Currently, for pickles containing around 2000 tweets, we require under 10 seconds of processing. \n",
    "\n",
    "To create the dictionaries the given locations were set as keys with alternative spellings as well as string formatting to maximize the chance of identifying the country. What is time consuming however is creating the dictionaries themselves which is why the dictionaries were pickled once the process was finished. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geocoder, geopy\n",
    "import time\n",
    "import unicodedata\n",
    "import pickle\n",
    "import contextlib\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String formatting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/8694815/removing-accent-and-special-characters\n",
    "def remove_accents(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "    else :\n",
    "        clean = ''.join(x.lower().strip() for x in unicodedata.normalize('NFKD', data) if \\\n",
    "                unicodedata.category(x)[0] == 'L').lower()\n",
    "        return clean\n",
    "\n",
    "def string_formatting(string):\n",
    "    string = string.replace(\"-\", \" \").replace(\" \", \",\").split(\",\")\n",
    "    formatted_string = [remove_accents(x) for x in string]\n",
    "    return string,formatted_string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to apply transformations to elements in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sublist(x):\n",
    "    return list(set(filter(None, np.hstack(x))))\n",
    "\n",
    "def remove_accents_in_sublist(l):\n",
    "    return list(map(lambda x:remove_accents(x),l))\n",
    "    \n",
    "def remove_accents_in_list(lists):\n",
    "    return list(map(lambda x:remove_accents_in_sublist(x),lists))\n",
    "\n",
    "def clean_and_remove_accents_in_list(lists):\n",
    "    return list(map(lambda x:clean_sublist(remove_accents_in_sublist(x)),lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [['أفغانستان', 'afganistán', '阿富汗', 'афганистан', 'Kabul', 'afghanistan'], ['阿尔巴尼亚', 'албания', 'Tirana', 'ألبانيا', 'albania', 'albanie']]\n",
    "clean_and_remove_accents_in_list([[\"édjndfu\",\"édjndfu\", \"àoinidè\"],[\"édjndfu\", \"àoinidè\"]])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_dict(df, do_prints = False):\n",
    "    \n",
    "    t = time.time()\n",
    "    df_list = list(map(lambda x:clean_sublist(x),df.values.tolist()))\n",
    "    if do_prints : print(\"Converting to list :\", time.time()-t)\n",
    "\n",
    "    t = time.time()\n",
    "    df_variants = clean_and_remove_accents_in_list(df_list)\n",
    "    if do_prints : print(\"Getting variants :\", time.time()-t)\n",
    "    \n",
    "    t = time.time()\n",
    "    df_all =  list(map(lambda x: list(set(df_list[x] + df_variants[x])),range(len(df))))\n",
    "    if do_prints : print(\"Combining Lists :\", time.time()-t)\n",
    "        \n",
    "    t = time.time()\n",
    "    keys = list(map(lambda x: [df.index[x]]*(len(df_all[x])),range(len(df_all))))\n",
    "    if do_prints : print(\"Getting all keys :\", time.time()-t)\n",
    "        \n",
    "    t = time.time()\n",
    "    mapping = dict(zip(sum(df_all, []),sum(keys, [])))\n",
    "    if do_prints : print(\"Converting to dict :\", time.time()-t)\n",
    "        \n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Country and Capitals Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Creating the Mappings\n",
    "\n",
    "**Mapping 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the country names in different languages mapping\n",
    "country_codes = pd.read_csv(\"Mapping Files/country-codes.csv\")\n",
    "keep_columns = ['official_name_ar', 'official_name_cn', 'official_name_en',\n",
    "                'official_name_es', 'official_name_fr', 'official_name_ru',\n",
    "                'ISO3166-1-Alpha-2', 'ISO3166-1-Alpha-3', 'ISO3166-1-numeric',\n",
    "                'Capital', 'Continent', 'Region Name','Sub-region Name']       \n",
    "country_codes = country_codes[keep_columns]\n",
    "country_codes.rename(inplace = True, index=str, columns={\"official_name_ar\": \"arabic\", \"official_name_cn\":\"chinese\", \"official_name_en\":\"english\", \n",
    "                                                         \"official_name_es\":\"spanish\", \"official_name_fr\":\"french\", \"official_name_ru\":\"russian\",\n",
    "                                                         \"ISO3166-1-Alpha-2\":\"ISO2\", \"ISO3166-1-Alpha-3\":\"ISO3\", \"ISO3166-1-numeric\":\"ISONum\"})\n",
    "country_codes = country_codes.iloc[1:]\n",
    "languages = [\"english\", \"french\", \"spanish\", \"chinese\", \"russian\", \"arabic\"]\n",
    "\n",
    "country_names = dict()\n",
    "for lan in languages:\n",
    "    country_codes[lan] = country_codes[lan].apply(lambda x: x.lower())\n",
    "    country_names[lan] = dict(zip(country_codes[lan].tolist(), country_codes[\"ISO2\"]))\n",
    "\n",
    "country_codes.set_index(\"ISO2\", inplace = True)\n",
    "\n",
    "\n",
    "country_codes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes.loc[\"BE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = languages + [\"Capital\"]\n",
    "country_mapping1 = convert_df_to_dict(country_codes[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mapping 2**\n",
    "\n",
    "https://raw.githubusercontent.com/mledoze/countries/master/countries.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_native_name(x):\n",
    "    try:\n",
    "        return x[\"native\"][list(x[\"native\"].keys())[0]][\"common\"]\n",
    "    except:\n",
    "        return \n",
    "\n",
    "def extract_translations(x):\n",
    "    val = x.values()\n",
    "    try:\n",
    "        return[name[\"official\"] for name in x.values()]\n",
    "    except:\n",
    "        return \n",
    "    \n",
    "country_df = pd.read_json(\"Mapping Files/countries.json\")\n",
    "country_df = country_df[[\"altSpellings\", \"capital\", \"cca2\", \"name\", \"translations\"]]\n",
    "country_df.rename(inplace = True, index=str, columns={\"cca2\": \"ISO2\"})\n",
    "country_df.set_index(\"ISO2\", inplace = True)\n",
    "country_df[\"common\"] = country_df[\"name\"].apply(lambda x: x[\"common\"])\n",
    "country_df[\"official\"] = country_df[\"name\"].apply(lambda x: x[\"official\"])\n",
    "country_df[\"native\"] = country_df[\"name\"].apply(lambda x: extract_native_name(x))\n",
    "country_df[\"common translations\"] = country_df[\"translations\"].apply(lambda x: extract_translations(x))\n",
    "country_df[\"altSpellings\"] = country_df[\"altSpellings\"] .apply(lambda x: x[1:] if len(x)>1 else [])\n",
    "\n",
    "country_df.drop([\"name\",\"translations\"], axis = 1, inplace = True)\n",
    "\n",
    "country_mapping2 = convert_df_to_dict(country_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Both Country Mappings and Pickling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_mapping = {**country_mapping1, **country_mapping2}\n",
    "\n",
    "file = open(\"country_mapping.pickle\", 'wb')\n",
    "pickle.dump(country_mapping, file, protocol=4)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the Country Mappings**\n",
    "\n",
    "Function used to test whether the name of a country is in a string. A similar version is used with the different mappings in the final method for the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_in_string(loc, do_prints = False): \n",
    "    t = time.time()\n",
    "    words, formatted_words = string_formatting(loc)\n",
    "    if do_prints : print(words)\n",
    "    words = [x for x in words if len(x)>2]\n",
    "    formatted_words = [x for x in formatted_words if len(x)>2]\n",
    "    \n",
    "    word_combinations = [\" \".join(words[i:j]) for j in range(len(words)+1) for i in range(j)]\n",
    "    word_combinations += [\" \".join(words[i:j]) for j in range(len(formatted_words)+1) for i in range(j)]\n",
    "    if do_prints : print(word_combinations)\n",
    "    \n",
    "    matching = []\n",
    "    for word in word_combinations:\n",
    "        if do_prints : print(\"Testing: \", word)\n",
    "        if word in country_mapping:\n",
    "            print(time.time()-t)\n",
    "            return country_mapping[word]\n",
    "\n",
    "    print(time.time()-t)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that the function works properly as well as the execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(country_in_string(\"أفغانستان hello my name is bloop\"))\n",
    "print(country_in_string(\"أفغانستان hello my name Japan\"))\n",
    "print(country_in_string(\"España hello my name Japan\"))\n",
    "print(country_in_string(\"autriche\"))\n",
    "print(country_in_string(\"oesterreich\"))\n",
    "print(country_in_string(\"osterreich\"))\n",
    "print(country_in_string(\"austria\"))\n",
    "print(country_in_string(\"vienna\"))\n",
    "print(country_in_string(\"Hello New Zealand\"))\n",
    "print(country_in_string(\"Hello New Zealand\"))\n",
    "print(country_in_string(\"Washington\"))\n",
    "print(country_in_string(\"cairo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. City Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : GEODATASOURCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the mapping taken from the GEODATASOURCE-CITIES-FREE.TXT from https://www.geodatasource.com/file-download. As we can see with a few simple tests,the output is almost always wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv(\"Mapping Files/GEODATASOURCE-CITIES-FREE.TXT\", sep = \"\\t\")\n",
    "cities.head()\n",
    "city_mapping = dict(zip(cities[\"FULL_NAME_ND\"].tolist(), cities[\"CC_FIPS\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Beijing in \", city_mapping[\"Beijing\"])\n",
    "print(\"Cairo in \", city_mapping[\"Cairo\"])\n",
    "print(\"Paris in \", city_mapping[\"Paris\"])\n",
    "print(\"Lausanne in \", city_mapping[\"Lausanne\"])\n",
    "print(\"Morges in \", city_mapping[\"Morges\"])\n",
    "print(\"Ontario in \", city_mapping[\"Ontario\"])\n",
    "print(\"Oxford in \", city_mapping[\"Oxford\"])\n",
    "print(\"Shanghai in \", city_mapping[\"Shanghai\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Cities of the world in Json, based on GeoNames Gazetteer\n",
    "https://github.com/lutangar/cities.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "city_df = pd.read_json(\"Mapping Files/cities.json\")\n",
    "city_df.drop([\"lat\", \"lng\"], axis = 1, inplace = True)\n",
    "city_df.rename(inplace = True, index=str, columns={\"country\": \"ISO2\", \"name\":\"city\"})\n",
    "city_df.set_index(\"city\", inplace = True)\n",
    "city_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with this mapping is that there are multiple cities with the same name in different countries. As we have no way of determining which city is the most likely, we drop those rows from the dataframe and store them in a second one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doublons = city_df.copy()\n",
    "doublons[\"num\"] = 1\n",
    "doublons = doublons.groupby(\"city\").sum()\n",
    "doublons = doublons[doublons.num>1]\n",
    "doublons = doublons.index.tolist()\n",
    "print(len(doublons))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an example of why the mapping provided is problematic, especially since we cannot rely on language to determine to which country the city belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.loc[\"Toronto\",\"ISO2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping all problematic cities from the mapping and creating a dictionary from the remaining cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_city_df = city_df.drop(doublons)\n",
    "city_mapping = dict(zip(reduced_city_df.index, reduced_city_df.ISO2))\n",
    "\n",
    "alt_names = [remove_accents(x) for x in reduced_city_df.index]\n",
    "city_mapping = {**dict(zip(alt_names, reduced_city_df.ISO2)), **city_mapping}\n",
    "\n",
    "file = open(\"city_mapping.pickle\", 'wb')\n",
    "pickle.dump(city_mapping, file, protocol=4)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this mapping is far from complete and is missing many cities, especially after having removed the cities with identical names. However we can quickly check a few of the cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nantes in :\", city_mapping[remove_accents(\"Nantes\")])\n",
    "print(\"Lausanne in :\", city_mapping[remove_accents(\"Lausanne\")])\n",
    "print(\"Abu Dhabi in :\", city_mapping[remove_accents(\"Abu Dhabi\")])\n",
    "print(\"Shanghai in :\", city_mapping[remove_accents(\"Shanghai\")])\n",
    "print(\"Beijing in :\", city_mapping[remove_accents(\"Beijing\")])\n",
    "print(\"Tokyo in :\", city_mapping[remove_accents(\"Tokyo\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 : Using APIs - https://github.com/geopy/geopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple APIs were considered in order to map the cities which were neither in the Country/Capital mapping nor the city mapping where the duplicates were removed. \n",
    "\n",
    "http://geocoder.readthedocs.io/providers/GeoNames.html\n",
    "\n",
    "https://github.com/geopy/geopy\n",
    "\n",
    "https://github.com/dsoprea/GeonamesRdf\n",
    "\n",
    "Unforturnately there were multiple issues with this method. First the results are not consistent. Running the query multiple times does not always lead to the same result. Then the APIs are limited in number of queries without actually subscribing to their services. That is why this method was not kept for the final geolocalisation method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "geolocator = Nominatim()\n",
    "location = geolocator.geocode(\"stalingrad\")\n",
    "print(location.raw['display_name'].split(\",\")[-1])    \n",
    "\n",
    "def query_geocoder_api(loc):\n",
    "    g = geocoder.google(loc)\n",
    "    try:\n",
    "        country = g.json[\"country\"]\n",
    "        return country\n",
    "    except:\n",
    "        return \n",
    "    \n",
    "for i in range(10):\n",
    "    print(\"Stalingrad\", query_geocoder_api(\"Stalingrad\"))\n",
    "    print(\"5th Avenue\", query_geocoder_api(\"5th Avenue\"))\n",
    "    print(\"Morges\", query_geocoder_api(\"Morges\"))\n",
    "    print(\"Champs élysées\", query_geocoder_api(\"Champs élysées\"))\n",
    "    print(\"Alexandria\", query_geocoder_api(\"Alexandria\"))\n",
    "    print(\"Zurich\", query_geocoder_api(\"Zurich\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4 : Using the Geonames Database \n",
    "http://download.geonames.org/export/dump/\n",
    "\n",
    "This database contains a zip file for each country with a textfile containing the different cities as well as alternate names. The functions below are used to load and process the text files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_alternate_names(x):\n",
    "    try:\n",
    "        out = x.split(\",\")\n",
    "        return out\n",
    "    except:\n",
    "        return []\n",
    "        \n",
    "def process_dataframe(full_filename):\n",
    "    # Load the text file as a csv\n",
    "    \n",
    "    dtypes = [int,str, str,str,float,float,str,str,\\\n",
    "             str,str,str,str, str,str,int,str,\\\n",
    "             str,str,str]\n",
    "    \n",
    "    columns = [\"geonameid\",\"name\", \"asciiname\",\"alternatenames\",\\\n",
    "               \"latitude\",\"longitude\",\"feature class\",\"feature code\",\\\n",
    "               \"country code\",\"cc2\",\"admin1 code\",\"admin2 code\",\\\n",
    "               \"admin3 code\",\"admin4 code\",\"population\",\"elevation\",\\\n",
    "               \"dem\",\"timezone\",\"modification date\"]\n",
    "    \n",
    "    cities = pd.read_csv(full_filename, sep = \"\\t\", header=None, names=columns, dtype = dict(zip(columns,dtypes)))\n",
    "        \n",
    "    #print(\"Loaded\")\n",
    "    \n",
    "    cities = cities[[\"name\",\"asciiname\", \"alternatenames\", \"country code\",\"population\"]]\n",
    "    \n",
    "    cities[\"name\"] = cities[\"name\"].apply(lambda x: extract_alternate_names(x))\n",
    "    cities[\"asciiname\"] = cities[\"asciiname\"].apply(lambda x: extract_alternate_names(x))\n",
    "    cities[\"alternatenames\"] = cities[\"alternatenames\"].astype(\"object\")\n",
    "    cities[\"alternatenames\"] = cities[\"alternatenames\"].apply(lambda x: extract_alternate_names(x))\n",
    "    \n",
    "    #print(\"Processed\")\n",
    "    pop = cities.copy()\n",
    "    \n",
    "    pop.drop([\"country code\"], axis = 1, inplace = True)\n",
    "    cities.drop([\"population\"], axis = 1, inplace = True)\n",
    "    \n",
    "    cities.set_index(\"country code\", inplace = True)\n",
    "    pop.set_index(\"population\", inplace = True)\n",
    "    \n",
    "    #print(\"Indexed\")\n",
    "    \n",
    "    return cities, pop\n",
    "\n",
    "def modulo(i,l):\n",
    "    return i%l\n",
    "\n",
    "def writeline(fd_out, line):\n",
    "    fd_out.write('{}\\n'.format(line))\n",
    "\n",
    "def split_large_files(file_path, file_large):\n",
    "    l = 15*10**2  # lines per split file\n",
    "    idx = 0\n",
    "    new_files = []\n",
    "    split_file_path = os.path.join(file_path, \"split\")\n",
    "    #print(file_path)\n",
    "    \n",
    "    with contextlib.ExitStack() as stack:\n",
    "        with open(os.path.join(file_path,file_large)) as open_file:\n",
    "            with stack.enter_context(open_file) as fd_in:\n",
    "                for i, line in enumerate(fd_in):\n",
    "                    if not modulo(i,l):\n",
    "                        if not os.path.isdir(split_file_path):\n",
    "                            os.makedirs(split_file_path)\n",
    "                        file_split = '{}{}.txt'.format(os.path.join(split_file_path,file.split(\".\")[0]),idx)\n",
    "                        new_files.append(file_split)\n",
    "                        idx +=1\n",
    "                        try: \n",
    "                            fd_out.close()\n",
    "                            fd_out = stack.enter_context(open(file_split, 'w'))\n",
    "                        except:\n",
    "                            fd_out = stack.enter_context(open(file_split, 'w'))\n",
    "                    fd_out.write('{}\\n'.format(line))\n",
    "            \n",
    "    return new_files\n",
    "\n",
    "def process_text_files_and_pickle(folders_path, folder, file, not_processed):\n",
    "    statinfo = os.stat(os.path.join(folders_path, folder, file))\n",
    "    print(file,statinfo.st_size//10**6 )\n",
    "    \n",
    "    if statinfo.st_size>2*10**6:\n",
    "        new_files = split_large_files(os.path.join(folders_path, folder),file)\n",
    "        #print(new_files)\n",
    "        #not_processed.append(os.path.join(folders_path, folder, file))\n",
    "    else :\n",
    "        new_files = [file]\n",
    "        \n",
    "    save_path = os.path.join(folders_path, folder)\n",
    "    \n",
    "    for file in tqdm(new_files):\n",
    "        try : \n",
    "            if len(new_files) == 1:\n",
    "                path = save_path\n",
    "                full_filename = os.path.join(path, file)\n",
    "            else:\n",
    "                full_filename = file\n",
    "                path = \"/\".join(file.split(\"/\")[:-1])\n",
    "                file = file.split(\"/\")[-1]\n",
    "                file = file.split(\".\")[0]\n",
    "            \n",
    "            cities, pop = process_dataframe(full_filename)\n",
    "            #print(\"Done Processing\")\n",
    "            city_mapping2 = convert_df_to_dict(cities)\n",
    "            #print(\"City to Dict\")\n",
    "            pop_mapping = convert_df_to_dict(pop)\n",
    "            #print(\"Pop to Dict\")\n",
    "\n",
    "            pickle_file = open(os.path.join(save_path, file+\"_city_map.pickle\"), 'wb')\n",
    "            pickle.dump(city_mapping2, pickle_file, protocol=4)\n",
    "            pickle_file.close()\n",
    "            #print(\"Pickled City\")\n",
    "            pickle_file = open(os.path.join(save_path, file+\"_pop_map.pickle\"), 'wb')\n",
    "            pickle.dump(pop_mapping, pickle_file, protocol=4)\n",
    "            pickle_file.close()\n",
    "        \n",
    "        except:\n",
    "            not_processed.append(os.path.join(folders_path, folder, file))\n",
    "    \n",
    "    return not_processed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "# Get all the files in the Cities Folder\n",
    "folders_path = os.path.join(cwd,\"../../../Project Data\", \"Cities\")\n",
    "folders = os.listdir(folders_path)\n",
    "folders = [x for x in folders if len(x)==2]\n",
    "\n",
    "repeat = False\n",
    "not_processed = list()\n",
    "\n",
    "# Go through all the folders\n",
    "for folder in tqdm(folders):\n",
    "    # Extract only the country text file\n",
    "    file = [x for x in os.listdir(os.path.join(folders_path, folder)) \\\n",
    "            if \"readme\" not in x if \"DS_Store\" not in x if \"pickle\" not in x\\\n",
    "            if \"split\" not in x][0]\n",
    "    \n",
    "    if do_prints: print(file)\n",
    "        \n",
    "    if not repeat: \n",
    "        pickle_files = [x for x in os.listdir(os.path.join(folders_path, folder)) \\\n",
    "                        if \"pickle\" in x]\n",
    "        if len(pickle_files):\n",
    "            continue\n",
    "    \n",
    "    not_processed = process_text_files_and_pickle(folders_path, folder, file, not_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_processed\n",
    "#pickle_file = open(\"not_processed.pickle\", 'wb')\n",
    "#pickle.dump(not_processed, pickle_file, protocol=4)\n",
    "#pickle_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dictionaries from the city map pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "# Get all the files in the Cities Folder\n",
    "folders_path = os.path.join(cwd,\"../../../Project Data\", \"Cities\")\n",
    "folders = os.listdir(folders_path)\n",
    "folders = [x for x in folders if len(x)==2]\n",
    "\n",
    "full_city_mapping = dict()\n",
    "dict_not_processed = list()\n",
    "\n",
    "idx_mapping = 0\n",
    "num_processed_files = 0\n",
    "do_prints = False\n",
    "\n",
    "for folder in tqdm(folders):\n",
    "    # Extract only the country text file\n",
    "    files = [x for x in os.listdir(os.path.join(folders_path, folder)) \\\n",
    "            if \"city_map.pickle\" in x]\n",
    "    if do_prints : print(\"Processing :\", files)\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        try : \n",
    "            file_path = os.path.join(folders_path, folder, file)\n",
    "            if do_prints : print(file_path)\n",
    "            pkl_file = open(file_path, 'rb')\n",
    "            country_city_dict = pickle.load(pkl_file)\n",
    "            if do_prints : print(len(country_city_dict))\n",
    "            \n",
    "            full_city_mapping = {**full_city_mapping, **country_city_dict}\n",
    "            \n",
    "            if do_prints : print(file, len(full_city_mapping), (set(list(full_city_mapping.values()))))\n",
    "            \n",
    "            if (num_processed_files+1)%100 == 0:\n",
    "                filename = os.path.join(folders_path, \"full_city_mapping_{}.pickle\".format(idx_mapping))\n",
    "                pkl_file = open(filename, 'wb')\n",
    "                pickle.dump(full_city_mapping, pkl_file, protocol=4)\n",
    "                pkl_file.close()\n",
    "                full_city_mapping = dict()\n",
    "                idx_mapping +=1        \n",
    "            \n",
    "            if do_prints : print(\"Finished :\", file)\n",
    "            num_processed_files += 1\n",
    "            \n",
    "        except:\n",
    "            dict_not_processed.append(file)\n",
    "            filename = os.path.join(folders_path, \"dicts_not_processed.pickle\")\n",
    "            pkl_file = open(filename, 'wb')\n",
    "            pickle.dump(dict_not_processed, pkl_file, protocol=4)\n",
    "            pkl_file.close()\n",
    "            if do_prints : print(\"Failed :\", file)\n",
    "\n",
    "    if do_prints : print(\"Pickled Pop\")\n",
    "        \n",
    "filename = os.path.join(folders_path, \"full_city_mapping_{}.pickle\".format(idx_mapping))\n",
    "pkl_file = open(filename, 'wb')\n",
    "pickle.dump(full_city_mapping, pkl_file, protocol=4)\n",
    "pkl_file.close()\n",
    "\n",
    "if do_prints : \n",
    "    print(dict_not_processed)\n",
    "    pickles = os.listdir(folders_path)\n",
    "    pickles = [x for x in pickles if \"full_city_mapping\" in x]\n",
    "\n",
    "    for pkl in pickles:\n",
    "        pkl_file = open(pkl, 'rb')\n",
    "        interm_dict = pickle.load(pkl_file)\n",
    "        print(pkl, len(interm_dict), (set(list(interm_dict.values()))))\n",
    "        pkl_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA]",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
