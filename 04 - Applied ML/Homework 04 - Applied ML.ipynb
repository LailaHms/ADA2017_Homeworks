{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline\n",
    "\n",
    "Wednesday, November 22, 2017, 11:59PM\n",
    "\n",
    "## Important notes\n",
    "\n",
    "- When you push your Notebook to GitHub, all the cells must already have been evaluated.\n",
    "- Don't forget to add a textual description of your thought process and of any assumptions you've made.\n",
    "- Please write all your comments in English, and use meaningful variable names in your code.\n",
    "\n",
    "## Question 1: Propensity score matching\n",
    "\n",
    "In this exercise, you will apply [propensity score matching](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf), which we discussed in lecture 5 (\"Observational studies\"), in order to draw conclusions from an observational study.\n",
    "\n",
    "We will work with a by-now classic dataset from Robert LaLonde's study \"[Evaluating the Econometric Evaluations of Training Programs](http://people.hbs.edu/nashraf/LaLonde_1986.pdf)\" (1986).\n",
    "The study investigated the effect of a job training program (\"National Supported Work Demonstration\") on the real earnings of an individual, a couple of years after completion of the program.\n",
    "Your task is to determine the effectiveness of the \"treatment\" represented by the job training program.\n",
    "\n",
    "#### Dataset description\n",
    "\n",
    "- `treat`: 1 if the subject participated in the job training program, 0 otherwise\n",
    "- `age`: the subject's age\n",
    "- `educ`: years of education\n",
    "- `race`: categorical variable with three possible values: Black, Hispanic, or White\n",
    "- `married`: 1 if the subject was married at the time of the training program, 0 otherwise\n",
    "- `nodegree`: 1 if the subject has earned no school degree, 0 otherwise\n",
    "- `re74`: real earnings in 1974 (pre-treatment)\n",
    "- `re75`: real earnings in 1975 (pre-treatment)\n",
    "- `re78`: real earnings in 1978 (outcome)\n",
    "\n",
    "If you want to brush up your knowledge on propensity scores and observational studies, we highly recommend Rosenbaum's excellent book on the [\"Design of Observational Studies\"](http://www.stewartschultz.com/statistics/books/Design%20of%20observational%20studies.pdf). Even just reading the first chapter (18 pages) will help you a lot.\n",
    "\n",
    "#### 1. A naive analysis\n",
    "\n",
    "Compare the distribution of the outcome variable (`re78`) between the two groups, using plots and numbers.\n",
    "To summarize and compare the distributions, you may use the techniques we discussed in lectures 4 (\"Read the stats carefully\") and 6 (\"Data visualization\").\n",
    "\n",
    "What might a naive \"researcher\" conclude from this superficial analysis?\n",
    "\n",
    "#### 2. A closer look at the data\n",
    "\n",
    "You're not naive, of course (and even if you are, you've learned certain things in ADA), so you aren't content with a superficial analysis such as the above.\n",
    "You're aware of the dangers of observational studies, so you take a closer look at the data before jumping to conclusions.\n",
    "\n",
    "For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers.\n",
    "As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "\n",
    "What do you observe?\n",
    "Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis.\n",
    "\n",
    "#### 3. A propsensity score model\n",
    "\n",
    "Use logistic regression to estimate propensity scores for all points in the dataset.\n",
    "You may use `sklearn` to fit the logistic regression model and apply it to each data point to obtain propensity scores:\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model\n",
    "logistic = linear_model.LogisticRegression()\n",
    "```\n",
    "\n",
    "Recall that the propensity score of a data point represents its probability of receiving the treatment, based on its pre-treatment features (in this case, age, education, pre-treatment income, etc.).\n",
    "To brush up on propensity scores, you may read chapter 3.3 of the above-cited book by Rosenbaum or [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).\n",
    "\n",
    "Note: you do not need a train/test split here. Train and apply the model on the entire dataset. If you're wondering why this is the right thing to do in this situation, recall that the propensity score model is not used in order to make predictions about unseen data. Its sole purpose is to balance the dataset across treatment groups.\n",
    "(See p. 74 of Rosenbaum's book for an explanation why slight overfitting is even good for propensity scores.\n",
    "If you want even more information, read [this article](https://drive.google.com/file/d/0B4jctQY-uqhzTlpBaTBJRTJFVFE/view).)\n",
    "\n",
    "#### 4. Balancing the dataset via matching\n",
    "\n",
    "Use the propensity scores to match each data point from the treated group with exactly one data point from the control group, while ensuring that each data point from the control group is matched with at most one data point from the treated group.\n",
    "(Hint: you may explore the `networkx` package in Python for predefined matching functions.)\n",
    "\n",
    "Your matching should maximize the similarity between matched subjects, as captured by their propensity scores.\n",
    "In other words, the sum (over all matched pairs) of absolute propensity-score differences between the two matched subjects should be minimized.\n",
    "\n",
    "After matching, you have as many treated as you have control subjects.\n",
    "Compare the outcomes (`re78`) between the two groups (treated and control).\n",
    "\n",
    "Also, compare again the feature-value distributions between the two groups, as you've done in part 2 above, but now only for the matched subjects.\n",
    "What do you observe?\n",
    "Are you closer to being able to draw valid conclusions now than you were before?\n",
    "\n",
    "\n",
    "#### 5. Balancing the groups further\n",
    "\n",
    "Based on your comparison of feature-value distributions from part 4, are you fully satisfied with your matching?\n",
    "Would you say your dataset is sufficiently balanced?\n",
    "If not, in what ways could the \"balanced\" dataset you have obtained still not allow you to draw valid conclusions?\n",
    "\n",
    "Improve your matching by explicitly making sure that you match only subjects that have the same value for the problematic feature.\n",
    "Argue with numbers and plots that the two groups (treated and control) are now better balanced than after part 4.\n",
    "\n",
    "\n",
    "#### 6. A less naive analysis\n",
    "\n",
    "Compare the outcomes (`re78`) between treated and control subjects, as you've done in part 1, but now only for the matched dataset you've obtained from part 5.\n",
    "What do you conclude about the effectiveness of the job training program?\n",
    "\n",
    "\n",
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "Importing the libraries we will need for the homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Resolution\n",
    "\n",
    "### Thought process and Implementation plan:\n",
    "The objective of this exercise is to analyze the effect of a training program on an individual's earnings. \n",
    "\n",
    "** Loading and Cleaning the Dataset ** \n",
    "\n",
    "The dataset is provided in a csv file containing the suss-mentioned categories : \n",
    "- *treat* : whether the subject followed the tranining program, binary value\n",
    "- *age* : the age of the subject\n",
    "- *educ* : the number of years of education \n",
    "- *race* : Black, White or Hispanic\n",
    "- *married* : whether the subject is married or not, binary value\n",
    "- *nodegree* : whether the subject has a degree (1 if they don't, 0 if they do)\n",
    "- *re74*, *re75* and *re78*: earnings for the given year\n",
    "\n",
    "The dtypes and unique values of the columns were verified to make sure that there weren't any suprises along the way. \n",
    "\n",
    "When looking at the data we noticed that there wasn't a white column. This column was added manually and individuals were set to 1 if they were neither black nor hispanic. We also added a race column which would take values B,H,W whether the person was black, hispanic or white.\n",
    "\n",
    "**1. Naive Analysis**\n",
    "\n",
    "The objective here was to quickly assess whether having followed the training program had an influence on the salary. Therefore we wanted to compare simply the *re78* for people with and without treatment. For that we used the groupby functionality. To obtain statistical information we used the describe functionality of the dataframes and computed the correlation between having taken the treatment and the salary. We plotted the histograms of the data with treatment, without treatment and overall with a boxplot to explicit certain statistical aspects of the data. We also plotter the boxplots of with and without treatment against one another with a swarmplot to determine where the datapoints lie. All of this was done with multiple functions so that they can be reused in following questions\n",
    "\n",
    "\n",
    "**2. Closer Look **\n",
    "\n",
    "We conducted similar tests but with all the features this time combining visualization of multiple boxplots with descriptions of all the variables, notably the categorical ones which required slightly different handling \n",
    "\n",
    "\n",
    "**3. Propensity Score Model**\n",
    "\n",
    "Using the cleaned data we can now determine the propensity score, which is to say the probability of receiving treatment given the pre-treatment features. This is done using logistic regression as shown during the Tutorial : \n",
    "1. Create feature vectors with dummy\n",
    "2. Normalize the feature vectors\n",
    "3. Create the logistic regression model\n",
    "4. Get the predictions\n",
    "5. Threshold to get estimate labels\n",
    "6. Compute the confusion matrix to assess quality of the regression\n",
    "\n",
    "**4. Balancing the Dataset Via Matching**\n",
    "\n",
    "In order to do the feature matching, and following the recommendation to use networkx, we first needed to create a connected graph from our observations : \n",
    "1. Create a bipartite graph \n",
    "2. Add edges between the nodes from the different partitions using the differences in propensity scores from the previous question\n",
    "3. Used the networkx max_weight_matching algorithm on the bipartite graph in order to find the optimal pairs.\n",
    "\n",
    "We then observed the feature distributions for the new dataframe containing only the matched pairs. \n",
    "\n",
    "**5. Balancing Groups Further**\n",
    "\n",
    "\n",
    "**6. A Less Naive Analysis**\n",
    "\n",
    "\n",
    "\n",
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv file\n",
    "data = pd.read_csv(\"lalonde.csv\")\n",
    "\n",
    "# Adding the white column\n",
    "data['white'] = (data[\"black\"] + data[\"hispan\"]<1).astype(int)\n",
    "\n",
    "# Adding a \"race\" column\n",
    "data['race'] = \"unknown\"\n",
    "data.loc[data[\"white\"] ==1, 'race'] = \"W\"\n",
    "data.loc[data[\"hispan\"] ==1, 'race'] = \"H\"\n",
    "data.loc[data[\"black\"] ==1, 'race'] = \"B\"\n",
    "\n",
    "#Setting the index to the user id\n",
    "data.set_index(\"id\", inplace = True)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used for analysis \n",
    "** ADD EXPLANATIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_distribution_test(data):\n",
    "    k2, p = scipy.stats.normaltest(data)\n",
    "    if p>0.05:\n",
    "        print(\"\\tThe null hypothesis cannot be rejected\\n\")\n",
    "        return False\n",
    "    else :\n",
    "        print(\"\\tThe null hypothesis can be rejected\\n\")\n",
    "        return True\n",
    "\n",
    "def same_distribution_test(data1, data2):\n",
    "    stats, p = scipy.stats.kruskal(data1, data2)\n",
    "    if p>0.05:\n",
    "        print(\"\\tThe null hypothesis cannot be rejected\\n\")\n",
    "        return False\n",
    "    else :\n",
    "        print(\"\\tThe null hypothesis can be rejected\\n\")\n",
    "        return True\n",
    "\n",
    "def statistical_description(df, col):\n",
    "\n",
    "    print(\"Testing whether the entire data is normally distributed\")        \n",
    "    all_normal = normal_distribution_test(list(df[col]))\n",
    "    \n",
    "    print(\"Testing whether the results with treatment are normally distributed\")        \n",
    "    treat_normal = normal_distribution_test(list(df.loc[df['treat'] == 1, col]))\n",
    "    \n",
    "    print(\"Testing whether the results without treatment are normally distributed\")        \n",
    "    no_treat_normal = normal_distribution_test(list(df.loc[df['treat'] == 0, col]))\n",
    "       \n",
    "    print(\"Testing whether the data comes from the same distributions\")\n",
    "    same_distribution_test(list(df.loc[df['treat'] == 0, col]),\\\n",
    "                           list(df.loc[df['treat'] == 1, col]))\n",
    "    \n",
    "    print(\"Computing correlation between Treatment and {}\".format(col))\n",
    "    # Taking into account whether the data is normally distributed or not\n",
    "    # to select the correlatoin type\n",
    "    if all_normal or (treat_normal and no_treat_normal):\n",
    "        corr_type = \"pearson\"   \n",
    "    else:\n",
    "        corr_type = \"spearman\"\n",
    "\n",
    "    correlation = df[[col, \"treat\"]].corr(corr_type)\n",
    "    print(\"\\t{} correlation between treatment and {} : {:4f}\".format(corr_type, col, correlation.loc[\"treat\", col]))\n",
    "\n",
    "def histogram_boxplot(df, col, title):\n",
    "    x = list(df[col])\n",
    "    ax = sns.distplot(x)\n",
    "    ax2 = ax.twinx()\n",
    "    sns.boxplot(x=x, ax=ax2)\n",
    "    ax2.set(ylim=(-.5, 10))  \n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"percentage\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "def categorical_boxplots(df, col, year):\n",
    "    x = 're'+ year\n",
    "    data = df.copy()\n",
    "    data[col] = data[col].astype('category')\n",
    "    if len(data[col].unique()) < 4:\n",
    "        fig, axs = plt.subplots(1,len(data[col].unique()), figsize=(10,3))\n",
    "\n",
    "        for i, categ in enumerate(data[col].unique()):\n",
    "            categ_data = data.loc[data[col] == categ]\n",
    "            axs[i].set_title(categ)\n",
    "            axs[i].set_xlim(0,61000)\n",
    "            try:\n",
    "                ax = sns.boxplot(x=x, y=\"treat\", ax=axs[i], data=categ_data, orient=\"h\")\n",
    "                ax = sns.swarmplot(x=x, y=\"treat\", ax=axs[i], data=categ_data, color=\".25\", orient=\"h\")\n",
    "                \n",
    "            except:\n",
    "                data[col] = data[col].cat.codes\n",
    "                ax = sns.boxplot(x=x, y=\"treat\", ax=axs[i], data=categ_data, orient=\"h\")\n",
    "                ax = sns.swarmplot(x=x, y=\"treat\", ax=axs[i], data=categ_data, color=\".25\", orient=\"h\")\n",
    "\n",
    "def boxplot_with_swarmplot(df,col):\n",
    "    data = df.copy()\n",
    "    data[col] = data[col].astype('category')\n",
    "    try:\n",
    "        ax = sns.boxplot(x=col, y=\"treat\", data=data, orient=\"h\")\n",
    "        ax = sns.swarmplot(x=col, y=\"treat\", data=data, color=\".25\", orient=\"h\")\n",
    "    except:\n",
    "        data[col] = data[col].cat.codes\n",
    "        ax = sns.boxplot(x=col, y=\"treat\", data=data, orient=\"h\")\n",
    "        ax = sns.swarmplot(x=col, y=\"treat\", data=data, color=\".25\", orient=\"h\")\n",
    "        \n",
    "def plotting_data(df, col):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Histograms with boxplots and distribution\n",
    "    plt.subplot(2,2,1)\n",
    "    histogram_boxplot(df.loc[df[\"treat\"] == 1], col, \"With Treatment\")\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    histogram_boxplot(df.loc[df[\"treat\"] == 0], col, \"Without Treatment\")\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    histogram_boxplot(df, col, \"With and Without Treatment\")\n",
    "    \n",
    "    # Boxplots with datapoints\n",
    "    plt.subplot(2,2,4)\n",
    "    boxplot_with_swarmplot(df,col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def compare_distributions_of_feature(df,col, year = \"78\"):\n",
    "    data = df.copy()\n",
    "    data[col] = data[col].astype('category')\n",
    "    \n",
    "    if len(data[col].unique()) < 4:\n",
    "        categorical_boxplots(df, col, year)\n",
    "    else:\n",
    "        boxplot_with_swarmplot(df,col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_all_for_non_categorical_variables(df):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for idx,col in enumerate([\"age\", \"educ\",\"re74\",\"re75\",\"re78\"]):\n",
    "        if col in [\"age\", \"educ\"]:\n",
    "            plt.subplot(4,2,idx+1)\n",
    "        else:\n",
    "            plt.subplot(4,1,idx)\n",
    "            plt.xlim(0,61000)\n",
    "        boxplot_with_swarmplot(df,col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_distributions(df):\n",
    "    fig = plt.figure(figsize = (10,10))\n",
    "    for i, attribute in enumerate([\"married\", \"nodegree\", \"black\", \"white\", \"hispan\"]):\n",
    "        ax = plt.subplot(3,3,i+1)\n",
    "\n",
    "\n",
    "        ax.hist([df.loc[df_matched[attribute] == 1, \"treat\"],\\\n",
    "                 df.loc[df_matched[attribute] == 0, \"treat\"]], color=['r','b'], alpha=0.5)  \n",
    "        ax.set_title(attribute)\n",
    "        ax.legend([\"Treated\", \"Not Treated\"])\n",
    "    plt.tight_layout;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions used for computation and visualization of the confusion matrices**\n",
    "\n",
    "Code taken from [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html) for visualization of the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues, print_cnf = False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    if print_cnf:\n",
    "        print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to compute all the attributes of the confusion matrix as well as the true positive rates, false positive rates, precision, recall and F1 measure. The function also calls the plotting function above. Note that the function uses the weighted option for precision, recall and f1 score to take into account the fact that the classes are imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_confusion_matrix(y_test, y_est, classes, print_cnf = True):    \n",
    "    # Computing the confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_test, y_est);\n",
    "    # Displaying the values \n",
    "    if print_cnf:\n",
    "        print(cnf_matrix)\n",
    "    # Plotting the confusion matrix\n",
    "    plot_confusion_matrix(cnf_matrix, classes=classes, normalize=True,\n",
    "                          title='Normalized confusion matrix', print_cnf = print_cnf);\n",
    "    # Outputting the TPR, FPR, Precision, Recall, F1\n",
    "    TPR = cnf_matrix[0,0]/len(prop_data)\n",
    "    FPR = cnf_matrix[1,0]/len(prop_data)\n",
    "    precision = precision_score(y_test, y_est, average='weighted')\n",
    "    recall = recall_score(y_test, y_est, average='weighted')\n",
    "    F1 = f1_score(y_test, y_est, average='weighted')\n",
    "    print(\"--------------------------\")\n",
    "    print(\"True Positive Rate: {:.4f}\".format(TPR))\n",
    "    print(\"False Positive Rate: {:.4f}\".format(FPR))\n",
    "    print(\"Precision : {:.4f}\".format(precision))\n",
    "    print(\"Recall : {:.4f}\".format(recall))\n",
    "    print(\"F1 Score: {:.4f}\".format(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Naive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Raw Data **\n",
    "\n",
    "We started by recovering certain non parametric metrics regarding the raw data and plotting the correponding boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_description(data, \"re78\")\n",
    "\n",
    "print(\"Describing the data with parametric and non parametric metrics\")\n",
    "data[[\"re78\", \"treat\"]].groupby(\"treat\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotting_data(data, \"re78\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looking at the data description and boxplot :**\n",
    "\n",
    "Given the average revenues it would even seem that following the training program actually has a negative impact. Looking at the boxplot it would even seem that a larger portion of people suffer from lower salaries (looking at the difference between q3 and q1). However there are certain outlyers with significantly higher salaries when having followed the training program. This may be that there are certain people which benefitted significantly from the traning program.\n",
    "\n",
    "**Looking at the histograms **\n",
    "\n",
    "It is evident that a significant number of people do not have a job. This could potentially be biasing the results. We can also note that the distributions are not normal which can also be seen in the statistical tests where the null hypothesis is rejected when testing normality\n",
    "\n",
    "**Looking at the statistical results :**\n",
    "\n",
    "Both treatment and no treatment come from the same distribution. This could indicate that there is no difference between having taken the treatment or not. To compute the correlation we must use a non parametric correlation method such as the spearman correlation. This correlation is quite low (-0.04) which would indicate that there is no link between the revenue in 78 and having followed the training program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Closer Look\n",
    "\n",
    "**TODO : For each feature in the dataset, compare its distribution in the treated group with its distribution in the control group, using plots and numbers. As above, you may use the techniques we discussed in class for summarizing and comparing the distributions.\n",
    "What do you observe? Describe what your observations mean for the conclusions drawn by the naive \"researcher\" from his superficial analysis.**\n",
    "\n",
    "**Questions for TAS : **\n",
    "- Do we need to describe all the data with plots AND numbers? Or is one or the other sufficient. Especially since the boxplots are explicit enough \n",
    "\n",
    "**To Adrian and Jordi : do either of you have any ideas of what else can be visualized?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualising boxplots for different non categorical features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_for_non_categorical_variables(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain comments can already be made regarding the different features observed. In nearly all cases, the data is not normally distributed which is why it is important to consider non parametric metrics. That is why the boxplots are ideal to get a broad sense of what is going on in the data. Note that there may be a slight exception where the number of years of education is concerned though which seem to be part of the same distribution.\n",
    "\n",
    "**TODO : run the test numerically to validate that the ages are part of the same distribution and that it is normally distributed**\n",
    "\n",
    "More specifically :\n",
    "- In terms of age, the majority of people interviewed are below their 30s. The same goes for the number of people who did the training. \n",
    "- In terms of education, the majority of people interviewed are around 10 years of education. The same goes for the people who did the training. \n",
    "- Where the revenues are concerned\n",
    "    - There are a significant number of people who had 0 revenue in the given years. This means that there are people who are unemployed throughout the entire period which is not suprising in the sense that there is a majority of people under 30 who are being interviewed. This means that at the time of treatment, most of them are most likely still studying and not looking for jobs. \n",
    "    - The control group experienced a general decrease in revenue between 74 and 75. This is most likely due to the oil crisis which experienced it's first peak around that time ([see here](https://en.wikipedia.org/wiki/1970s_energy_crisis#/media/File:Nominalrealoilprices1968-2006.png)). Their revenues increased significantly by 78 even though they had not participated in the training program. This could also be linked to a drop in the oil prices before the peak in the 80s.\n",
    "    - Those who followed the training program essentially had revenues below 3000 dollars in 74 and 75. This is coherent with the fact that a majority of these people are young as mentioned previously and not necessarily looking for jobs. Therefore it would be logical to assume that in 78 when a larger fraction of people got jobs that their revenues would increase but that they would be lower than that of the control group where more people had jobs to begin with, especially since the salary is supposed to increase over time.\n",
    "    - The outlyers we can see in the treatment group were already present in 74 and 75. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Race **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [\"74\", \"75\",\"78\"]:\n",
    "    compare_distributions_of_feature(data,\"race\", year = year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Married **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [\"74\", \"75\",\"78\"]:\n",
    "    compare_distributions_of_feature(data,\"married\", year = year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No Degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [\"74\", \"75\",\"78\"]:\n",
    "    compare_distributions_of_feature(data,\"nodegree\", year = year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting information for categorical values regarding the treatment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,col in enumerate([\"married\", \"nodegree\", \"race\"]):\n",
    "    description = data[[col, \"treat\"]].groupby(col).describe()\n",
    "    print(description[\"treat\"].drop([\"min\", \"25%\", \"50%\", \"75%\", \"max\"],axis = 1))  \n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at how many people followed the seminar we can quickly see that none of the treatment vs. no treatment for the different categories are balanced neither in terms of count, average or distribution. This is something that will be addressed in part 4 of the exercise when matching. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Propensity Score Model\n",
    "\n",
    "Function used to compute the propensity scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity_scores(df):\n",
    "    treatment_features = ['age', 'black', 'white', 'hispan', 'educ', 'married', 'nodegree', 're74', 're75']\n",
    "    treatment_class = 'treat'\n",
    "\n",
    "    # The features vector\n",
    "    X = pd.get_dummies(df[treatment_features])\n",
    "    X.head(10)\n",
    "    # The labels\n",
    "    y = df[treatment_class]\n",
    "    # The model\n",
    "    logistic = LogisticRegression()\n",
    "    # Fit the model to the data\n",
    "    logistic.fit(X, y)\n",
    "    # Predictions \n",
    "    pred = logistic.predict_proba(X)\n",
    "\n",
    "    # Saving predictions to dataframe\n",
    "    df[\"Probability of Treatment\"] = pred[:,1]\n",
    "    df[\"Estimated Treatment\"] = np.where(df[\"Probability of Treatment\"]>=0.5, 1, 0)\n",
    "\n",
    "    return pred, y, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_data = data.copy()\n",
    "prop_data.reset_index(inplace = True)\n",
    "\n",
    "treated = prop_data[prop_data['treat']==1]\n",
    "control = prop_data[prop_data['treat']==0]\n",
    "\n",
    "print(\"Treated {0}, Not Treated {1}\".format(len(treated), len(control)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously the classes are imbalanced which will be adressed in the next question "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, labels, prop_data = propensity_scores(prop_data)\n",
    "# Computing the confusion matrix\n",
    "my_confusion_matrix(y, prop_data[\"Estimated Treatment\"].tolist(), [\"treatment\", \"control\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the normalized confusion matrix for the prediction labels we can see that with a simple non optimized threshold of 0.5 we can already obtain a realtively good estimation of whether a person was going to follow the training program or not. \n",
    "\n",
    "**Lookup how to read the confusion matrices again, especially in the case of unbalanced classes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Balancing the Dataset Via Matching\n",
    "\n",
    "http://jfinkels-networkx.readthedocs.io/en/latest/reference/algorithms/generated/networkx.algorithms.matching.max_weight_matching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bipartite_matching(labels, pred):\n",
    "    # Create a bipartite graph from the weighted adjacency matrix\n",
    "    G=nx.Graph()\n",
    "\n",
    "    idx_0 = [i for i in range(len(labels)) if labels[i] == 0] # not treated\n",
    "    idx_1 = [i for i in range(len(labels)) if labels[i] == 1] # treated\n",
    "\n",
    "    G.add_nodes_from(idx_0, bipartite=0) \n",
    "    G.add_nodes_from(idx_1, bipartite=1)\n",
    "\n",
    "    # Adding the edges to the graph as the difference in propensity score\n",
    "    for i in idx_0:\n",
    "        for j in idx_1:\n",
    "            G.add_edge(i,j,weight=1/((pred[i,1]-pred[j,1])**2+10**(-10)))\n",
    "\n",
    "    # Use the networkx max_weight_matching algorithm on the graph in order \n",
    "    # to find the optimal pairs.\n",
    "    matching = nx.max_weight_matching(G)\n",
    "\n",
    "    # Removing permutations \n",
    "    new_matching = [[k,v] for k, v in matching.items()]\n",
    "    new_matching = dict(set(tuple(sorted(pair)) for pair in new_matching))\n",
    "\n",
    "    return new_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_matching = bipartite_matching(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched.loc[[list(new_matching.keys())[5],list(new_matching.values())[5]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retaining only the matched pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched = data.reset_index()\n",
    "df_matched = df_matched.loc[ list(new_matching.keys()) + list(new_matching.values())]\n",
    "df_matched.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observing the distribution of the classes for categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distributions(df_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the distribution of the different variables we see that there is a big imbalance where the hispanics are concerned. \n",
    "\n",
    "**Observing the output for the year 1978**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotting_data(df_matched, \"re78\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_for_non_categorical_variables(df_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Balancing Groups Further\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_data = data.copy()\n",
    "stand_data.drop([\"race\"], axis = 1, inplace = True)\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(stand_data) \n",
    "stand_data.loc[:,:] = scaled_values\n",
    "stand_data[\"white\"] = 10*stand_data[\"white\"]\n",
    "stand_data[\"black\"] = 10*stand_data[\"black\"]\n",
    "stand_data[\"hispan\"] = 10*stand_data[\"hispan\"]\n",
    "stand_data[\"treat\"] = data[\"treat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computing the New Propensity Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_pred, stand_labels, stand_df = propensity_scores(stand_data)\n",
    "# Computing the confusion matrix\n",
    "my_confusion_matrix(stand_labels, stand_df[\"Estimated Treatment\"].tolist(), [\"treatment\", \"control\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stand_matching = bipartite_matching(stand_labels, stand_pred)\n",
    "stand_df_matched = data.reset_index()\n",
    "stand_df_matched = stand_df_matched.loc[ list(new_stand_matching.keys()) + list(new_stand_matching.values())]\n",
    "plot_distributions(stand_df_matched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = stand_df_matched.copy()\n",
    "categs = [\"treat\", \"age\", \"educ\", \"married\", \"nodegree\",\"re74\", \"re75\", \"black\", \"white\", \"hispan\"]\n",
    "temp1 = temp.loc[list(new_stand_matching.keys()),categs]                        \n",
    "temp2 = temp.loc[list(new_stand_matching.values()), categs]\n",
    "\n",
    "temp1.reset_index(inplace = True)       \n",
    "temp2.reset_index(inplace = True)  \n",
    "\n",
    "diff = temp1-temp2\n",
    "diff.drop(\"index\", axis = 1, inplace = True)\n",
    "diff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "## Question 2: Applied ML\n",
    "\n",
    "We are going to build a classifier of news to directly assign them to 20 news categories. Note that the pipeline that you will build in this exercise could be of great help during your project if you plan to work with text!\n",
    "\n",
    "1. Load the 20newsgroup dataset. It is, again, a classic dataset that can directly be loaded using sklearn ([link](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)).  \n",
    "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), short for term frequencyâ€“inverse document frequency, is of great help when if comes to compute textual features. Indeed, it gives more importance to terms that are more specific to the considered articles (TF) but reduces the importance of terms that are very frequent in the entire corpus (IDF). Compute TF-IDF features for every article using [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Then, split your dataset into a training, a testing and a validation set (10% for validation and 10% for testing). Each observation should be paired with its corresponding label (the article category).\n",
    "\n",
    "2. Train a random forest on your training set. Try to fine-tune the parameters of your predictor on your validation set using a simple grid search on the number of estimator \"n_estimators\" and the max depth of the trees \"max_depth\". Then, display a confusion matrix of your classification pipeline. Lastly, once you assessed your model, inspect the `feature_importances_` attribute of your random forest and discuss the obtained results.\n",
    "\n",
    "___\n",
    "___\n",
    "___\n",
    "\n",
    "**1. Loading the dataset **\n",
    "\n",
    "We started by importing the news groups dataset for the scikit learn library. We isolated the data and the target names then we computed the term frequency inverse document frequency for each of the entries. This becomes our matrix of features X. We then split the dataset into training, validation and testing set using `np.split` function. We were careful to have a sparse array at the end in order to speed up the classification with the random forests. It is important to note that there are 170'000 features for 18'000 samples. Therefore it would have been interesting to do a preliminary feature selection before classification.\n",
    "\n",
    "**2. Training and Classification With Random Forests **\n",
    "\n",
    "To find the best parameters we ran a grid search as asked, although it would have been faster to run a random search using the scikit learn RandomSearchCV. We chose not to use the GridSearchCV function because the grid search Cv function outputs the solution with the best results while there is often a tradeoff to be made between the number of parameters / computation time and the accuracy. We wanted to be able to select the set of parameters which offered the best compromise which is why we output the results of the grid search in the form of a heatmap before finding the best tradeoff. We then retrained the model with the optimal parameters, did the confusion matrix and assessed the results as well as the features selected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the entire dataset from the website with shuffled ordering\n",
    "newsgroups = fetch_20newsgroups(subset = \"all\")\n",
    "newsgroups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the features \n",
    "vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X = vectorizer.fit_transform(newsgroups[\"data\"])\n",
    "# Labels\n",
    "Y = newsgroups.target\n",
    "# Mapping the indexes of targets to the names\n",
    "targets_dict = {i:newsgroups['target_names'][i] for i in range(len(newsgroups['target_names']))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data is already shuffled when loading with the fetching function, we can simply slice the arrays without randomizing the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999575506738831 0.10002122466305848 0.10002122466305848\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data into a training, testing and validation set\n",
    "total = len(Y)\n",
    "prop = int(np.ceil(0.1*len(Y)))\n",
    "# Split the training data and the labels \n",
    "X_train, X_valid, X_test = np.split(X.toarray(), [total-2*prop, total-prop])\n",
    "y_train, y_valid, y_test = np.split(Y, [total-2*prop, total-prop])\n",
    "\n",
    "# Verifying the proportions for the training, validation and testing sets\n",
    "print(len(X_train)/total, len(X_valid)/total, len(X_test)/total)\n",
    "\n",
    "# Convert to sparse matrices\n",
    "X_train, X_valid, X_test = sparse.csc_matrix(X_train), sparse.csc_matrix(X_valid), sparse.csc_matrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training and Classification With Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First a functions which are useful for the question :\n",
    "- train the random forest classifier model given a training set, the desired number of estimators and the max depth\n",
    "- test the given model on a given test set and output the accuracy\n",
    "- run a grid serach on a training and testing set for a given range of estimators and max_depth parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, num_estimators, max_depth):\n",
    "    rf = RandomForestClassifier(n_estimators=num_estimators, max_depth = max_depth,\\\n",
    "                                random_state = 0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    return rf\n",
    "\n",
    "def test_model(X_test, y_test, rf):\n",
    "    predicted = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "    return predicted, accuracy\n",
    "\n",
    "def grid_search(estimators, depth, step, X_train, y_train, X_test, y_test):\n",
    "    accuracy_scores = dict()\n",
    "    accuracy_mat = np.zeros((max_num_est//step,max_num_depth//step))\n",
    "    \n",
    "    for n, n_estimators in enumerate(estimators):\n",
    "        for md, max_depth in tqdm(enumerate(depth)):\n",
    "            rf = train_model(X_train, y_train, n_estimators, max_depth)\n",
    "            _,accuracy = test_model(X_valid, y_valid, rf)       \n",
    "            accuracy_scores[(n_estimators, max_depth)]= accuracy\n",
    "            accuracy_mat[n,md] = accuracy \n",
    "            \n",
    "    return accuracy_scores, accuracy_mat\n",
    "\n",
    "def plot_grid_search_results(accuracy_mat, estimators, depth):\n",
    "    ax = sns.heatmap(accuracy_mat, annot=True)\n",
    "    ax.set_xlabel(\"Estimators\")\n",
    "    ax.set_ylabel(\"Depth\")\n",
    "    ax.set_xticklabels(estimators,rotation=90)\n",
    "    ax.set_yticklabels(depth,rotation=0)\n",
    "    ax.set_title(\"Accuracy depending on the number of decision trees and max depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting off with a rough estimation of the accuracy over a limited set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [31:57, 383.54s/it]\n",
      "5it [1:04:43, 776.69s/it]\n",
      "5it [1:45:49, 1269.90s/it]\n",
      "5it [2:12:25, 1589.03s/it]\n",
      "5it [2:58:13, 2138.76s/it]\n"
     ]
    }
   ],
   "source": [
    "max_num_est, max_num_depth, step = 1001, 1001, 200\n",
    "estimators, depth = range(step,max_num_est,step), range(step,max_num_depth,step)\n",
    "\n",
    "accuracy_scores, accuracy_mat = grid_search(estimators, depth, step, X_train, y_train, \n",
    "                                            X_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAEmCAYAAADRIc8sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11296e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_grid_search_results(accuracy_mat, estimators, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [15:20, 31.73s/it]\n",
      "29it [12:38, 26.15s/it]\n",
      "29it [13:18, 27.53s/it]\n",
      "29it [18:05, 37.44s/it]\n",
      "29it [21:04, 43.61s/it]\n",
      "29it [25:21, 52.48s/it]\n",
      "29it [29:59, 62.05s/it]\n",
      "29it [34:25, 71.21s/it]\n",
      "29it [37:55, 78.47s/it]\n",
      "29it [42:30, 87.93s/it]\n",
      "29it [46:35, 96.40s/it]\n",
      "29it [51:25, 106.38s/it]\n",
      "29it [54:35, 112.94s/it]\n",
      "29it [59:45, 123.64s/it]\n",
      "29it [1:03:44, 131.88s/it]\n",
      "29it [1:42:50, 212.78s/it]\n",
      "29it [1:54:10, 236.22s/it]\n",
      "29it [1:21:07, 167.83s/it]\n",
      "29it [1:28:52, 183.87s/it]\n",
      "29it [1:40:18, 207.55s/it]\n",
      "25it [1:22:00, 196.82s/it]"
     ]
    }
   ],
   "source": [
    "max_num_est, max_num_depth, step = 300, 300, 10\n",
    "estimators, depth = range(step,max_num_est,step), range(step,max_num_depth,step)\n",
    "\n",
    "accuracy_scores, accuracy_mat = grid_search(estimators, depth, step, X_train, y_train, \n",
    "                                            X_valid, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_n_est,opt_max_depth = max(accuracy_scores, key=accuracy_scores.get)\n",
    "rf = RandomForestClassifier(n_estimators=opt_n_est, max_depth = opt_max_depth,\\\n",
    "                            random_state = 0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "predicted_train, train_accuracy = test_model(X_train, y_train, rf)\n",
    "predicted_valid, valid_accuracy = test_model(X_valid, y_valid, rf)\n",
    "predicted_test, test_accuracy = test_model(X_test, y_test, rf)\n",
    "\n",
    "print(\"Training Accuracy : {}\".format(train_accuracy))\n",
    "print(\"Validation Accuracy : {}\".format(valid_accuracy))\n",
    "print(\"Testing Accuracy : {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_est, test_accuracy = test_model(X_test, y_test, rf)\n",
    "plt.figure(figsize = (12,12))\n",
    "my_confusion_matrix(y_test, y_est, newsgroups['target_names'], print_cnf = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_relevant_features = np.count_nonzero(rf.feature_importances_)\n",
    "prop_relevant_features = num_relevant_features/len(rf.feature_importances_)*100\n",
    "print(\"Number of features with non zero importance {}\".format(num_relevant_features))\n",
    "print(\"Proportion of features with non zero importance : {:.1f} %\".format(prop_relevant_features))\n",
    "print(\"Highest importance obtained : {:.2f}%\".format(max(rf.feature_importances_)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ADA]",
   "language": "python",
   "name": "conda-env-ADA-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
